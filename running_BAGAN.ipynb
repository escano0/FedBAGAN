{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17144,"status":"ok","timestamp":1654150365722,"user":{"displayName":"Escano Xiao","userId":"11935749661615477607"},"user_tz":-480},"id":"-YLCCdrmnzFX","outputId":"8b1f101a-302c-4d51-d00f-bfdf3a36059e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive \n","drive.mount('/content/drive')\n","\n","import os \n","os.chdir('/content/drive/My Drive/BAGAN')\n"]},{"cell_type":"markdown","metadata":{"id":"qi_jvR9ekuHl"},"source":["# Autoencoder training"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1VFgNizloB_u","executionInfo":{"status":"ok","timestamp":1654017013267,"user_tz":-480,"elapsed":19716083,"user":{"displayName":"Escano Xiao","userId":"11935749661615477607"}},"outputId":"a01cabd1-0419-46c5-ded4-25f11034ab3f"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1;30;43m流式输出内容被截断，只能显示最后 5000 行内容。\u001b[0m\n","Loss = 0.0023 (ave = 0.0035)\n","\n","epoch: [212/400] iteration: [198/782] step: 165200 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0035)\n","\n","epoch: [212/400] iteration: [298/782] step: 165300 Learning rate: 0.0002\n","Loss = 0.0023 (ave = 0.0035)\n","\n","epoch: [212/400] iteration: [398/782] step: 165400 Learning rate: 0.0002\n","Loss = 0.0023 (ave = 0.0035)\n","\n","epoch: [212/400] iteration: [498/782] step: 165500 Learning rate: 0.0002\n","Loss = 0.0023 (ave = 0.0035)\n","\n","epoch: [212/400] iteration: [598/782] step: 165600 Learning rate: 0.0002\n","Loss = 0.0023 (ave = 0.0035)\n","\n","epoch: [212/400] iteration: [698/782] step: 165700 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0035)\n","\n","epoch: [212/400] iteration: [782/782] step: 165785 Learning rate: 0.0002\n","Loss = 0.0041 (ave = 0.0035)\n","\n","epoch: [213/400] iteration: [16/782] step: 165800 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0035)\n","\n","epoch: [213/400] iteration: [116/782] step: 165900 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0035)\n","\n","epoch: [213/400] iteration: [216/782] step: 166000 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0035)\n","\n","epoch: [213/400] iteration: [316/782] step: 166100 Learning rate: 0.0002\n","Loss = 0.0023 (ave = 0.0035)\n","\n","epoch: [213/400] iteration: [416/782] step: 166200 Learning rate: 0.0002\n","Loss = 0.0023 (ave = 0.0035)\n","\n","epoch: [213/400] iteration: [516/782] step: 166300 Learning rate: 0.0002\n","Loss = 0.0023 (ave = 0.0035)\n","\n","epoch: [213/400] iteration: [616/782] step: 166400 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0035)\n","\n","epoch: [213/400] iteration: [716/782] step: 166500 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0035)\n","\n","epoch: [213/400] iteration: [782/782] step: 166567 Learning rate: 0.0002\n","Loss = 0.0026 (ave = 0.0035)\n","\n","epoch: [214/400] iteration: [34/782] step: 166600 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0035)\n","\n","epoch: [214/400] iteration: [134/782] step: 166700 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0035)\n","\n","epoch: [214/400] iteration: [234/782] step: 166800 Learning rate: 0.0002\n","Loss = 0.0023 (ave = 0.0035)\n","\n","epoch: [214/400] iteration: [334/782] step: 166900 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0035)\n","\n","epoch: [214/400] iteration: [434/782] step: 167000 Learning rate: 0.0002\n","Loss = 0.0023 (ave = 0.0035)\n","\n","epoch: [214/400] iteration: [534/782] step: 167100 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0035)\n","\n","epoch: [214/400] iteration: [634/782] step: 167200 Learning rate: 0.0002\n","Loss = 0.0024 (ave = 0.0035)\n","\n","epoch: [214/400] iteration: [734/782] step: 167300 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0035)\n","\n","epoch: [214/400] iteration: [782/782] step: 167349 Learning rate: 0.0002\n","Loss = 0.0086 (ave = 0.0035)\n","\n","epoch: [215/400] iteration: [52/782] step: 167400 Learning rate: 0.0002\n","Loss = 0.0023 (ave = 0.0035)\n","\n","epoch: [215/400] iteration: [152/782] step: 167500 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0035)\n","\n","epoch: [215/400] iteration: [252/782] step: 167600 Learning rate: 0.0002\n","Loss = 0.0023 (ave = 0.0035)\n","\n","epoch: [215/400] iteration: [352/782] step: 167700 Learning rate: 0.0002\n","Loss = 0.0024 (ave = 0.0035)\n","\n","epoch: [215/400] iteration: [452/782] step: 167800 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0035)\n","\n","epoch: [215/400] iteration: [552/782] step: 167900 Learning rate: 0.0002\n","Loss = 0.0023 (ave = 0.0035)\n","\n","epoch: [215/400] iteration: [652/782] step: 168000 Learning rate: 0.0002\n","Loss = 0.0026 (ave = 0.0035)\n","\n","epoch: [215/400] iteration: [752/782] step: 168100 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0035)\n","\n","epoch: [215/400] iteration: [782/782] step: 168131 Learning rate: 0.0002\n","Loss = 0.0030 (ave = 0.0035)\n","\n","epoch: [216/400] iteration: [70/782] step: 168200 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0035)\n","\n","epoch: [216/400] iteration: [170/782] step: 168300 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0035)\n","\n","epoch: [216/400] iteration: [270/782] step: 168400 Learning rate: 0.0002\n","Loss = 0.0023 (ave = 0.0035)\n","\n","epoch: [216/400] iteration: [370/782] step: 168500 Learning rate: 0.0002\n","Loss = 0.0026 (ave = 0.0035)\n","\n","epoch: [216/400] iteration: [470/782] step: 168600 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0035)\n","\n","epoch: [216/400] iteration: [570/782] step: 168700 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0035)\n","\n","epoch: [216/400] iteration: [670/782] step: 168800 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0035)\n","\n","epoch: [216/400] iteration: [770/782] step: 168900 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0035)\n","\n","epoch: [216/400] iteration: [782/782] step: 168913 Learning rate: 0.0002\n","Loss = 0.0037 (ave = 0.0035)\n","\n","epoch: [217/400] iteration: [88/782] step: 169000 Learning rate: 0.0002\n","Loss = 0.0023 (ave = 0.0035)\n","\n","epoch: [217/400] iteration: [188/782] step: 169100 Learning rate: 0.0002\n","Loss = 0.0023 (ave = 0.0035)\n","\n","epoch: [217/400] iteration: [288/782] step: 169200 Learning rate: 0.0002\n","Loss = 0.0023 (ave = 0.0035)\n","\n","epoch: [217/400] iteration: [388/782] step: 169300 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0035)\n","\n","epoch: [217/400] iteration: [488/782] step: 169400 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0035)\n","\n","epoch: [217/400] iteration: [588/782] step: 169500 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0035)\n","\n","epoch: [217/400] iteration: [688/782] step: 169600 Learning rate: 0.0002\n","Loss = 0.0024 (ave = 0.0035)\n","\n","epoch: [217/400] iteration: [782/782] step: 169695 Learning rate: 0.0002\n","Loss = 0.0035 (ave = 0.0035)\n","\n","epoch: [218/400] iteration: [6/782] step: 169700 Learning rate: 0.0002\n","Loss = 0.0023 (ave = 0.0035)\n","\n","epoch: [218/400] iteration: [106/782] step: 169800 Learning rate: 0.0002\n","Loss = 0.0023 (ave = 0.0035)\n","\n","epoch: [218/400] iteration: [206/782] step: 169900 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0035)\n","\n","epoch: [218/400] iteration: [306/782] step: 170000 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0035)\n","\n","epoch: [218/400] iteration: [406/782] step: 170100 Learning rate: 0.0002\n","Loss = 0.0023 (ave = 0.0035)\n","\n","epoch: [218/400] iteration: [506/782] step: 170200 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0035)\n","\n","epoch: [218/400] iteration: [606/782] step: 170300 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0035)\n","\n","epoch: [218/400] iteration: [706/782] step: 170400 Learning rate: 0.0002\n","Loss = 0.0023 (ave = 0.0035)\n","\n","epoch: [218/400] iteration: [782/782] step: 170477 Learning rate: 0.0002\n","Loss = 0.0031 (ave = 0.0035)\n","\n","epoch: [219/400] iteration: [24/782] step: 170500 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0035)\n","\n","epoch: [219/400] iteration: [124/782] step: 170600 Learning rate: 0.0002\n","Loss = 0.0023 (ave = 0.0035)\n","\n","epoch: [219/400] iteration: [224/782] step: 170700 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0035)\n","\n","epoch: [219/400] iteration: [324/782] step: 170800 Learning rate: 0.0002\n","Loss = 0.0023 (ave = 0.0035)\n","\n","epoch: [219/400] iteration: [424/782] step: 170900 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0035)\n","\n","epoch: [219/400] iteration: [524/782] step: 171000 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0035)\n","\n","epoch: [219/400] iteration: [624/782] step: 171100 Learning rate: 0.0002\n","Loss = 0.0026 (ave = 0.0035)\n","\n","epoch: [219/400] iteration: [724/782] step: 171200 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0035)\n","\n","epoch: [219/400] iteration: [782/782] step: 171259 Learning rate: 0.0002\n","Loss = 0.0026 (ave = 0.0035)\n","\n","epoch: [220/400] iteration: [42/782] step: 171300 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0035)\n","\n","epoch: [220/400] iteration: [142/782] step: 171400 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0035)\n","\n","epoch: [220/400] iteration: [242/782] step: 171500 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0035)\n","\n","epoch: [220/400] iteration: [342/782] step: 171600 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0035)\n","\n","epoch: [220/400] iteration: [442/782] step: 171700 Learning rate: 0.0002\n","Loss = 0.0024 (ave = 0.0035)\n","\n","epoch: [220/400] iteration: [542/782] step: 171800 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0035)\n","\n","epoch: [220/400] iteration: [642/782] step: 171900 Learning rate: 0.0002\n","Loss = 0.0023 (ave = 0.0035)\n","\n","epoch: [220/400] iteration: [742/782] step: 172000 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0035)\n","\n","epoch: [220/400] iteration: [782/782] step: 172041 Learning rate: 0.0002\n","Loss = 0.0038 (ave = 0.0035)\n","\n","epoch: [221/400] iteration: [60/782] step: 172100 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0035)\n","\n","epoch: [221/400] iteration: [160/782] step: 172200 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0035)\n","\n","epoch: [221/400] iteration: [260/782] step: 172300 Learning rate: 0.0002\n","Loss = 0.0023 (ave = 0.0035)\n","\n","epoch: [221/400] iteration: [360/782] step: 172400 Learning rate: 0.0002\n","Loss = 0.0024 (ave = 0.0035)\n","\n","epoch: [221/400] iteration: [460/782] step: 172500 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0035)\n","\n","epoch: [221/400] iteration: [560/782] step: 172600 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0035)\n","\n","epoch: [221/400] iteration: [660/782] step: 172700 Learning rate: 0.0002\n","Loss = 0.0023 (ave = 0.0035)\n","\n","epoch: [221/400] iteration: [760/782] step: 172800 Learning rate: 0.0002\n","Loss = 0.0025 (ave = 0.0035)\n","\n","epoch: [221/400] iteration: [782/782] step: 172823 Learning rate: 0.0002\n","Loss = 0.0028 (ave = 0.0035)\n","\n","epoch: [222/400] iteration: [78/782] step: 172900 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0035)\n","\n","epoch: [222/400] iteration: [178/782] step: 173000 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0035)\n","\n","epoch: [222/400] iteration: [278/782] step: 173100 Learning rate: 0.0002\n","Loss = 0.0023 (ave = 0.0035)\n","\n","epoch: [222/400] iteration: [378/782] step: 173200 Learning rate: 0.0002\n","Loss = 0.0025 (ave = 0.0035)\n","\n","epoch: [222/400] iteration: [478/782] step: 173300 Learning rate: 0.0002\n","Loss = 0.0023 (ave = 0.0035)\n","\n","epoch: [222/400] iteration: [578/782] step: 173400 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0035)\n","\n","epoch: [222/400] iteration: [678/782] step: 173500 Learning rate: 0.0002\n","Loss = 0.0024 (ave = 0.0035)\n","\n","epoch: [222/400] iteration: [778/782] step: 173600 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0035)\n","\n","epoch: [222/400] iteration: [782/782] step: 173605 Learning rate: 0.0002\n","Loss = 0.0027 (ave = 0.0035)\n","\n","epoch: [223/400] iteration: [96/782] step: 173700 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0035)\n","\n","epoch: [223/400] iteration: [196/782] step: 173800 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0035)\n","\n","epoch: [223/400] iteration: [296/782] step: 173900 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0034)\n","\n","epoch: [223/400] iteration: [396/782] step: 174000 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0034)\n","\n","epoch: [223/400] iteration: [496/782] step: 174100 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0034)\n","\n","epoch: [223/400] iteration: [596/782] step: 174200 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0034)\n","\n","epoch: [223/400] iteration: [696/782] step: 174300 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0034)\n","\n","epoch: [223/400] iteration: [782/782] step: 174387 Learning rate: 0.0002\n","Loss = 0.0038 (ave = 0.0034)\n","\n","epoch: [224/400] iteration: [14/782] step: 174400 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0034)\n","\n","epoch: [224/400] iteration: [114/782] step: 174500 Learning rate: 0.0002\n","Loss = 0.0023 (ave = 0.0034)\n","\n","epoch: [224/400] iteration: [214/782] step: 174600 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0034)\n","\n","epoch: [224/400] iteration: [314/782] step: 174700 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0034)\n","\n","epoch: [224/400] iteration: [414/782] step: 174800 Learning rate: 0.0002\n","Loss = 0.0024 (ave = 0.0034)\n","\n","epoch: [224/400] iteration: [514/782] step: 174900 Learning rate: 0.0002\n","Loss = 0.0025 (ave = 0.0034)\n","\n","epoch: [224/400] iteration: [614/782] step: 175000 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0034)\n","\n","epoch: [224/400] iteration: [714/782] step: 175100 Learning rate: 0.0002\n","Loss = 0.0023 (ave = 0.0034)\n","\n","epoch: [224/400] iteration: [782/782] step: 175169 Learning rate: 0.0002\n","Loss = 0.0036 (ave = 0.0034)\n","\n","epoch: [225/400] iteration: [32/782] step: 175200 Learning rate: 0.0002\n","Loss = 0.0025 (ave = 0.0034)\n","\n","epoch: [225/400] iteration: [132/782] step: 175300 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0034)\n","\n","epoch: [225/400] iteration: [232/782] step: 175400 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0034)\n","\n","epoch: [225/400] iteration: [332/782] step: 175500 Learning rate: 0.0002\n","Loss = 0.0023 (ave = 0.0034)\n","\n","epoch: [225/400] iteration: [432/782] step: 175600 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0034)\n","\n","epoch: [225/400] iteration: [532/782] step: 175700 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0034)\n","\n","epoch: [225/400] iteration: [632/782] step: 175800 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0034)\n","\n","epoch: [225/400] iteration: [732/782] step: 175900 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0034)\n","\n","epoch: [225/400] iteration: [782/782] step: 175951 Learning rate: 0.0002\n","Loss = 0.0036 (ave = 0.0034)\n","\n","epoch: [226/400] iteration: [50/782] step: 176000 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0034)\n","\n","epoch: [226/400] iteration: [150/782] step: 176100 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0034)\n","\n","epoch: [226/400] iteration: [250/782] step: 176200 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0034)\n","\n","epoch: [226/400] iteration: [350/782] step: 176300 Learning rate: 0.0002\n","Loss = 0.0023 (ave = 0.0034)\n","\n","epoch: [226/400] iteration: [450/782] step: 176400 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0034)\n","\n","epoch: [226/400] iteration: [550/782] step: 176500 Learning rate: 0.0002\n","Loss = 0.0023 (ave = 0.0034)\n","\n","epoch: [226/400] iteration: [650/782] step: 176600 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0034)\n","\n","epoch: [226/400] iteration: [750/782] step: 176700 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0034)\n","\n","epoch: [226/400] iteration: [782/782] step: 176733 Learning rate: 0.0002\n","Loss = 0.0032 (ave = 0.0034)\n","\n","epoch: [227/400] iteration: [68/782] step: 176800 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0034)\n","\n","epoch: [227/400] iteration: [168/782] step: 176900 Learning rate: 0.0002\n","Loss = 0.0023 (ave = 0.0034)\n","\n","epoch: [227/400] iteration: [268/782] step: 177000 Learning rate: 0.0002\n","Loss = 0.0024 (ave = 0.0034)\n","\n","epoch: [227/400] iteration: [368/782] step: 177100 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0034)\n","\n","epoch: [227/400] iteration: [468/782] step: 177200 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0034)\n","\n","epoch: [227/400] iteration: [568/782] step: 177300 Learning rate: 0.0002\n","Loss = 0.0024 (ave = 0.0034)\n","\n","epoch: [227/400] iteration: [668/782] step: 177400 Learning rate: 0.0002\n","Loss = 0.0024 (ave = 0.0034)\n","\n","epoch: [227/400] iteration: [768/782] step: 177500 Learning rate: 0.0002\n","Loss = 0.0023 (ave = 0.0034)\n","\n","epoch: [227/400] iteration: [782/782] step: 177515 Learning rate: 0.0002\n","Loss = 0.0028 (ave = 0.0034)\n","\n","epoch: [228/400] iteration: [86/782] step: 177600 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0034)\n","\n","epoch: [228/400] iteration: [186/782] step: 177700 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0034)\n","\n","epoch: [228/400] iteration: [286/782] step: 177800 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0034)\n","\n","epoch: [228/400] iteration: [386/782] step: 177900 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0034)\n","\n","epoch: [228/400] iteration: [486/782] step: 178000 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0034)\n","\n","epoch: [228/400] iteration: [586/782] step: 178100 Learning rate: 0.0002\n","Loss = 0.0024 (ave = 0.0034)\n","\n","epoch: [228/400] iteration: [686/782] step: 178200 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0034)\n","\n","epoch: [228/400] iteration: [782/782] step: 178297 Learning rate: 0.0002\n","Loss = 0.0025 (ave = 0.0034)\n","\n","epoch: [229/400] iteration: [4/782] step: 178300 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0034)\n","\n","epoch: [229/400] iteration: [104/782] step: 178400 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0034)\n","\n","epoch: [229/400] iteration: [204/782] step: 178500 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0034)\n","\n","epoch: [229/400] iteration: [304/782] step: 178600 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0034)\n","\n","epoch: [229/400] iteration: [404/782] step: 178700 Learning rate: 0.0002\n","Loss = 0.0023 (ave = 0.0034)\n","\n","epoch: [229/400] iteration: [504/782] step: 178800 Learning rate: 0.0002\n","Loss = 0.0023 (ave = 0.0034)\n","\n","epoch: [229/400] iteration: [604/782] step: 178900 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0034)\n","\n","epoch: [229/400] iteration: [704/782] step: 179000 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0034)\n","\n","epoch: [229/400] iteration: [782/782] step: 179079 Learning rate: 0.0002\n","Loss = 0.0024 (ave = 0.0034)\n","\n","epoch: [230/400] iteration: [22/782] step: 179100 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0034)\n","\n","epoch: [230/400] iteration: [122/782] step: 179200 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0034)\n","\n","epoch: [230/400] iteration: [222/782] step: 179300 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0034)\n","\n","epoch: [230/400] iteration: [322/782] step: 179400 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0034)\n","\n","epoch: [230/400] iteration: [422/782] step: 179500 Learning rate: 0.0002\n","Loss = 0.0024 (ave = 0.0034)\n","\n","epoch: [230/400] iteration: [522/782] step: 179600 Learning rate: 0.0002\n","Loss = 0.0025 (ave = 0.0034)\n","\n","epoch: [230/400] iteration: [622/782] step: 179700 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0034)\n","\n","epoch: [230/400] iteration: [722/782] step: 179800 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0034)\n","\n","epoch: [230/400] iteration: [782/782] step: 179861 Learning rate: 0.0002\n","Loss = 0.0025 (ave = 0.0034)\n","\n","epoch: [231/400] iteration: [40/782] step: 179900 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0034)\n","\n","epoch: [231/400] iteration: [140/782] step: 180000 Learning rate: 0.0002\n","Loss = 0.0024 (ave = 0.0034)\n","\n","epoch: [231/400] iteration: [240/782] step: 180100 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0034)\n","\n","epoch: [231/400] iteration: [340/782] step: 180200 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0034)\n","\n","epoch: [231/400] iteration: [440/782] step: 180300 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0034)\n","\n","epoch: [231/400] iteration: [540/782] step: 180400 Learning rate: 0.0002\n","Loss = 0.0023 (ave = 0.0034)\n","\n","epoch: [231/400] iteration: [640/782] step: 180500 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0034)\n","\n","epoch: [231/400] iteration: [740/782] step: 180600 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0034)\n","\n","epoch: [231/400] iteration: [782/782] step: 180643 Learning rate: 0.0002\n","Loss = 0.0026 (ave = 0.0034)\n","\n","epoch: [232/400] iteration: [58/782] step: 180700 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0034)\n","\n","epoch: [232/400] iteration: [158/782] step: 180800 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0034)\n","\n","epoch: [232/400] iteration: [258/782] step: 180900 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0034)\n","\n","epoch: [232/400] iteration: [358/782] step: 181000 Learning rate: 0.0002\n","Loss = 0.0026 (ave = 0.0034)\n","\n","epoch: [232/400] iteration: [458/782] step: 181100 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0034)\n","\n","epoch: [232/400] iteration: [558/782] step: 181200 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0034)\n","\n","epoch: [232/400] iteration: [658/782] step: 181300 Learning rate: 0.0002\n","Loss = 0.0024 (ave = 0.0034)\n","\n","epoch: [232/400] iteration: [758/782] step: 181400 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0034)\n","\n","epoch: [232/400] iteration: [782/782] step: 181425 Learning rate: 0.0002\n","Loss = 0.0036 (ave = 0.0034)\n","\n","epoch: [233/400] iteration: [76/782] step: 181500 Learning rate: 0.0002\n","Loss = 0.0024 (ave = 0.0034)\n","\n","epoch: [233/400] iteration: [176/782] step: 181600 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0034)\n","\n","epoch: [233/400] iteration: [276/782] step: 181700 Learning rate: 0.0002\n","Loss = 0.0023 (ave = 0.0034)\n","\n","epoch: [233/400] iteration: [376/782] step: 181800 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0034)\n","\n","epoch: [233/400] iteration: [476/782] step: 181900 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0034)\n","\n","epoch: [233/400] iteration: [576/782] step: 182000 Learning rate: 0.0002\n","Loss = 0.0023 (ave = 0.0034)\n","\n","epoch: [233/400] iteration: [676/782] step: 182100 Learning rate: 0.0002\n","Loss = 0.0023 (ave = 0.0034)\n","\n","epoch: [233/400] iteration: [776/782] step: 182200 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0034)\n","\n","epoch: [233/400] iteration: [782/782] step: 182207 Learning rate: 0.0002\n","Loss = 0.0026 (ave = 0.0034)\n","\n","epoch: [234/400] iteration: [94/782] step: 182300 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0034)\n","\n","epoch: [234/400] iteration: [194/782] step: 182400 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0034)\n","\n","epoch: [234/400] iteration: [294/782] step: 182500 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0034)\n","\n","epoch: [234/400] iteration: [394/782] step: 182600 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0034)\n","\n","epoch: [234/400] iteration: [494/782] step: 182700 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0034)\n","\n","epoch: [234/400] iteration: [594/782] step: 182800 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0034)\n","\n","epoch: [234/400] iteration: [694/782] step: 182900 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0034)\n","\n","epoch: [234/400] iteration: [782/782] step: 182989 Learning rate: 0.0002\n","Loss = 0.0029 (ave = 0.0034)\n","\n","epoch: [235/400] iteration: [12/782] step: 183000 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0034)\n","\n","epoch: [235/400] iteration: [112/782] step: 183100 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0034)\n","\n","epoch: [235/400] iteration: [212/782] step: 183200 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0034)\n","\n","epoch: [235/400] iteration: [312/782] step: 183300 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0034)\n","\n","epoch: [235/400] iteration: [412/782] step: 183400 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0034)\n","\n","epoch: [235/400] iteration: [512/782] step: 183500 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0034)\n","\n","epoch: [235/400] iteration: [612/782] step: 183600 Learning rate: 0.0002\n","Loss = 0.0023 (ave = 0.0034)\n","\n","epoch: [235/400] iteration: [712/782] step: 183700 Learning rate: 0.0002\n","Loss = 0.0023 (ave = 0.0034)\n","\n","epoch: [235/400] iteration: [782/782] step: 183771 Learning rate: 0.0002\n","Loss = 0.0026 (ave = 0.0034)\n","\n","epoch: [236/400] iteration: [30/782] step: 183800 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0034)\n","\n","epoch: [236/400] iteration: [130/782] step: 183900 Learning rate: 0.0002\n","Loss = 0.0024 (ave = 0.0034)\n","\n","epoch: [236/400] iteration: [230/782] step: 184000 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0034)\n","\n","epoch: [236/400] iteration: [330/782] step: 184100 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0034)\n","\n","epoch: [236/400] iteration: [430/782] step: 184200 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0034)\n","\n","epoch: [236/400] iteration: [530/782] step: 184300 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0034)\n","\n","epoch: [236/400] iteration: [630/782] step: 184400 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0034)\n","\n","epoch: [236/400] iteration: [730/782] step: 184500 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0034)\n","\n","epoch: [236/400] iteration: [782/782] step: 184553 Learning rate: 0.0002\n","Loss = 0.0032 (ave = 0.0034)\n","\n","epoch: [237/400] iteration: [48/782] step: 184600 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0034)\n","\n","epoch: [237/400] iteration: [148/782] step: 184700 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0034)\n","\n","epoch: [237/400] iteration: [248/782] step: 184800 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0034)\n","\n","epoch: [237/400] iteration: [348/782] step: 184900 Learning rate: 0.0002\n","Loss = 0.0023 (ave = 0.0034)\n","\n","epoch: [237/400] iteration: [448/782] step: 185000 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0034)\n","\n","epoch: [237/400] iteration: [548/782] step: 185100 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0034)\n","\n","epoch: [237/400] iteration: [648/782] step: 185200 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0034)\n","\n","epoch: [237/400] iteration: [748/782] step: 185300 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0034)\n","\n","epoch: [237/400] iteration: [782/782] step: 185335 Learning rate: 0.0002\n","Loss = 0.0028 (ave = 0.0034)\n","\n","epoch: [238/400] iteration: [66/782] step: 185400 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0034)\n","\n","epoch: [238/400] iteration: [166/782] step: 185500 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0034)\n","\n","epoch: [238/400] iteration: [266/782] step: 185600 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0034)\n","\n","epoch: [238/400] iteration: [366/782] step: 185700 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0034)\n","\n","epoch: [238/400] iteration: [466/782] step: 185800 Learning rate: 0.0002\n","Loss = 0.0023 (ave = 0.0034)\n","\n","epoch: [238/400] iteration: [566/782] step: 185900 Learning rate: 0.0002\n","Loss = 0.0023 (ave = 0.0034)\n","\n","epoch: [238/400] iteration: [666/782] step: 186000 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0034)\n","\n","epoch: [238/400] iteration: [766/782] step: 186100 Learning rate: 0.0002\n","Loss = 0.0023 (ave = 0.0034)\n","\n","epoch: [238/400] iteration: [782/782] step: 186117 Learning rate: 0.0002\n","Loss = 0.0029 (ave = 0.0034)\n","\n","epoch: [239/400] iteration: [84/782] step: 186200 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0034)\n","\n","epoch: [239/400] iteration: [184/782] step: 186300 Learning rate: 0.0002\n","Loss = 0.0025 (ave = 0.0034)\n","\n","epoch: [239/400] iteration: [284/782] step: 186400 Learning rate: 0.0002\n","Loss = 0.0023 (ave = 0.0034)\n","\n","epoch: [239/400] iteration: [384/782] step: 186500 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0034)\n","\n","epoch: [239/400] iteration: [484/782] step: 186600 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0034)\n","\n","epoch: [239/400] iteration: [584/782] step: 186700 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0034)\n","\n","epoch: [239/400] iteration: [684/782] step: 186800 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0034)\n","\n","epoch: [239/400] iteration: [782/782] step: 186899 Learning rate: 0.0002\n","Loss = 0.0077 (ave = 0.0034)\n","\n","epoch: [240/400] iteration: [2/782] step: 186900 Learning rate: 0.0002\n","Loss = 0.0043 (ave = 0.0034)\n","\n","epoch: [240/400] iteration: [102/782] step: 187000 Learning rate: 0.0002\n","Loss = 0.0023 (ave = 0.0034)\n","\n","epoch: [240/400] iteration: [202/782] step: 187100 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0034)\n","\n","epoch: [240/400] iteration: [302/782] step: 187200 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0034)\n","\n","epoch: [240/400] iteration: [402/782] step: 187300 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0034)\n","\n","epoch: [240/400] iteration: [502/782] step: 187400 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0034)\n","\n","epoch: [240/400] iteration: [602/782] step: 187500 Learning rate: 0.0002\n","Loss = 0.0023 (ave = 0.0034)\n","\n","epoch: [240/400] iteration: [702/782] step: 187600 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0034)\n","\n","epoch: [240/400] iteration: [782/782] step: 187681 Learning rate: 0.0002\n","Loss = 0.0027 (ave = 0.0034)\n","\n","epoch: [241/400] iteration: [20/782] step: 187700 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0034)\n","\n","epoch: [241/400] iteration: [120/782] step: 187800 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0034)\n","\n","epoch: [241/400] iteration: [220/782] step: 187900 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0034)\n","\n","epoch: [241/400] iteration: [320/782] step: 188000 Learning rate: 0.0002\n","Loss = 0.0023 (ave = 0.0034)\n","\n","epoch: [241/400] iteration: [420/782] step: 188100 Learning rate: 0.0002\n","Loss = 0.0023 (ave = 0.0034)\n","\n","epoch: [241/400] iteration: [520/782] step: 188200 Learning rate: 0.0002\n","Loss = 0.0023 (ave = 0.0034)\n","\n","epoch: [241/400] iteration: [620/782] step: 188300 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0034)\n","\n","epoch: [241/400] iteration: [720/782] step: 188400 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0034)\n","\n","epoch: [241/400] iteration: [782/782] step: 188463 Learning rate: 0.0002\n","Loss = 0.0029 (ave = 0.0034)\n","\n","epoch: [242/400] iteration: [38/782] step: 188500 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0034)\n","\n","epoch: [242/400] iteration: [138/782] step: 188600 Learning rate: 0.0002\n","Loss = 0.0025 (ave = 0.0034)\n","\n","epoch: [242/400] iteration: [238/782] step: 188700 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0034)\n","\n","epoch: [242/400] iteration: [338/782] step: 188800 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0034)\n","\n","epoch: [242/400] iteration: [438/782] step: 188900 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0034)\n","\n","epoch: [242/400] iteration: [538/782] step: 189000 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0033)\n","\n","epoch: [242/400] iteration: [638/782] step: 189100 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0033)\n","\n","epoch: [242/400] iteration: [738/782] step: 189200 Learning rate: 0.0002\n","Loss = 0.0024 (ave = 0.0033)\n","\n","epoch: [242/400] iteration: [782/782] step: 189245 Learning rate: 0.0002\n","Loss = 0.0027 (ave = 0.0033)\n","\n","epoch: [243/400] iteration: [56/782] step: 189300 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0033)\n","\n","epoch: [243/400] iteration: [156/782] step: 189400 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0033)\n","\n","epoch: [243/400] iteration: [256/782] step: 189500 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0033)\n","\n","epoch: [243/400] iteration: [356/782] step: 189600 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0033)\n","\n","epoch: [243/400] iteration: [456/782] step: 189700 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0033)\n","\n","epoch: [243/400] iteration: [556/782] step: 189800 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0033)\n","\n","epoch: [243/400] iteration: [656/782] step: 189900 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0033)\n","\n","epoch: [243/400] iteration: [756/782] step: 190000 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0033)\n","\n","epoch: [243/400] iteration: [782/782] step: 190027 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0033)\n","\n","epoch: [244/400] iteration: [74/782] step: 190100 Learning rate: 0.0002\n","Loss = 0.0024 (ave = 0.0033)\n","\n","epoch: [244/400] iteration: [174/782] step: 190200 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0033)\n","\n","epoch: [244/400] iteration: [274/782] step: 190300 Learning rate: 0.0002\n","Loss = 0.0023 (ave = 0.0033)\n","\n","epoch: [244/400] iteration: [374/782] step: 190400 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0033)\n","\n","epoch: [244/400] iteration: [474/782] step: 190500 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0033)\n","\n","epoch: [244/400] iteration: [574/782] step: 190600 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0033)\n","\n","epoch: [244/400] iteration: [674/782] step: 190700 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0033)\n","\n","epoch: [244/400] iteration: [774/782] step: 190800 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0033)\n","\n","epoch: [244/400] iteration: [782/782] step: 190809 Learning rate: 0.0002\n","Loss = 0.0047 (ave = 0.0033)\n","\n","epoch: [245/400] iteration: [92/782] step: 190900 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0033)\n","\n","epoch: [245/400] iteration: [192/782] step: 191000 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0033)\n","\n","epoch: [245/400] iteration: [292/782] step: 191100 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0033)\n","\n","epoch: [245/400] iteration: [392/782] step: 191200 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0033)\n","\n","epoch: [245/400] iteration: [492/782] step: 191300 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0033)\n","\n","epoch: [245/400] iteration: [592/782] step: 191400 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0033)\n","\n","epoch: [245/400] iteration: [692/782] step: 191500 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0033)\n","\n","epoch: [245/400] iteration: [782/782] step: 191591 Learning rate: 0.0002\n","Loss = 0.0029 (ave = 0.0033)\n","\n","epoch: [246/400] iteration: [10/782] step: 191600 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0033)\n","\n","epoch: [246/400] iteration: [110/782] step: 191700 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0033)\n","\n","epoch: [246/400] iteration: [210/782] step: 191800 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0033)\n","\n","epoch: [246/400] iteration: [310/782] step: 191900 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0033)\n","\n","epoch: [246/400] iteration: [410/782] step: 192000 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0033)\n","\n","epoch: [246/400] iteration: [510/782] step: 192100 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0033)\n","\n","epoch: [246/400] iteration: [610/782] step: 192200 Learning rate: 0.0002\n","Loss = 0.0023 (ave = 0.0033)\n","\n","epoch: [246/400] iteration: [710/782] step: 192300 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0033)\n","\n","epoch: [246/400] iteration: [782/782] step: 192373 Learning rate: 0.0002\n","Loss = 0.0028 (ave = 0.0033)\n","\n","epoch: [247/400] iteration: [28/782] step: 192400 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0033)\n","\n","epoch: [247/400] iteration: [128/782] step: 192500 Learning rate: 0.0002\n","Loss = 0.0024 (ave = 0.0033)\n","\n","epoch: [247/400] iteration: [228/782] step: 192600 Learning rate: 0.0002\n","Loss = 0.0025 (ave = 0.0033)\n","\n","epoch: [247/400] iteration: [328/782] step: 192700 Learning rate: 0.0002\n","Loss = 0.0023 (ave = 0.0033)\n","\n","epoch: [247/400] iteration: [428/782] step: 192800 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0033)\n","\n","epoch: [247/400] iteration: [528/782] step: 192900 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0033)\n","\n","epoch: [247/400] iteration: [628/782] step: 193000 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0033)\n","\n","epoch: [247/400] iteration: [728/782] step: 193100 Learning rate: 0.0002\n","Loss = 0.0023 (ave = 0.0033)\n","\n","epoch: [247/400] iteration: [782/782] step: 193155 Learning rate: 0.0002\n","Loss = 0.0036 (ave = 0.0033)\n","\n","epoch: [248/400] iteration: [46/782] step: 193200 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0033)\n","\n","epoch: [248/400] iteration: [146/782] step: 193300 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0033)\n","\n","epoch: [248/400] iteration: [246/782] step: 193400 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0033)\n","\n","epoch: [248/400] iteration: [346/782] step: 193500 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0033)\n","\n","epoch: [248/400] iteration: [446/782] step: 193600 Learning rate: 0.0002\n","Loss = 0.0023 (ave = 0.0033)\n","\n","epoch: [248/400] iteration: [546/782] step: 193700 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0033)\n","\n","epoch: [248/400] iteration: [646/782] step: 193800 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0033)\n","\n","epoch: [248/400] iteration: [746/782] step: 193900 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0033)\n","\n","epoch: [248/400] iteration: [782/782] step: 193937 Learning rate: 0.0002\n","Loss = 0.0025 (ave = 0.0033)\n","\n","epoch: [249/400] iteration: [64/782] step: 194000 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0033)\n","\n","epoch: [249/400] iteration: [164/782] step: 194100 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0033)\n","\n","epoch: [249/400] iteration: [264/782] step: 194200 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0033)\n","\n","epoch: [249/400] iteration: [364/782] step: 194300 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0033)\n","\n","epoch: [249/400] iteration: [464/782] step: 194400 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0033)\n","\n","epoch: [249/400] iteration: [564/782] step: 194500 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0033)\n","\n","epoch: [249/400] iteration: [664/782] step: 194600 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0033)\n","\n","epoch: [249/400] iteration: [764/782] step: 194700 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0033)\n","\n","epoch: [249/400] iteration: [782/782] step: 194719 Learning rate: 0.0002\n","Loss = 0.0028 (ave = 0.0033)\n","\n","epoch: [250/400] iteration: [82/782] step: 194800 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0033)\n","\n","epoch: [250/400] iteration: [182/782] step: 194900 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0033)\n","\n","epoch: [250/400] iteration: [282/782] step: 195000 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0033)\n","\n","epoch: [250/400] iteration: [382/782] step: 195100 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0033)\n","\n","epoch: [250/400] iteration: [482/782] step: 195200 Learning rate: 0.0002\n","Loss = 0.0023 (ave = 0.0033)\n","\n","epoch: [250/400] iteration: [582/782] step: 195300 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0033)\n","\n","epoch: [250/400] iteration: [682/782] step: 195400 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0033)\n","\n","epoch: [250/400] iteration: [782/782] step: 195500 Learning rate: 0.0002\n","Loss = 0.0051 (ave = 0.0033)\n","\n","epoch: [250/400] iteration: [782/782] step: 195501 Learning rate: 0.0002\n","Loss = 0.0051 (ave = 0.0033)\n","\n","epoch: [251/400] iteration: [100/782] step: 195600 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0033)\n","\n","epoch: [251/400] iteration: [200/782] step: 195700 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0033)\n","\n","epoch: [251/400] iteration: [300/782] step: 195800 Learning rate: 0.0002\n","Loss = 0.0024 (ave = 0.0033)\n","\n","epoch: [251/400] iteration: [400/782] step: 195900 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0033)\n","\n","epoch: [251/400] iteration: [500/782] step: 196000 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0033)\n","\n","epoch: [251/400] iteration: [600/782] step: 196100 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0033)\n","\n","epoch: [251/400] iteration: [700/782] step: 196200 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0033)\n","\n","epoch: [251/400] iteration: [782/782] step: 196283 Learning rate: 0.0002\n","Loss = 0.0040 (ave = 0.0033)\n","\n","epoch: [252/400] iteration: [18/782] step: 196300 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0033)\n","\n","epoch: [252/400] iteration: [118/782] step: 196400 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0033)\n","\n","epoch: [252/400] iteration: [218/782] step: 196500 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0033)\n","\n","epoch: [252/400] iteration: [318/782] step: 196600 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0033)\n","\n","epoch: [252/400] iteration: [418/782] step: 196700 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0033)\n","\n","epoch: [252/400] iteration: [518/782] step: 196800 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0033)\n","\n","epoch: [252/400] iteration: [618/782] step: 196900 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0033)\n","\n","epoch: [252/400] iteration: [718/782] step: 197000 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0033)\n","\n","epoch: [252/400] iteration: [782/782] step: 197065 Learning rate: 0.0002\n","Loss = 0.0030 (ave = 0.0033)\n","\n","epoch: [253/400] iteration: [36/782] step: 197100 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0033)\n","\n","epoch: [253/400] iteration: [136/782] step: 197200 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0033)\n","\n","epoch: [253/400] iteration: [236/782] step: 197300 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0033)\n","\n","epoch: [253/400] iteration: [336/782] step: 197400 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0033)\n","\n","epoch: [253/400] iteration: [436/782] step: 197500 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0033)\n","\n","epoch: [253/400] iteration: [536/782] step: 197600 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0033)\n","\n","epoch: [253/400] iteration: [636/782] step: 197700 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0033)\n","\n","epoch: [253/400] iteration: [736/782] step: 197800 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0033)\n","\n","epoch: [253/400] iteration: [782/782] step: 197847 Learning rate: 0.0002\n","Loss = 0.0031 (ave = 0.0033)\n","\n","epoch: [254/400] iteration: [54/782] step: 197900 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0033)\n","\n","epoch: [254/400] iteration: [154/782] step: 198000 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0033)\n","\n","epoch: [254/400] iteration: [254/782] step: 198100 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0033)\n","\n","epoch: [254/400] iteration: [354/782] step: 198200 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0033)\n","\n","epoch: [254/400] iteration: [454/782] step: 198300 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0033)\n","\n","epoch: [254/400] iteration: [554/782] step: 198400 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0033)\n","\n","epoch: [254/400] iteration: [654/782] step: 198500 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0033)\n","\n","epoch: [254/400] iteration: [754/782] step: 198600 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0033)\n","\n","epoch: [254/400] iteration: [782/782] step: 198629 Learning rate: 0.0002\n","Loss = 0.0032 (ave = 0.0033)\n","\n","epoch: [255/400] iteration: [72/782] step: 198700 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0033)\n","\n","epoch: [255/400] iteration: [172/782] step: 198800 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0033)\n","\n","epoch: [255/400] iteration: [272/782] step: 198900 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0033)\n","\n","epoch: [255/400] iteration: [372/782] step: 199000 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0033)\n","\n","epoch: [255/400] iteration: [472/782] step: 199100 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0033)\n","\n","epoch: [255/400] iteration: [572/782] step: 199200 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0033)\n","\n","epoch: [255/400] iteration: [672/782] step: 199300 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0033)\n","\n","epoch: [255/400] iteration: [772/782] step: 199400 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0033)\n","\n","epoch: [255/400] iteration: [782/782] step: 199411 Learning rate: 0.0002\n","Loss = 0.0030 (ave = 0.0033)\n","\n","epoch: [256/400] iteration: [90/782] step: 199500 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0033)\n","\n","epoch: [256/400] iteration: [190/782] step: 199600 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0033)\n","\n","epoch: [256/400] iteration: [290/782] step: 199700 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0033)\n","\n","epoch: [256/400] iteration: [390/782] step: 199800 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0033)\n","\n","epoch: [256/400] iteration: [490/782] step: 199900 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0033)\n","\n","epoch: [256/400] iteration: [590/782] step: 200000 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0033)\n","\n","epoch: [256/400] iteration: [690/782] step: 200100 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0033)\n","\n","epoch: [256/400] iteration: [782/782] step: 200193 Learning rate: 0.0002\n","Loss = 0.0029 (ave = 0.0033)\n","\n","epoch: [257/400] iteration: [8/782] step: 200200 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0033)\n","\n","epoch: [257/400] iteration: [108/782] step: 200300 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0033)\n","\n","epoch: [257/400] iteration: [208/782] step: 200400 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0033)\n","\n","epoch: [257/400] iteration: [308/782] step: 200500 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0033)\n","\n","epoch: [257/400] iteration: [408/782] step: 200600 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0033)\n","\n","epoch: [257/400] iteration: [508/782] step: 200700 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0033)\n","\n","epoch: [257/400] iteration: [608/782] step: 200800 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0033)\n","\n","epoch: [257/400] iteration: [708/782] step: 200900 Learning rate: 0.0002\n","Loss = 0.0024 (ave = 0.0033)\n","\n","epoch: [257/400] iteration: [782/782] step: 200975 Learning rate: 0.0002\n","Loss = 0.0029 (ave = 0.0033)\n","\n","epoch: [258/400] iteration: [26/782] step: 201000 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0033)\n","\n","epoch: [258/400] iteration: [126/782] step: 201100 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0033)\n","\n","epoch: [258/400] iteration: [226/782] step: 201200 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0033)\n","\n","epoch: [258/400] iteration: [326/782] step: 201300 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0033)\n","\n","epoch: [258/400] iteration: [426/782] step: 201400 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0033)\n","\n","epoch: [258/400] iteration: [526/782] step: 201500 Learning rate: 0.0002\n","Loss = 0.0023 (ave = 0.0033)\n","\n","epoch: [258/400] iteration: [626/782] step: 201600 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0033)\n","\n","epoch: [258/400] iteration: [726/782] step: 201700 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0033)\n","\n","epoch: [258/400] iteration: [782/782] step: 201757 Learning rate: 0.0002\n","Loss = 0.0036 (ave = 0.0033)\n","\n","epoch: [259/400] iteration: [44/782] step: 201800 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0033)\n","\n","epoch: [259/400] iteration: [144/782] step: 201900 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0033)\n","\n","epoch: [259/400] iteration: [244/782] step: 202000 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0033)\n","\n","epoch: [259/400] iteration: [344/782] step: 202100 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0033)\n","\n","epoch: [259/400] iteration: [444/782] step: 202200 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0033)\n","\n","epoch: [259/400] iteration: [544/782] step: 202300 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0033)\n","\n","epoch: [259/400] iteration: [644/782] step: 202400 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0033)\n","\n","epoch: [259/400] iteration: [744/782] step: 202500 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0033)\n","\n","epoch: [259/400] iteration: [782/782] step: 202539 Learning rate: 0.0002\n","Loss = 0.0031 (ave = 0.0033)\n","\n","epoch: [260/400] iteration: [62/782] step: 202600 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0033)\n","\n","epoch: [260/400] iteration: [162/782] step: 202700 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0033)\n","\n","epoch: [260/400] iteration: [262/782] step: 202800 Learning rate: 0.0002\n","Loss = 0.0023 (ave = 0.0033)\n","\n","epoch: [260/400] iteration: [362/782] step: 202900 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0033)\n","\n","epoch: [260/400] iteration: [462/782] step: 203000 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0033)\n","\n","epoch: [260/400] iteration: [562/782] step: 203100 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0033)\n","\n","epoch: [260/400] iteration: [662/782] step: 203200 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0033)\n","\n","epoch: [260/400] iteration: [762/782] step: 203300 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0033)\n","\n","epoch: [260/400] iteration: [782/782] step: 203321 Learning rate: 0.0002\n","Loss = 0.0035 (ave = 0.0033)\n","\n","epoch: [261/400] iteration: [80/782] step: 203400 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0033)\n","\n","epoch: [261/400] iteration: [180/782] step: 203500 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0033)\n","\n","epoch: [261/400] iteration: [280/782] step: 203600 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0033)\n","\n","epoch: [261/400] iteration: [380/782] step: 203700 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0033)\n","\n","epoch: [261/400] iteration: [480/782] step: 203800 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0033)\n","\n","epoch: [261/400] iteration: [580/782] step: 203900 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0033)\n","\n","epoch: [261/400] iteration: [680/782] step: 204000 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0033)\n","\n","epoch: [261/400] iteration: [780/782] step: 204100 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0033)\n","\n","epoch: [261/400] iteration: [782/782] step: 204103 Learning rate: 0.0002\n","Loss = 0.0025 (ave = 0.0033)\n","\n","epoch: [262/400] iteration: [98/782] step: 204200 Learning rate: 0.0002\n","Loss = 0.0023 (ave = 0.0033)\n","\n","epoch: [262/400] iteration: [198/782] step: 204300 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0033)\n","\n","epoch: [262/400] iteration: [298/782] step: 204400 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0033)\n","\n","epoch: [262/400] iteration: [398/782] step: 204500 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0033)\n","\n","epoch: [262/400] iteration: [498/782] step: 204600 Learning rate: 0.0002\n","Loss = 0.0025 (ave = 0.0033)\n","\n","epoch: [262/400] iteration: [598/782] step: 204700 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0033)\n","\n","epoch: [262/400] iteration: [698/782] step: 204800 Learning rate: 0.0002\n","Loss = 0.0024 (ave = 0.0033)\n","\n","epoch: [262/400] iteration: [782/782] step: 204885 Learning rate: 0.0002\n","Loss = 0.0027 (ave = 0.0033)\n","\n","epoch: [263/400] iteration: [16/782] step: 204900 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0033)\n","\n","epoch: [263/400] iteration: [116/782] step: 205000 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0033)\n","\n","epoch: [263/400] iteration: [216/782] step: 205100 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0033)\n","\n","epoch: [263/400] iteration: [316/782] step: 205200 Learning rate: 0.0002\n","Loss = 0.0024 (ave = 0.0033)\n","\n","epoch: [263/400] iteration: [416/782] step: 205300 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0033)\n","\n","epoch: [263/400] iteration: [516/782] step: 205400 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0033)\n","\n","epoch: [263/400] iteration: [616/782] step: 205500 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0033)\n","\n","epoch: [263/400] iteration: [716/782] step: 205600 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0033)\n","\n","epoch: [263/400] iteration: [782/782] step: 205667 Learning rate: 0.0002\n","Loss = 0.0028 (ave = 0.0033)\n","\n","epoch: [264/400] iteration: [34/782] step: 205700 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0033)\n","\n","epoch: [264/400] iteration: [134/782] step: 205800 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0033)\n","\n","epoch: [264/400] iteration: [234/782] step: 205900 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0033)\n","\n","epoch: [264/400] iteration: [334/782] step: 206000 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0032)\n","\n","epoch: [264/400] iteration: [434/782] step: 206100 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0032)\n","\n","epoch: [264/400] iteration: [534/782] step: 206200 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0032)\n","\n","epoch: [264/400] iteration: [634/782] step: 206300 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0032)\n","\n","epoch: [264/400] iteration: [734/782] step: 206400 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0032)\n","\n","epoch: [264/400] iteration: [782/782] step: 206449 Learning rate: 0.0002\n","Loss = 0.0036 (ave = 0.0032)\n","\n","epoch: [265/400] iteration: [52/782] step: 206500 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0032)\n","\n","epoch: [265/400] iteration: [152/782] step: 206600 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0032)\n","\n","epoch: [265/400] iteration: [252/782] step: 206700 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0032)\n","\n","epoch: [265/400] iteration: [352/782] step: 206800 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0032)\n","\n","epoch: [265/400] iteration: [452/782] step: 206900 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0032)\n","\n","epoch: [265/400] iteration: [552/782] step: 207000 Learning rate: 0.0002\n","Loss = 0.0023 (ave = 0.0032)\n","\n","epoch: [265/400] iteration: [652/782] step: 207100 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0032)\n","\n","epoch: [265/400] iteration: [752/782] step: 207200 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0032)\n","\n","epoch: [265/400] iteration: [782/782] step: 207231 Learning rate: 0.0002\n","Loss = 0.0032 (ave = 0.0032)\n","\n","epoch: [266/400] iteration: [70/782] step: 207300 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0032)\n","\n","epoch: [266/400] iteration: [170/782] step: 207400 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0032)\n","\n","epoch: [266/400] iteration: [270/782] step: 207500 Learning rate: 0.0002\n","Loss = 0.0023 (ave = 0.0032)\n","\n","epoch: [266/400] iteration: [370/782] step: 207600 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0032)\n","\n","epoch: [266/400] iteration: [470/782] step: 207700 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0032)\n","\n","epoch: [266/400] iteration: [570/782] step: 207800 Learning rate: 0.0002\n","Loss = 0.0023 (ave = 0.0032)\n","\n","epoch: [266/400] iteration: [670/782] step: 207900 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0032)\n","\n","epoch: [266/400] iteration: [770/782] step: 208000 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0032)\n","\n","epoch: [266/400] iteration: [782/782] step: 208013 Learning rate: 0.0002\n","Loss = 0.0034 (ave = 0.0032)\n","\n","epoch: [267/400] iteration: [88/782] step: 208100 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0032)\n","\n","epoch: [267/400] iteration: [188/782] step: 208200 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0032)\n","\n","epoch: [267/400] iteration: [288/782] step: 208300 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0032)\n","\n","epoch: [267/400] iteration: [388/782] step: 208400 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0032)\n","\n","epoch: [267/400] iteration: [488/782] step: 208500 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0032)\n","\n","epoch: [267/400] iteration: [588/782] step: 208600 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0032)\n","\n","epoch: [267/400] iteration: [688/782] step: 208700 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0032)\n","\n","epoch: [267/400] iteration: [782/782] step: 208795 Learning rate: 0.0002\n","Loss = 0.0027 (ave = 0.0032)\n","\n","epoch: [268/400] iteration: [6/782] step: 208800 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0032)\n","\n","epoch: [268/400] iteration: [106/782] step: 208900 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0032)\n","\n","epoch: [268/400] iteration: [206/782] step: 209000 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0032)\n","\n","epoch: [268/400] iteration: [306/782] step: 209100 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0032)\n","\n","epoch: [268/400] iteration: [406/782] step: 209200 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0032)\n","\n","epoch: [268/400] iteration: [506/782] step: 209300 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0032)\n","\n","epoch: [268/400] iteration: [606/782] step: 209400 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0032)\n","\n","epoch: [268/400] iteration: [706/782] step: 209500 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0032)\n","\n","epoch: [268/400] iteration: [782/782] step: 209577 Learning rate: 0.0002\n","Loss = 0.0029 (ave = 0.0032)\n","\n","epoch: [269/400] iteration: [24/782] step: 209600 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0032)\n","\n","epoch: [269/400] iteration: [124/782] step: 209700 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0032)\n","\n","epoch: [269/400] iteration: [224/782] step: 209800 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0032)\n","\n","epoch: [269/400] iteration: [324/782] step: 209900 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0032)\n","\n","epoch: [269/400] iteration: [424/782] step: 210000 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0032)\n","\n","epoch: [269/400] iteration: [524/782] step: 210100 Learning rate: 0.0002\n","Loss = 0.0024 (ave = 0.0032)\n","\n","epoch: [269/400] iteration: [624/782] step: 210200 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0032)\n","\n","epoch: [269/400] iteration: [724/782] step: 210300 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0032)\n","\n","epoch: [269/400] iteration: [782/782] step: 210359 Learning rate: 0.0002\n","Loss = 0.0044 (ave = 0.0032)\n","\n","epoch: [270/400] iteration: [42/782] step: 210400 Learning rate: 0.0002\n","Loss = 0.0023 (ave = 0.0032)\n","\n","epoch: [270/400] iteration: [142/782] step: 210500 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0032)\n","\n","epoch: [270/400] iteration: [242/782] step: 210600 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0032)\n","\n","epoch: [270/400] iteration: [342/782] step: 210700 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0032)\n","\n","epoch: [270/400] iteration: [442/782] step: 210800 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0032)\n","\n","epoch: [270/400] iteration: [542/782] step: 210900 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0032)\n","\n","epoch: [270/400] iteration: [642/782] step: 211000 Learning rate: 0.0002\n","Loss = 0.0023 (ave = 0.0032)\n","\n","epoch: [270/400] iteration: [742/782] step: 211100 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0032)\n","\n","epoch: [270/400] iteration: [782/782] step: 211141 Learning rate: 0.0002\n","Loss = 0.0023 (ave = 0.0032)\n","\n","epoch: [271/400] iteration: [60/782] step: 211200 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0032)\n","\n","epoch: [271/400] iteration: [160/782] step: 211300 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0032)\n","\n","epoch: [271/400] iteration: [260/782] step: 211400 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0032)\n","\n","epoch: [271/400] iteration: [360/782] step: 211500 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0032)\n","\n","epoch: [271/400] iteration: [460/782] step: 211600 Learning rate: 0.0002\n","Loss = 0.0024 (ave = 0.0032)\n","\n","epoch: [271/400] iteration: [560/782] step: 211700 Learning rate: 0.0002\n","Loss = 0.0038 (ave = 0.0032)\n","\n","epoch: [271/400] iteration: [660/782] step: 211800 Learning rate: 0.0002\n","Loss = 0.0023 (ave = 0.0032)\n","\n","epoch: [271/400] iteration: [760/782] step: 211900 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0032)\n","\n","epoch: [271/400] iteration: [782/782] step: 211923 Learning rate: 0.0002\n","Loss = 0.0029 (ave = 0.0032)\n","\n","epoch: [272/400] iteration: [78/782] step: 212000 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0032)\n","\n","epoch: [272/400] iteration: [178/782] step: 212100 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0032)\n","\n","epoch: [272/400] iteration: [278/782] step: 212200 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0032)\n","\n","epoch: [272/400] iteration: [378/782] step: 212300 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0032)\n","\n","epoch: [272/400] iteration: [478/782] step: 212400 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0032)\n","\n","epoch: [272/400] iteration: [578/782] step: 212500 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0032)\n","\n","epoch: [272/400] iteration: [678/782] step: 212600 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0032)\n","\n","epoch: [272/400] iteration: [778/782] step: 212700 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0032)\n","\n","epoch: [272/400] iteration: [782/782] step: 212705 Learning rate: 0.0002\n","Loss = 0.0025 (ave = 0.0032)\n","\n","epoch: [273/400] iteration: [96/782] step: 212800 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0032)\n","\n","epoch: [273/400] iteration: [196/782] step: 212900 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0032)\n","\n","epoch: [273/400] iteration: [296/782] step: 213000 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0032)\n","\n","epoch: [273/400] iteration: [396/782] step: 213100 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0032)\n","\n","epoch: [273/400] iteration: [496/782] step: 213200 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0032)\n","\n","epoch: [273/400] iteration: [596/782] step: 213300 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0032)\n","\n","epoch: [273/400] iteration: [696/782] step: 213400 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0032)\n","\n","epoch: [273/400] iteration: [782/782] step: 213487 Learning rate: 0.0002\n","Loss = 0.0024 (ave = 0.0032)\n","\n","epoch: [274/400] iteration: [14/782] step: 213500 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0032)\n","\n","epoch: [274/400] iteration: [114/782] step: 213600 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0032)\n","\n","epoch: [274/400] iteration: [214/782] step: 213700 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0032)\n","\n","epoch: [274/400] iteration: [314/782] step: 213800 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0032)\n","\n","epoch: [274/400] iteration: [414/782] step: 213900 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0032)\n","\n","epoch: [274/400] iteration: [514/782] step: 214000 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0032)\n","\n","epoch: [274/400] iteration: [614/782] step: 214100 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0032)\n","\n","epoch: [274/400] iteration: [714/782] step: 214200 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0032)\n","\n","epoch: [274/400] iteration: [782/782] step: 214269 Learning rate: 0.0002\n","Loss = 0.0034 (ave = 0.0032)\n","\n","epoch: [275/400] iteration: [32/782] step: 214300 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0032)\n","\n","epoch: [275/400] iteration: [132/782] step: 214400 Learning rate: 0.0002\n","Loss = 0.0025 (ave = 0.0032)\n","\n","epoch: [275/400] iteration: [232/782] step: 214500 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0032)\n","\n","epoch: [275/400] iteration: [332/782] step: 214600 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0032)\n","\n","epoch: [275/400] iteration: [432/782] step: 214700 Learning rate: 0.0002\n","Loss = 0.0023 (ave = 0.0032)\n","\n","epoch: [275/400] iteration: [532/782] step: 214800 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0032)\n","\n","epoch: [275/400] iteration: [632/782] step: 214900 Learning rate: 0.0002\n","Loss = 0.0023 (ave = 0.0032)\n","\n","epoch: [275/400] iteration: [732/782] step: 215000 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0032)\n","\n","epoch: [275/400] iteration: [782/782] step: 215051 Learning rate: 0.0002\n","Loss = 0.0032 (ave = 0.0032)\n","\n","epoch: [276/400] iteration: [50/782] step: 215100 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0032)\n","\n","epoch: [276/400] iteration: [150/782] step: 215200 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0032)\n","\n","epoch: [276/400] iteration: [250/782] step: 215300 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0032)\n","\n","epoch: [276/400] iteration: [350/782] step: 215400 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0032)\n","\n","epoch: [276/400] iteration: [450/782] step: 215500 Learning rate: 0.0002\n","Loss = 0.0023 (ave = 0.0032)\n","\n","epoch: [276/400] iteration: [550/782] step: 215600 Learning rate: 0.0002\n","Loss = 0.0023 (ave = 0.0032)\n","\n","epoch: [276/400] iteration: [650/782] step: 215700 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0032)\n","\n","epoch: [276/400] iteration: [750/782] step: 215800 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0032)\n","\n","epoch: [276/400] iteration: [782/782] step: 215833 Learning rate: 0.0002\n","Loss = 0.0027 (ave = 0.0032)\n","\n","epoch: [277/400] iteration: [68/782] step: 215900 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0032)\n","\n","epoch: [277/400] iteration: [168/782] step: 216000 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0032)\n","\n","epoch: [277/400] iteration: [268/782] step: 216100 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0032)\n","\n","epoch: [277/400] iteration: [368/782] step: 216200 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0032)\n","\n","epoch: [277/400] iteration: [468/782] step: 216300 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0032)\n","\n","epoch: [277/400] iteration: [568/782] step: 216400 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0032)\n","\n","epoch: [277/400] iteration: [668/782] step: 216500 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0032)\n","\n","epoch: [277/400] iteration: [768/782] step: 216600 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0032)\n","\n","epoch: [277/400] iteration: [782/782] step: 216615 Learning rate: 0.0002\n","Loss = 0.0027 (ave = 0.0032)\n","\n","epoch: [278/400] iteration: [86/782] step: 216700 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0032)\n","\n","epoch: [278/400] iteration: [186/782] step: 216800 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0032)\n","\n","epoch: [278/400] iteration: [286/782] step: 216900 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0032)\n","\n","epoch: [278/400] iteration: [386/782] step: 217000 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0032)\n","\n","epoch: [278/400] iteration: [486/782] step: 217100 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0032)\n","\n","epoch: [278/400] iteration: [586/782] step: 217200 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0032)\n","\n","epoch: [278/400] iteration: [686/782] step: 217300 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0032)\n","\n","epoch: [278/400] iteration: [782/782] step: 217397 Learning rate: 0.0002\n","Loss = 0.0031 (ave = 0.0032)\n","\n","epoch: [279/400] iteration: [4/782] step: 217400 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0032)\n","\n","epoch: [279/400] iteration: [104/782] step: 217500 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0032)\n","\n","epoch: [279/400] iteration: [204/782] step: 217600 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0032)\n","\n","epoch: [279/400] iteration: [304/782] step: 217700 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0032)\n","\n","epoch: [279/400] iteration: [404/782] step: 217800 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0032)\n","\n","epoch: [279/400] iteration: [504/782] step: 217900 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0032)\n","\n","epoch: [279/400] iteration: [604/782] step: 218000 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0032)\n","\n","epoch: [279/400] iteration: [704/782] step: 218100 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0032)\n","\n","epoch: [279/400] iteration: [782/782] step: 218179 Learning rate: 0.0002\n","Loss = 0.0024 (ave = 0.0032)\n","\n","epoch: [280/400] iteration: [22/782] step: 218200 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0032)\n","\n","epoch: [280/400] iteration: [122/782] step: 218300 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0032)\n","\n","epoch: [280/400] iteration: [222/782] step: 218400 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0032)\n","\n","epoch: [280/400] iteration: [322/782] step: 218500 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0032)\n","\n","epoch: [280/400] iteration: [422/782] step: 218600 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0032)\n","\n","epoch: [280/400] iteration: [522/782] step: 218700 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0032)\n","\n","epoch: [280/400] iteration: [622/782] step: 218800 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0032)\n","\n","epoch: [280/400] iteration: [722/782] step: 218900 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0032)\n","\n","epoch: [280/400] iteration: [782/782] step: 218961 Learning rate: 0.0002\n","Loss = 0.0034 (ave = 0.0032)\n","\n","epoch: [281/400] iteration: [40/782] step: 219000 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0032)\n","\n","epoch: [281/400] iteration: [140/782] step: 219100 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0032)\n","\n","epoch: [281/400] iteration: [240/782] step: 219200 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0032)\n","\n","epoch: [281/400] iteration: [340/782] step: 219300 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0032)\n","\n","epoch: [281/400] iteration: [440/782] step: 219400 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0032)\n","\n","epoch: [281/400] iteration: [540/782] step: 219500 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0032)\n","\n","epoch: [281/400] iteration: [640/782] step: 219600 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0032)\n","\n","epoch: [281/400] iteration: [740/782] step: 219700 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0032)\n","\n","epoch: [281/400] iteration: [782/782] step: 219743 Learning rate: 0.0002\n","Loss = 0.0036 (ave = 0.0032)\n","\n","epoch: [282/400] iteration: [58/782] step: 219800 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0032)\n","\n","epoch: [282/400] iteration: [158/782] step: 219900 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0032)\n","\n","epoch: [282/400] iteration: [258/782] step: 220000 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0032)\n","\n","epoch: [282/400] iteration: [358/782] step: 220100 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0032)\n","\n","epoch: [282/400] iteration: [458/782] step: 220200 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0032)\n","\n","epoch: [282/400] iteration: [558/782] step: 220300 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0032)\n","\n","epoch: [282/400] iteration: [658/782] step: 220400 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0032)\n","\n","epoch: [282/400] iteration: [758/782] step: 220500 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0032)\n","\n","epoch: [282/400] iteration: [782/782] step: 220525 Learning rate: 0.0002\n","Loss = 0.0028 (ave = 0.0032)\n","\n","epoch: [283/400] iteration: [76/782] step: 220600 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0032)\n","\n","epoch: [283/400] iteration: [176/782] step: 220700 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0032)\n","\n","epoch: [283/400] iteration: [276/782] step: 220800 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0032)\n","\n","epoch: [283/400] iteration: [376/782] step: 220900 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0032)\n","\n","epoch: [283/400] iteration: [476/782] step: 221000 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0032)\n","\n","epoch: [283/400] iteration: [576/782] step: 221100 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0032)\n","\n","epoch: [283/400] iteration: [676/782] step: 221200 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0032)\n","\n","epoch: [283/400] iteration: [776/782] step: 221300 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0032)\n","\n","epoch: [283/400] iteration: [782/782] step: 221307 Learning rate: 0.0002\n","Loss = 0.0026 (ave = 0.0032)\n","\n","epoch: [284/400] iteration: [94/782] step: 221400 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0032)\n","\n","epoch: [284/400] iteration: [194/782] step: 221500 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0032)\n","\n","epoch: [284/400] iteration: [294/782] step: 221600 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0032)\n","\n","epoch: [284/400] iteration: [394/782] step: 221700 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0032)\n","\n","epoch: [284/400] iteration: [494/782] step: 221800 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0032)\n","\n","epoch: [284/400] iteration: [594/782] step: 221900 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0032)\n","\n","epoch: [284/400] iteration: [694/782] step: 222000 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0032)\n","\n","epoch: [284/400] iteration: [782/782] step: 222089 Learning rate: 0.0002\n","Loss = 0.0027 (ave = 0.0032)\n","\n","epoch: [285/400] iteration: [12/782] step: 222100 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0032)\n","\n","epoch: [285/400] iteration: [112/782] step: 222200 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0032)\n","\n","epoch: [285/400] iteration: [212/782] step: 222300 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0032)\n","\n","epoch: [285/400] iteration: [312/782] step: 222400 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0032)\n","\n","epoch: [285/400] iteration: [412/782] step: 222500 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0032)\n","\n","epoch: [285/400] iteration: [512/782] step: 222600 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0032)\n","\n","epoch: [285/400] iteration: [612/782] step: 222700 Learning rate: 0.0002\n","Loss = 0.0023 (ave = 0.0032)\n","\n","epoch: [285/400] iteration: [712/782] step: 222800 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0032)\n","\n","epoch: [285/400] iteration: [782/782] step: 222871 Learning rate: 0.0002\n","Loss = 0.0031 (ave = 0.0032)\n","\n","epoch: [286/400] iteration: [30/782] step: 222900 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0032)\n","\n","epoch: [286/400] iteration: [130/782] step: 223000 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0032)\n","\n","epoch: [286/400] iteration: [230/782] step: 223100 Learning rate: 0.0002\n","Loss = 0.0023 (ave = 0.0032)\n","\n","epoch: [286/400] iteration: [330/782] step: 223200 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0032)\n","\n","epoch: [286/400] iteration: [430/782] step: 223300 Learning rate: 0.0002\n","Loss = 0.0023 (ave = 0.0032)\n","\n","epoch: [286/400] iteration: [530/782] step: 223400 Learning rate: 0.0002\n","Loss = 0.0023 (ave = 0.0032)\n","\n","epoch: [286/400] iteration: [630/782] step: 223500 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0032)\n","\n","epoch: [286/400] iteration: [730/782] step: 223600 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0032)\n","\n","epoch: [286/400] iteration: [782/782] step: 223653 Learning rate: 0.0002\n","Loss = 0.0028 (ave = 0.0032)\n","\n","epoch: [287/400] iteration: [48/782] step: 223700 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0032)\n","\n","epoch: [287/400] iteration: [148/782] step: 223800 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0032)\n","\n","epoch: [287/400] iteration: [248/782] step: 223900 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0032)\n","\n","epoch: [287/400] iteration: [348/782] step: 224000 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0032)\n","\n","epoch: [287/400] iteration: [448/782] step: 224100 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0032)\n","\n","epoch: [287/400] iteration: [548/782] step: 224200 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0032)\n","\n","epoch: [287/400] iteration: [648/782] step: 224300 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0032)\n","\n","epoch: [287/400] iteration: [748/782] step: 224400 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0032)\n","\n","epoch: [287/400] iteration: [782/782] step: 224435 Learning rate: 0.0002\n","Loss = 0.0033 (ave = 0.0032)\n","\n","epoch: [288/400] iteration: [66/782] step: 224500 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0032)\n","\n","epoch: [288/400] iteration: [166/782] step: 224600 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0032)\n","\n","epoch: [288/400] iteration: [266/782] step: 224700 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0032)\n","\n","epoch: [288/400] iteration: [366/782] step: 224800 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0032)\n","\n","epoch: [288/400] iteration: [466/782] step: 224900 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0032)\n","\n","epoch: [288/400] iteration: [566/782] step: 225000 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0032)\n","\n","epoch: [288/400] iteration: [666/782] step: 225100 Learning rate: 0.0002\n","Loss = 0.0024 (ave = 0.0032)\n","\n","epoch: [288/400] iteration: [766/782] step: 225200 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0031)\n","\n","epoch: [288/400] iteration: [782/782] step: 225217 Learning rate: 0.0002\n","Loss = 0.0029 (ave = 0.0031)\n","\n","epoch: [289/400] iteration: [84/782] step: 225300 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0031)\n","\n","epoch: [289/400] iteration: [184/782] step: 225400 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0031)\n","\n","epoch: [289/400] iteration: [284/782] step: 225500 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0031)\n","\n","epoch: [289/400] iteration: [384/782] step: 225600 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0031)\n","\n","epoch: [289/400] iteration: [484/782] step: 225700 Learning rate: 0.0002\n","Loss = 0.0023 (ave = 0.0031)\n","\n","epoch: [289/400] iteration: [584/782] step: 225800 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0031)\n","\n","epoch: [289/400] iteration: [684/782] step: 225900 Learning rate: 0.0002\n","Loss = 0.0018 (ave = 0.0031)\n","\n","epoch: [289/400] iteration: [782/782] step: 225999 Learning rate: 0.0002\n","Loss = 0.0024 (ave = 0.0031)\n","\n","epoch: [290/400] iteration: [2/782] step: 226000 Learning rate: 0.0002\n","Loss = 0.0026 (ave = 0.0031)\n","\n","epoch: [290/400] iteration: [102/782] step: 226100 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0031)\n","\n","epoch: [290/400] iteration: [202/782] step: 226200 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0031)\n","\n","epoch: [290/400] iteration: [302/782] step: 226300 Learning rate: 0.0002\n","Loss = 0.0023 (ave = 0.0031)\n","\n","epoch: [290/400] iteration: [402/782] step: 226400 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0031)\n","\n","epoch: [290/400] iteration: [502/782] step: 226500 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0031)\n","\n","epoch: [290/400] iteration: [602/782] step: 226600 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0031)\n","\n","epoch: [290/400] iteration: [702/782] step: 226700 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0031)\n","\n","epoch: [290/400] iteration: [782/782] step: 226781 Learning rate: 0.0002\n","Loss = 0.0025 (ave = 0.0031)\n","\n","epoch: [291/400] iteration: [20/782] step: 226800 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0031)\n","\n","epoch: [291/400] iteration: [120/782] step: 226900 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0031)\n","\n","epoch: [291/400] iteration: [220/782] step: 227000 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0031)\n","\n","epoch: [291/400] iteration: [320/782] step: 227100 Learning rate: 0.0002\n","Loss = 0.0023 (ave = 0.0031)\n","\n","epoch: [291/400] iteration: [420/782] step: 227200 Learning rate: 0.0002\n","Loss = 0.0023 (ave = 0.0031)\n","\n","epoch: [291/400] iteration: [520/782] step: 227300 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0031)\n","\n","epoch: [291/400] iteration: [620/782] step: 227400 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0031)\n","\n","epoch: [291/400] iteration: [720/782] step: 227500 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0031)\n","\n","epoch: [291/400] iteration: [782/782] step: 227563 Learning rate: 0.0002\n","Loss = 0.0032 (ave = 0.0031)\n","\n","epoch: [292/400] iteration: [38/782] step: 227600 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0031)\n","\n","epoch: [292/400] iteration: [138/782] step: 227700 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0031)\n","\n","epoch: [292/400] iteration: [238/782] step: 227800 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0031)\n","\n","epoch: [292/400] iteration: [338/782] step: 227900 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0031)\n","\n","epoch: [292/400] iteration: [438/782] step: 228000 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0031)\n","\n","epoch: [292/400] iteration: [538/782] step: 228100 Learning rate: 0.0002\n","Loss = 0.0025 (ave = 0.0031)\n","\n","epoch: [292/400] iteration: [638/782] step: 228200 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0031)\n","\n","epoch: [292/400] iteration: [738/782] step: 228300 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0031)\n","\n","epoch: [292/400] iteration: [782/782] step: 228345 Learning rate: 0.0002\n","Loss = 0.0024 (ave = 0.0031)\n","\n","epoch: [293/400] iteration: [56/782] step: 228400 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0031)\n","\n","epoch: [293/400] iteration: [156/782] step: 228500 Learning rate: 0.0002\n","Loss = 0.0018 (ave = 0.0031)\n","\n","epoch: [293/400] iteration: [256/782] step: 228600 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0031)\n","\n","epoch: [293/400] iteration: [356/782] step: 228700 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0031)\n","\n","epoch: [293/400] iteration: [456/782] step: 228800 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0031)\n","\n","epoch: [293/400] iteration: [556/782] step: 228900 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0031)\n","\n","epoch: [293/400] iteration: [656/782] step: 229000 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0031)\n","\n","epoch: [293/400] iteration: [756/782] step: 229100 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0031)\n","\n","epoch: [293/400] iteration: [782/782] step: 229127 Learning rate: 0.0002\n","Loss = 0.0035 (ave = 0.0031)\n","\n","epoch: [294/400] iteration: [74/782] step: 229200 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0031)\n","\n","epoch: [294/400] iteration: [174/782] step: 229300 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0031)\n","\n","epoch: [294/400] iteration: [274/782] step: 229400 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0031)\n","\n","epoch: [294/400] iteration: [374/782] step: 229500 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0031)\n","\n","epoch: [294/400] iteration: [474/782] step: 229600 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0031)\n","\n","epoch: [294/400] iteration: [574/782] step: 229700 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0031)\n","\n","epoch: [294/400] iteration: [674/782] step: 229800 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0031)\n","\n","epoch: [294/400] iteration: [774/782] step: 229900 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0031)\n","\n","epoch: [294/400] iteration: [782/782] step: 229909 Learning rate: 0.0002\n","Loss = 0.0028 (ave = 0.0031)\n","\n","epoch: [295/400] iteration: [92/782] step: 230000 Learning rate: 0.0002\n","Loss = 0.0018 (ave = 0.0031)\n","\n","epoch: [295/400] iteration: [192/782] step: 230100 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0031)\n","\n","epoch: [295/400] iteration: [292/782] step: 230200 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0031)\n","\n","epoch: [295/400] iteration: [392/782] step: 230300 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0031)\n","\n","epoch: [295/400] iteration: [492/782] step: 230400 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0031)\n","\n","epoch: [295/400] iteration: [592/782] step: 230500 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0031)\n","\n","epoch: [295/400] iteration: [692/782] step: 230600 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0031)\n","\n","epoch: [295/400] iteration: [782/782] step: 230691 Learning rate: 0.0002\n","Loss = 0.0026 (ave = 0.0031)\n","\n","epoch: [296/400] iteration: [10/782] step: 230700 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0031)\n","\n","epoch: [296/400] iteration: [110/782] step: 230800 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0031)\n","\n","epoch: [296/400] iteration: [210/782] step: 230900 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0031)\n","\n","epoch: [296/400] iteration: [310/782] step: 231000 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0031)\n","\n","epoch: [296/400] iteration: [410/782] step: 231100 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0031)\n","\n","epoch: [296/400] iteration: [510/782] step: 231200 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0031)\n","\n","epoch: [296/400] iteration: [610/782] step: 231300 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0031)\n","\n","epoch: [296/400] iteration: [710/782] step: 231400 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0031)\n","\n","epoch: [296/400] iteration: [782/782] step: 231473 Learning rate: 0.0002\n","Loss = 0.0027 (ave = 0.0031)\n","\n","epoch: [297/400] iteration: [28/782] step: 231500 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0031)\n","\n","epoch: [297/400] iteration: [128/782] step: 231600 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0031)\n","\n","epoch: [297/400] iteration: [228/782] step: 231700 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0031)\n","\n","epoch: [297/400] iteration: [328/782] step: 231800 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0031)\n","\n","epoch: [297/400] iteration: [428/782] step: 231900 Learning rate: 0.0002\n","Loss = 0.0023 (ave = 0.0031)\n","\n","epoch: [297/400] iteration: [528/782] step: 232000 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0031)\n","\n","epoch: [297/400] iteration: [628/782] step: 232100 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0031)\n","\n","epoch: [297/400] iteration: [728/782] step: 232200 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0031)\n","\n","epoch: [297/400] iteration: [782/782] step: 232255 Learning rate: 0.0002\n","Loss = 0.0030 (ave = 0.0031)\n","\n","epoch: [298/400] iteration: [46/782] step: 232300 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0031)\n","\n","epoch: [298/400] iteration: [146/782] step: 232400 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0031)\n","\n","epoch: [298/400] iteration: [246/782] step: 232500 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0031)\n","\n","epoch: [298/400] iteration: [346/782] step: 232600 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0031)\n","\n","epoch: [298/400] iteration: [446/782] step: 232700 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0031)\n","\n","epoch: [298/400] iteration: [546/782] step: 232800 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0031)\n","\n","epoch: [298/400] iteration: [646/782] step: 232900 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0031)\n","\n","epoch: [298/400] iteration: [746/782] step: 233000 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0031)\n","\n","epoch: [298/400] iteration: [782/782] step: 233037 Learning rate: 0.0002\n","Loss = 0.0023 (ave = 0.0031)\n","\n","epoch: [299/400] iteration: [64/782] step: 233100 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0031)\n","\n","epoch: [299/400] iteration: [164/782] step: 233200 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0031)\n","\n","epoch: [299/400] iteration: [264/782] step: 233300 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0031)\n","\n","epoch: [299/400] iteration: [364/782] step: 233400 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0031)\n","\n","epoch: [299/400] iteration: [464/782] step: 233500 Learning rate: 0.0002\n","Loss = 0.0018 (ave = 0.0031)\n","\n","epoch: [299/400] iteration: [564/782] step: 233600 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0031)\n","\n","epoch: [299/400] iteration: [664/782] step: 233700 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0031)\n","\n","epoch: [299/400] iteration: [764/782] step: 233800 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0031)\n","\n","epoch: [299/400] iteration: [782/782] step: 233819 Learning rate: 0.0002\n","Loss = 0.0026 (ave = 0.0031)\n","\n","epoch: [300/400] iteration: [82/782] step: 233900 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0031)\n","\n","epoch: [300/400] iteration: [182/782] step: 234000 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0031)\n","\n","epoch: [300/400] iteration: [282/782] step: 234100 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0031)\n","\n","epoch: [300/400] iteration: [382/782] step: 234200 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0031)\n","\n","epoch: [300/400] iteration: [482/782] step: 234300 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0031)\n","\n","epoch: [300/400] iteration: [582/782] step: 234400 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0031)\n","\n","epoch: [300/400] iteration: [682/782] step: 234500 Learning rate: 0.0002\n","Loss = 0.0023 (ave = 0.0031)\n","\n","epoch: [300/400] iteration: [782/782] step: 234600 Learning rate: 0.0002\n","Loss = 0.0028 (ave = 0.0031)\n","\n","epoch: [300/400] iteration: [782/782] step: 234601 Learning rate: 0.0002\n","Loss = 0.0028 (ave = 0.0031)\n","\n","epoch: [301/400] iteration: [100/782] step: 234700 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0031)\n","\n","epoch: [301/400] iteration: [200/782] step: 234800 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0031)\n","\n","epoch: [301/400] iteration: [300/782] step: 234900 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0031)\n","\n","epoch: [301/400] iteration: [400/782] step: 235000 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0031)\n","\n","epoch: [301/400] iteration: [500/782] step: 235100 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0031)\n","\n","epoch: [301/400] iteration: [600/782] step: 235200 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0031)\n","\n","epoch: [301/400] iteration: [700/782] step: 235300 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0031)\n","\n","epoch: [301/400] iteration: [782/782] step: 235383 Learning rate: 0.0002\n","Loss = 0.0025 (ave = 0.0031)\n","\n","epoch: [302/400] iteration: [18/782] step: 235400 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0031)\n","\n","epoch: [302/400] iteration: [118/782] step: 235500 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0031)\n","\n","epoch: [302/400] iteration: [218/782] step: 235600 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0031)\n","\n","epoch: [302/400] iteration: [318/782] step: 235700 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0031)\n","\n","epoch: [302/400] iteration: [418/782] step: 235800 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0031)\n","\n","epoch: [302/400] iteration: [518/782] step: 235900 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0031)\n","\n","epoch: [302/400] iteration: [618/782] step: 236000 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0031)\n","\n","epoch: [302/400] iteration: [718/782] step: 236100 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0031)\n","\n","epoch: [302/400] iteration: [782/782] step: 236165 Learning rate: 0.0002\n","Loss = 0.0030 (ave = 0.0031)\n","\n","epoch: [303/400] iteration: [36/782] step: 236200 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0031)\n","\n","epoch: [303/400] iteration: [136/782] step: 236300 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0031)\n","\n","epoch: [303/400] iteration: [236/782] step: 236400 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0031)\n","\n","epoch: [303/400] iteration: [336/782] step: 236500 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0031)\n","\n","epoch: [303/400] iteration: [436/782] step: 236600 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0031)\n","\n","epoch: [303/400] iteration: [536/782] step: 236700 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0031)\n","\n","epoch: [303/400] iteration: [636/782] step: 236800 Learning rate: 0.0002\n","Loss = 0.0018 (ave = 0.0031)\n","\n","epoch: [303/400] iteration: [736/782] step: 236900 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0031)\n","\n","epoch: [303/400] iteration: [782/782] step: 236947 Learning rate: 0.0002\n","Loss = 0.0031 (ave = 0.0031)\n","\n","epoch: [304/400] iteration: [54/782] step: 237000 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0031)\n","\n","epoch: [304/400] iteration: [154/782] step: 237100 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0031)\n","\n","epoch: [304/400] iteration: [254/782] step: 237200 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0031)\n","\n","epoch: [304/400] iteration: [354/782] step: 237300 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0031)\n","\n","epoch: [304/400] iteration: [454/782] step: 237400 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0031)\n","\n","epoch: [304/400] iteration: [554/782] step: 237500 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0031)\n","\n","epoch: [304/400] iteration: [654/782] step: 237600 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0031)\n","\n","epoch: [304/400] iteration: [754/782] step: 237700 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0031)\n","\n","epoch: [304/400] iteration: [782/782] step: 237729 Learning rate: 0.0002\n","Loss = 0.0025 (ave = 0.0031)\n","\n","epoch: [305/400] iteration: [72/782] step: 237800 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0031)\n","\n","epoch: [305/400] iteration: [172/782] step: 237900 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0031)\n","\n","epoch: [305/400] iteration: [272/782] step: 238000 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0031)\n","\n","epoch: [305/400] iteration: [372/782] step: 238100 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0031)\n","\n","epoch: [305/400] iteration: [472/782] step: 238200 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0031)\n","\n","epoch: [305/400] iteration: [572/782] step: 238300 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0031)\n","\n","epoch: [305/400] iteration: [672/782] step: 238400 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0031)\n","\n","epoch: [305/400] iteration: [772/782] step: 238500 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0031)\n","\n","epoch: [305/400] iteration: [782/782] step: 238511 Learning rate: 0.0002\n","Loss = 0.0025 (ave = 0.0031)\n","\n","epoch: [306/400] iteration: [90/782] step: 238600 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0031)\n","\n","epoch: [306/400] iteration: [190/782] step: 238700 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0031)\n","\n","epoch: [306/400] iteration: [290/782] step: 238800 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0031)\n","\n","epoch: [306/400] iteration: [390/782] step: 238900 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0031)\n","\n","epoch: [306/400] iteration: [490/782] step: 239000 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0031)\n","\n","epoch: [306/400] iteration: [590/782] step: 239100 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0031)\n","\n","epoch: [306/400] iteration: [690/782] step: 239200 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0031)\n","\n","epoch: [306/400] iteration: [782/782] step: 239293 Learning rate: 0.0002\n","Loss = 0.0023 (ave = 0.0031)\n","\n","epoch: [307/400] iteration: [8/782] step: 239300 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0031)\n","\n","epoch: [307/400] iteration: [108/782] step: 239400 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0031)\n","\n","epoch: [307/400] iteration: [208/782] step: 239500 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0031)\n","\n","epoch: [307/400] iteration: [308/782] step: 239600 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0031)\n","\n","epoch: [307/400] iteration: [408/782] step: 239700 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0031)\n","\n","epoch: [307/400] iteration: [508/782] step: 239800 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0031)\n","\n","epoch: [307/400] iteration: [608/782] step: 239900 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0031)\n","\n","epoch: [307/400] iteration: [708/782] step: 240000 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0031)\n","\n","epoch: [307/400] iteration: [782/782] step: 240075 Learning rate: 0.0002\n","Loss = 0.0025 (ave = 0.0031)\n","\n","epoch: [308/400] iteration: [26/782] step: 240100 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0031)\n","\n","epoch: [308/400] iteration: [126/782] step: 240200 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0031)\n","\n","epoch: [308/400] iteration: [226/782] step: 240300 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0031)\n","\n","epoch: [308/400] iteration: [326/782] step: 240400 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0031)\n","\n","epoch: [308/400] iteration: [426/782] step: 240500 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0031)\n","\n","epoch: [308/400] iteration: [526/782] step: 240600 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0031)\n","\n","epoch: [308/400] iteration: [626/782] step: 240700 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0031)\n","\n","epoch: [308/400] iteration: [726/782] step: 240800 Learning rate: 0.0002\n","Loss = 0.0018 (ave = 0.0031)\n","\n","epoch: [308/400] iteration: [782/782] step: 240857 Learning rate: 0.0002\n","Loss = 0.0024 (ave = 0.0031)\n","\n","epoch: [309/400] iteration: [44/782] step: 240900 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0031)\n","\n","epoch: [309/400] iteration: [144/782] step: 241000 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0031)\n","\n","epoch: [309/400] iteration: [244/782] step: 241100 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0031)\n","\n","epoch: [309/400] iteration: [344/782] step: 241200 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0031)\n","\n","epoch: [309/400] iteration: [444/782] step: 241300 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0031)\n","\n","epoch: [309/400] iteration: [544/782] step: 241400 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0031)\n","\n","epoch: [309/400] iteration: [644/782] step: 241500 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0031)\n","\n","epoch: [309/400] iteration: [744/782] step: 241600 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0031)\n","\n","epoch: [309/400] iteration: [782/782] step: 241639 Learning rate: 0.0002\n","Loss = 0.0029 (ave = 0.0031)\n","\n","epoch: [310/400] iteration: [62/782] step: 241700 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0031)\n","\n","epoch: [310/400] iteration: [162/782] step: 241800 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0031)\n","\n","epoch: [310/400] iteration: [262/782] step: 241900 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0031)\n","\n","epoch: [310/400] iteration: [362/782] step: 242000 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0031)\n","\n","epoch: [310/400] iteration: [462/782] step: 242100 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0031)\n","\n","epoch: [310/400] iteration: [562/782] step: 242200 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0031)\n","\n","epoch: [310/400] iteration: [662/782] step: 242300 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0031)\n","\n","epoch: [310/400] iteration: [762/782] step: 242400 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0031)\n","\n","epoch: [310/400] iteration: [782/782] step: 242421 Learning rate: 0.0002\n","Loss = 0.0037 (ave = 0.0031)\n","\n","epoch: [311/400] iteration: [80/782] step: 242500 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0031)\n","\n","epoch: [311/400] iteration: [180/782] step: 242600 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0031)\n","\n","epoch: [311/400] iteration: [280/782] step: 242700 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0031)\n","\n","epoch: [311/400] iteration: [380/782] step: 242800 Learning rate: 0.0002\n","Loss = 0.0018 (ave = 0.0031)\n","\n","epoch: [311/400] iteration: [480/782] step: 242900 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0031)\n","\n","epoch: [311/400] iteration: [580/782] step: 243000 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0031)\n","\n","epoch: [311/400] iteration: [680/782] step: 243100 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0031)\n","\n","epoch: [311/400] iteration: [780/782] step: 243200 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0031)\n","\n","epoch: [311/400] iteration: [782/782] step: 243203 Learning rate: 0.0002\n","Loss = 0.0027 (ave = 0.0031)\n","\n","epoch: [312/400] iteration: [98/782] step: 243300 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0031)\n","\n","epoch: [312/400] iteration: [198/782] step: 243400 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0031)\n","\n","epoch: [312/400] iteration: [298/782] step: 243500 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0031)\n","\n","epoch: [312/400] iteration: [398/782] step: 243600 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0031)\n","\n","epoch: [312/400] iteration: [498/782] step: 243700 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0031)\n","\n","epoch: [312/400] iteration: [598/782] step: 243800 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0031)\n","\n","epoch: [312/400] iteration: [698/782] step: 243900 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0031)\n","\n","epoch: [312/400] iteration: [782/782] step: 243985 Learning rate: 0.0002\n","Loss = 0.0028 (ave = 0.0031)\n","\n","epoch: [313/400] iteration: [16/782] step: 244000 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0031)\n","\n","epoch: [313/400] iteration: [116/782] step: 244100 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0031)\n","\n","epoch: [313/400] iteration: [216/782] step: 244200 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0031)\n","\n","epoch: [313/400] iteration: [316/782] step: 244300 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0031)\n","\n","epoch: [313/400] iteration: [416/782] step: 244400 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0031)\n","\n","epoch: [313/400] iteration: [516/782] step: 244500 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0031)\n","\n","epoch: [313/400] iteration: [616/782] step: 244600 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0031)\n","\n","epoch: [313/400] iteration: [716/782] step: 244700 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0031)\n","\n","epoch: [313/400] iteration: [782/782] step: 244767 Learning rate: 0.0002\n","Loss = 0.0033 (ave = 0.0031)\n","\n","epoch: [314/400] iteration: [34/782] step: 244800 Learning rate: 0.0002\n","Loss = 0.0018 (ave = 0.0031)\n","\n","epoch: [314/400] iteration: [134/782] step: 244900 Learning rate: 0.0002\n","Loss = 0.0018 (ave = 0.0031)\n","\n","epoch: [314/400] iteration: [234/782] step: 245000 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0031)\n","\n","epoch: [314/400] iteration: [334/782] step: 245100 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0031)\n","\n","epoch: [314/400] iteration: [434/782] step: 245200 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0031)\n","\n","epoch: [314/400] iteration: [534/782] step: 245300 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0031)\n","\n","epoch: [314/400] iteration: [634/782] step: 245400 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0031)\n","\n","epoch: [314/400] iteration: [734/782] step: 245500 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0031)\n","\n","epoch: [314/400] iteration: [782/782] step: 245549 Learning rate: 0.0002\n","Loss = 0.0031 (ave = 0.0031)\n","\n","epoch: [315/400] iteration: [52/782] step: 245600 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0031)\n","\n","epoch: [315/400] iteration: [152/782] step: 245700 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0031)\n","\n","epoch: [315/400] iteration: [252/782] step: 245800 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0031)\n","\n","epoch: [315/400] iteration: [352/782] step: 245900 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0031)\n","\n","epoch: [315/400] iteration: [452/782] step: 246000 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0031)\n","\n","epoch: [315/400] iteration: [552/782] step: 246100 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0031)\n","\n","epoch: [315/400] iteration: [652/782] step: 246200 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0031)\n","\n","epoch: [315/400] iteration: [752/782] step: 246300 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0031)\n","\n","epoch: [315/400] iteration: [782/782] step: 246331 Learning rate: 0.0002\n","Loss = 0.0029 (ave = 0.0031)\n","\n","epoch: [316/400] iteration: [70/782] step: 246400 Learning rate: 0.0002\n","Loss = 0.0018 (ave = 0.0031)\n","\n","epoch: [316/400] iteration: [170/782] step: 246500 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0031)\n","\n","epoch: [316/400] iteration: [270/782] step: 246600 Learning rate: 0.0002\n","Loss = 0.0018 (ave = 0.0031)\n","\n","epoch: [316/400] iteration: [370/782] step: 246700 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0031)\n","\n","epoch: [316/400] iteration: [470/782] step: 246800 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0031)\n","\n","epoch: [316/400] iteration: [570/782] step: 246900 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0031)\n","\n","epoch: [316/400] iteration: [670/782] step: 247000 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0031)\n","\n","epoch: [316/400] iteration: [770/782] step: 247100 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0031)\n","\n","epoch: [316/400] iteration: [782/782] step: 247113 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0031)\n","\n","epoch: [317/400] iteration: [88/782] step: 247200 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0030)\n","\n","epoch: [317/400] iteration: [188/782] step: 247300 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0030)\n","\n","epoch: [317/400] iteration: [288/782] step: 247400 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0030)\n","\n","epoch: [317/400] iteration: [388/782] step: 247500 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0030)\n","\n","epoch: [317/400] iteration: [488/782] step: 247600 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0030)\n","\n","epoch: [317/400] iteration: [588/782] step: 247700 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0030)\n","\n","epoch: [317/400] iteration: [688/782] step: 247800 Learning rate: 0.0002\n","Loss = 0.0018 (ave = 0.0030)\n","\n","epoch: [317/400] iteration: [782/782] step: 247895 Learning rate: 0.0002\n","Loss = 0.0026 (ave = 0.0030)\n","\n","epoch: [318/400] iteration: [6/782] step: 247900 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0030)\n","\n","epoch: [318/400] iteration: [106/782] step: 248000 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0030)\n","\n","epoch: [318/400] iteration: [206/782] step: 248100 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0030)\n","\n","epoch: [318/400] iteration: [306/782] step: 248200 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0030)\n","\n","epoch: [318/400] iteration: [406/782] step: 248300 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0030)\n","\n","epoch: [318/400] iteration: [506/782] step: 248400 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0030)\n","\n","epoch: [318/400] iteration: [606/782] step: 248500 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0030)\n","\n","epoch: [318/400] iteration: [706/782] step: 248600 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0030)\n","\n","epoch: [318/400] iteration: [782/782] step: 248677 Learning rate: 0.0002\n","Loss = 0.0040 (ave = 0.0030)\n","\n","epoch: [319/400] iteration: [24/782] step: 248700 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0030)\n","\n","epoch: [319/400] iteration: [124/782] step: 248800 Learning rate: 0.0002\n","Loss = 0.0023 (ave = 0.0030)\n","\n","epoch: [319/400] iteration: [224/782] step: 248900 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0030)\n","\n","epoch: [319/400] iteration: [324/782] step: 249000 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0030)\n","\n","epoch: [319/400] iteration: [424/782] step: 249100 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0030)\n","\n","epoch: [319/400] iteration: [524/782] step: 249200 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0030)\n","\n","epoch: [319/400] iteration: [624/782] step: 249300 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0030)\n","\n","epoch: [319/400] iteration: [724/782] step: 249400 Learning rate: 0.0002\n","Loss = 0.0023 (ave = 0.0030)\n","\n","epoch: [319/400] iteration: [782/782] step: 249459 Learning rate: 0.0002\n","Loss = 0.0024 (ave = 0.0030)\n","\n","epoch: [320/400] iteration: [42/782] step: 249500 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0030)\n","\n","epoch: [320/400] iteration: [142/782] step: 249600 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0030)\n","\n","epoch: [320/400] iteration: [242/782] step: 249700 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0030)\n","\n","epoch: [320/400] iteration: [342/782] step: 249800 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0030)\n","\n","epoch: [320/400] iteration: [442/782] step: 249900 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0030)\n","\n","epoch: [320/400] iteration: [542/782] step: 250000 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0030)\n","\n","epoch: [320/400] iteration: [642/782] step: 250100 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0030)\n","\n","epoch: [320/400] iteration: [742/782] step: 250200 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0030)\n","\n","epoch: [320/400] iteration: [782/782] step: 250241 Learning rate: 0.0002\n","Loss = 0.0026 (ave = 0.0030)\n","\n","epoch: [321/400] iteration: [60/782] step: 250300 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0030)\n","\n","epoch: [321/400] iteration: [160/782] step: 250400 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0030)\n","\n","epoch: [321/400] iteration: [260/782] step: 250500 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0030)\n","\n","epoch: [321/400] iteration: [360/782] step: 250600 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0030)\n","\n","epoch: [321/400] iteration: [460/782] step: 250700 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0030)\n","\n","epoch: [321/400] iteration: [560/782] step: 250800 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0030)\n","\n","epoch: [321/400] iteration: [660/782] step: 250900 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0030)\n","\n","epoch: [321/400] iteration: [760/782] step: 251000 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0030)\n","\n","epoch: [321/400] iteration: [782/782] step: 251023 Learning rate: 0.0002\n","Loss = 0.0029 (ave = 0.0030)\n","\n","epoch: [322/400] iteration: [78/782] step: 251100 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0030)\n","\n","epoch: [322/400] iteration: [178/782] step: 251200 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0030)\n","\n","epoch: [322/400] iteration: [278/782] step: 251300 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0030)\n","\n","epoch: [322/400] iteration: [378/782] step: 251400 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0030)\n","\n","epoch: [322/400] iteration: [478/782] step: 251500 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0030)\n","\n","epoch: [322/400] iteration: [578/782] step: 251600 Learning rate: 0.0002\n","Loss = 0.0018 (ave = 0.0030)\n","\n","epoch: [322/400] iteration: [678/782] step: 251700 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0030)\n","\n","epoch: [322/400] iteration: [778/782] step: 251800 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0030)\n","\n","epoch: [322/400] iteration: [782/782] step: 251805 Learning rate: 0.0002\n","Loss = 0.0023 (ave = 0.0030)\n","\n","epoch: [323/400] iteration: [96/782] step: 251900 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0030)\n","\n","epoch: [323/400] iteration: [196/782] step: 252000 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0030)\n","\n","epoch: [323/400] iteration: [296/782] step: 252100 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0030)\n","\n","epoch: [323/400] iteration: [396/782] step: 252200 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0030)\n","\n","epoch: [323/400] iteration: [496/782] step: 252300 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0030)\n","\n","epoch: [323/400] iteration: [596/782] step: 252400 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0030)\n","\n","epoch: [323/400] iteration: [696/782] step: 252500 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0030)\n","\n","epoch: [323/400] iteration: [782/782] step: 252587 Learning rate: 0.0002\n","Loss = 0.0023 (ave = 0.0030)\n","\n","epoch: [324/400] iteration: [14/782] step: 252600 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0030)\n","\n","epoch: [324/400] iteration: [114/782] step: 252700 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0030)\n","\n","epoch: [324/400] iteration: [214/782] step: 252800 Learning rate: 0.0002\n","Loss = 0.0025 (ave = 0.0030)\n","\n","epoch: [324/400] iteration: [314/782] step: 252900 Learning rate: 0.0002\n","Loss = 0.0018 (ave = 0.0030)\n","\n","epoch: [324/400] iteration: [414/782] step: 253000 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0030)\n","\n","epoch: [324/400] iteration: [514/782] step: 253100 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0030)\n","\n","epoch: [324/400] iteration: [614/782] step: 253200 Learning rate: 0.0002\n","Loss = 0.0023 (ave = 0.0030)\n","\n","epoch: [324/400] iteration: [714/782] step: 253300 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0030)\n","\n","epoch: [324/400] iteration: [782/782] step: 253369 Learning rate: 0.0002\n","Loss = 0.0030 (ave = 0.0030)\n","\n","epoch: [325/400] iteration: [32/782] step: 253400 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0030)\n","\n","epoch: [325/400] iteration: [132/782] step: 253500 Learning rate: 0.0002\n","Loss = 0.0018 (ave = 0.0030)\n","\n","epoch: [325/400] iteration: [232/782] step: 253600 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0030)\n","\n","epoch: [325/400] iteration: [332/782] step: 253700 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0030)\n","\n","epoch: [325/400] iteration: [432/782] step: 253800 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0030)\n","\n","epoch: [325/400] iteration: [532/782] step: 253900 Learning rate: 0.0002\n","Loss = 0.0018 (ave = 0.0030)\n","\n","epoch: [325/400] iteration: [632/782] step: 254000 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0030)\n","\n","epoch: [325/400] iteration: [732/782] step: 254100 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0030)\n","\n","epoch: [325/400] iteration: [782/782] step: 254151 Learning rate: 0.0002\n","Loss = 0.0028 (ave = 0.0030)\n","\n","epoch: [326/400] iteration: [50/782] step: 254200 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0030)\n","\n","epoch: [326/400] iteration: [150/782] step: 254300 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0030)\n","\n","epoch: [326/400] iteration: [250/782] step: 254400 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0030)\n","\n","epoch: [326/400] iteration: [350/782] step: 254500 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0030)\n","\n","epoch: [326/400] iteration: [450/782] step: 254600 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0030)\n","\n","epoch: [326/400] iteration: [550/782] step: 254700 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0030)\n","\n","epoch: [326/400] iteration: [650/782] step: 254800 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0030)\n","\n","epoch: [326/400] iteration: [750/782] step: 254900 Learning rate: 0.0002\n","Loss = 0.0018 (ave = 0.0030)\n","\n","epoch: [326/400] iteration: [782/782] step: 254933 Learning rate: 0.0002\n","Loss = 0.0028 (ave = 0.0030)\n","\n","epoch: [327/400] iteration: [68/782] step: 255000 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0030)\n","\n","epoch: [327/400] iteration: [168/782] step: 255100 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0030)\n","\n","epoch: [327/400] iteration: [268/782] step: 255200 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0030)\n","\n","epoch: [327/400] iteration: [368/782] step: 255300 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0030)\n","\n","epoch: [327/400] iteration: [468/782] step: 255400 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0030)\n","\n","epoch: [327/400] iteration: [568/782] step: 255500 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0030)\n","\n","epoch: [327/400] iteration: [668/782] step: 255600 Learning rate: 0.0002\n","Loss = 0.0023 (ave = 0.0030)\n","\n","epoch: [327/400] iteration: [768/782] step: 255700 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0030)\n","\n","epoch: [327/400] iteration: [782/782] step: 255715 Learning rate: 0.0002\n","Loss = 0.0030 (ave = 0.0030)\n","\n","epoch: [328/400] iteration: [86/782] step: 255800 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0030)\n","\n","epoch: [328/400] iteration: [186/782] step: 255900 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0030)\n","\n","epoch: [328/400] iteration: [286/782] step: 256000 Learning rate: 0.0002\n","Loss = 0.0018 (ave = 0.0030)\n","\n","epoch: [328/400] iteration: [386/782] step: 256100 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0030)\n","\n","epoch: [328/400] iteration: [486/782] step: 256200 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0030)\n","\n","epoch: [328/400] iteration: [586/782] step: 256300 Learning rate: 0.0002\n","Loss = 0.0018 (ave = 0.0030)\n","\n","epoch: [328/400] iteration: [686/782] step: 256400 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0030)\n","\n","epoch: [328/400] iteration: [782/782] step: 256497 Learning rate: 0.0002\n","Loss = 0.0026 (ave = 0.0030)\n","\n","epoch: [329/400] iteration: [4/782] step: 256500 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0030)\n","\n","epoch: [329/400] iteration: [104/782] step: 256600 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0030)\n","\n","epoch: [329/400] iteration: [204/782] step: 256700 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0030)\n","\n","epoch: [329/400] iteration: [304/782] step: 256800 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0030)\n","\n","epoch: [329/400] iteration: [404/782] step: 256900 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0030)\n","\n","epoch: [329/400] iteration: [504/782] step: 257000 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0030)\n","\n","epoch: [329/400] iteration: [604/782] step: 257100 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0030)\n","\n","epoch: [329/400] iteration: [704/782] step: 257200 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0030)\n","\n","epoch: [329/400] iteration: [782/782] step: 257279 Learning rate: 0.0002\n","Loss = 0.0025 (ave = 0.0030)\n","\n","epoch: [330/400] iteration: [22/782] step: 257300 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0030)\n","\n","epoch: [330/400] iteration: [122/782] step: 257400 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0030)\n","\n","epoch: [330/400] iteration: [222/782] step: 257500 Learning rate: 0.0002\n","Loss = 0.0018 (ave = 0.0030)\n","\n","epoch: [330/400] iteration: [322/782] step: 257600 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0030)\n","\n","epoch: [330/400] iteration: [422/782] step: 257700 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0030)\n","\n","epoch: [330/400] iteration: [522/782] step: 257800 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0030)\n","\n","epoch: [330/400] iteration: [622/782] step: 257900 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0030)\n","\n","epoch: [330/400] iteration: [722/782] step: 258000 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0030)\n","\n","epoch: [330/400] iteration: [782/782] step: 258061 Learning rate: 0.0002\n","Loss = 0.0036 (ave = 0.0030)\n","\n","epoch: [331/400] iteration: [40/782] step: 258100 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0030)\n","\n","epoch: [331/400] iteration: [140/782] step: 258200 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0030)\n","\n","epoch: [331/400] iteration: [240/782] step: 258300 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0030)\n","\n","epoch: [331/400] iteration: [340/782] step: 258400 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0030)\n","\n","epoch: [331/400] iteration: [440/782] step: 258500 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0030)\n","\n","epoch: [331/400] iteration: [540/782] step: 258600 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0030)\n","\n","epoch: [331/400] iteration: [640/782] step: 258700 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0030)\n","\n","epoch: [331/400] iteration: [740/782] step: 258800 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0030)\n","\n","epoch: [331/400] iteration: [782/782] step: 258843 Learning rate: 0.0002\n","Loss = 0.0028 (ave = 0.0030)\n","\n","epoch: [332/400] iteration: [58/782] step: 258900 Learning rate: 0.0002\n","Loss = 0.0018 (ave = 0.0030)\n","\n","epoch: [332/400] iteration: [158/782] step: 259000 Learning rate: 0.0002\n","Loss = 0.0018 (ave = 0.0030)\n","\n","epoch: [332/400] iteration: [258/782] step: 259100 Learning rate: 0.0002\n","Loss = 0.0018 (ave = 0.0030)\n","\n","epoch: [332/400] iteration: [358/782] step: 259200 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0030)\n","\n","epoch: [332/400] iteration: [458/782] step: 259300 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0030)\n","\n","epoch: [332/400] iteration: [558/782] step: 259400 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0030)\n","\n","epoch: [332/400] iteration: [658/782] step: 259500 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0030)\n","\n","epoch: [332/400] iteration: [758/782] step: 259600 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0030)\n","\n","epoch: [332/400] iteration: [782/782] step: 259625 Learning rate: 0.0002\n","Loss = 0.0033 (ave = 0.0030)\n","\n","epoch: [333/400] iteration: [76/782] step: 259700 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0030)\n","\n","epoch: [333/400] iteration: [176/782] step: 259800 Learning rate: 0.0002\n","Loss = 0.0023 (ave = 0.0030)\n","\n","epoch: [333/400] iteration: [276/782] step: 259900 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0030)\n","\n","epoch: [333/400] iteration: [376/782] step: 260000 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0030)\n","\n","epoch: [333/400] iteration: [476/782] step: 260100 Learning rate: 0.0002\n","Loss = 0.0018 (ave = 0.0030)\n","\n","epoch: [333/400] iteration: [576/782] step: 260200 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0030)\n","\n","epoch: [333/400] iteration: [676/782] step: 260300 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0030)\n","\n","epoch: [333/400] iteration: [776/782] step: 260400 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0030)\n","\n","epoch: [333/400] iteration: [782/782] step: 260407 Learning rate: 0.0002\n","Loss = 0.0030 (ave = 0.0030)\n","\n","epoch: [334/400] iteration: [94/782] step: 260500 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0030)\n","\n","epoch: [334/400] iteration: [194/782] step: 260600 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0030)\n","\n","epoch: [334/400] iteration: [294/782] step: 260700 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0030)\n","\n","epoch: [334/400] iteration: [394/782] step: 260800 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0030)\n","\n","epoch: [334/400] iteration: [494/782] step: 260900 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0030)\n","\n","epoch: [334/400] iteration: [594/782] step: 261000 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0030)\n","\n","epoch: [334/400] iteration: [694/782] step: 261100 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0030)\n","\n","epoch: [334/400] iteration: [782/782] step: 261189 Learning rate: 0.0002\n","Loss = 0.0025 (ave = 0.0030)\n","\n","epoch: [335/400] iteration: [12/782] step: 261200 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0030)\n","\n","epoch: [335/400] iteration: [112/782] step: 261300 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0030)\n","\n","epoch: [335/400] iteration: [212/782] step: 261400 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0030)\n","\n","epoch: [335/400] iteration: [312/782] step: 261500 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0030)\n","\n","epoch: [335/400] iteration: [412/782] step: 261600 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0030)\n","\n","epoch: [335/400] iteration: [512/782] step: 261700 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0030)\n","\n","epoch: [335/400] iteration: [612/782] step: 261800 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0030)\n","\n","epoch: [335/400] iteration: [712/782] step: 261900 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0030)\n","\n","epoch: [335/400] iteration: [782/782] step: 261971 Learning rate: 0.0002\n","Loss = 0.0040 (ave = 0.0030)\n","\n","epoch: [336/400] iteration: [30/782] step: 262000 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0030)\n","\n","epoch: [336/400] iteration: [130/782] step: 262100 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0030)\n","\n","epoch: [336/400] iteration: [230/782] step: 262200 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0030)\n","\n","epoch: [336/400] iteration: [330/782] step: 262300 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0030)\n","\n","epoch: [336/400] iteration: [430/782] step: 262400 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0030)\n","\n","epoch: [336/400] iteration: [530/782] step: 262500 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0030)\n","\n","epoch: [336/400] iteration: [630/782] step: 262600 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0030)\n","\n","epoch: [336/400] iteration: [730/782] step: 262700 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0030)\n","\n","epoch: [336/400] iteration: [782/782] step: 262753 Learning rate: 0.0002\n","Loss = 0.0025 (ave = 0.0030)\n","\n","epoch: [337/400] iteration: [48/782] step: 262800 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0030)\n","\n","epoch: [337/400] iteration: [148/782] step: 262900 Learning rate: 0.0002\n","Loss = 0.0017 (ave = 0.0030)\n","\n","epoch: [337/400] iteration: [248/782] step: 263000 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0030)\n","\n","epoch: [337/400] iteration: [348/782] step: 263100 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0030)\n","\n","epoch: [337/400] iteration: [448/782] step: 263200 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0030)\n","\n","epoch: [337/400] iteration: [548/782] step: 263300 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0030)\n","\n","epoch: [337/400] iteration: [648/782] step: 263400 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0030)\n","\n","epoch: [337/400] iteration: [748/782] step: 263500 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0030)\n","\n","epoch: [337/400] iteration: [782/782] step: 263535 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0030)\n","\n","epoch: [338/400] iteration: [66/782] step: 263600 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0030)\n","\n","epoch: [338/400] iteration: [166/782] step: 263700 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0030)\n","\n","epoch: [338/400] iteration: [266/782] step: 263800 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0030)\n","\n","epoch: [338/400] iteration: [366/782] step: 263900 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0030)\n","\n","epoch: [338/400] iteration: [466/782] step: 264000 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0030)\n","\n","epoch: [338/400] iteration: [566/782] step: 264100 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0030)\n","\n","epoch: [338/400] iteration: [666/782] step: 264200 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0030)\n","\n","epoch: [338/400] iteration: [766/782] step: 264300 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0030)\n","\n","epoch: [338/400] iteration: [782/782] step: 264317 Learning rate: 0.0002\n","Loss = 0.0030 (ave = 0.0030)\n","\n","epoch: [339/400] iteration: [84/782] step: 264400 Learning rate: 0.0002\n","Loss = 0.0018 (ave = 0.0030)\n","\n","epoch: [339/400] iteration: [184/782] step: 264500 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0030)\n","\n","epoch: [339/400] iteration: [284/782] step: 264600 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0030)\n","\n","epoch: [339/400] iteration: [384/782] step: 264700 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0030)\n","\n","epoch: [339/400] iteration: [484/782] step: 264800 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0030)\n","\n","epoch: [339/400] iteration: [584/782] step: 264900 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0030)\n","\n","epoch: [339/400] iteration: [684/782] step: 265000 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0030)\n","\n","epoch: [339/400] iteration: [782/782] step: 265099 Learning rate: 0.0002\n","Loss = 0.0023 (ave = 0.0030)\n","\n","epoch: [340/400] iteration: [2/782] step: 265100 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0030)\n","\n","epoch: [340/400] iteration: [102/782] step: 265200 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0030)\n","\n","epoch: [340/400] iteration: [202/782] step: 265300 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0030)\n","\n","epoch: [340/400] iteration: [302/782] step: 265400 Learning rate: 0.0002\n","Loss = 0.0018 (ave = 0.0030)\n","\n","epoch: [340/400] iteration: [402/782] step: 265500 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0030)\n","\n","epoch: [340/400] iteration: [502/782] step: 265600 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0030)\n","\n","epoch: [340/400] iteration: [602/782] step: 265700 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0030)\n","\n","epoch: [340/400] iteration: [702/782] step: 265800 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0030)\n","\n","epoch: [340/400] iteration: [782/782] step: 265881 Learning rate: 0.0002\n","Loss = 0.0031 (ave = 0.0030)\n","\n","epoch: [341/400] iteration: [20/782] step: 265900 Learning rate: 0.0002\n","Loss = 0.0024 (ave = 0.0030)\n","\n","epoch: [341/400] iteration: [120/782] step: 266000 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0030)\n","\n","epoch: [341/400] iteration: [220/782] step: 266100 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0030)\n","\n","epoch: [341/400] iteration: [320/782] step: 266200 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0030)\n","\n","epoch: [341/400] iteration: [420/782] step: 266300 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0030)\n","\n","epoch: [341/400] iteration: [520/782] step: 266400 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0030)\n","\n","epoch: [341/400] iteration: [620/782] step: 266500 Learning rate: 0.0002\n","Loss = 0.0018 (ave = 0.0030)\n","\n","epoch: [341/400] iteration: [720/782] step: 266600 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0030)\n","\n","epoch: [341/400] iteration: [782/782] step: 266663 Learning rate: 0.0002\n","Loss = 0.0029 (ave = 0.0030)\n","\n","epoch: [342/400] iteration: [38/782] step: 266700 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0030)\n","\n","epoch: [342/400] iteration: [138/782] step: 266800 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0030)\n","\n","epoch: [342/400] iteration: [238/782] step: 266900 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0030)\n","\n","epoch: [342/400] iteration: [338/782] step: 267000 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0030)\n","\n","epoch: [342/400] iteration: [438/782] step: 267100 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0030)\n","\n","epoch: [342/400] iteration: [538/782] step: 267200 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0030)\n","\n","epoch: [342/400] iteration: [638/782] step: 267300 Learning rate: 0.0002\n","Loss = 0.0018 (ave = 0.0030)\n","\n","epoch: [342/400] iteration: [738/782] step: 267400 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0030)\n","\n","epoch: [342/400] iteration: [782/782] step: 267445 Learning rate: 0.0002\n","Loss = 0.0027 (ave = 0.0030)\n","\n","epoch: [343/400] iteration: [56/782] step: 267500 Learning rate: 0.0002\n","Loss = 0.0018 (ave = 0.0030)\n","\n","epoch: [343/400] iteration: [156/782] step: 267600 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0030)\n","\n","epoch: [343/400] iteration: [256/782] step: 267700 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0030)\n","\n","epoch: [343/400] iteration: [356/782] step: 267800 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0030)\n","\n","epoch: [343/400] iteration: [456/782] step: 267900 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0030)\n","\n","epoch: [343/400] iteration: [556/782] step: 268000 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0030)\n","\n","epoch: [343/400] iteration: [656/782] step: 268100 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0030)\n","\n","epoch: [343/400] iteration: [756/782] step: 268200 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0030)\n","\n","epoch: [343/400] iteration: [782/782] step: 268227 Learning rate: 0.0002\n","Loss = 0.0028 (ave = 0.0030)\n","\n","epoch: [344/400] iteration: [74/782] step: 268300 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0030)\n","\n","epoch: [344/400] iteration: [174/782] step: 268400 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0030)\n","\n","epoch: [344/400] iteration: [274/782] step: 268500 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0030)\n","\n","epoch: [344/400] iteration: [374/782] step: 268600 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0030)\n","\n","epoch: [344/400] iteration: [474/782] step: 268700 Learning rate: 0.0002\n","Loss = 0.0018 (ave = 0.0030)\n","\n","epoch: [344/400] iteration: [574/782] step: 268800 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0030)\n","\n","epoch: [344/400] iteration: [674/782] step: 268900 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0030)\n","\n","epoch: [344/400] iteration: [774/782] step: 269000 Learning rate: 0.0002\n","Loss = 0.0018 (ave = 0.0030)\n","\n","epoch: [344/400] iteration: [782/782] step: 269009 Learning rate: 0.0002\n","Loss = 0.0030 (ave = 0.0030)\n","\n","epoch: [345/400] iteration: [92/782] step: 269100 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0030)\n","\n","epoch: [345/400] iteration: [192/782] step: 269200 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0030)\n","\n","epoch: [345/400] iteration: [292/782] step: 269300 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0030)\n","\n","epoch: [345/400] iteration: [392/782] step: 269400 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0030)\n","\n","epoch: [345/400] iteration: [492/782] step: 269500 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0030)\n","\n","epoch: [345/400] iteration: [592/782] step: 269600 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0030)\n","\n","epoch: [345/400] iteration: [692/782] step: 269700 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0030)\n","\n","epoch: [345/400] iteration: [782/782] step: 269791 Learning rate: 0.0002\n","Loss = 0.0026 (ave = 0.0030)\n","\n","epoch: [346/400] iteration: [10/782] step: 269800 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0030)\n","\n","epoch: [346/400] iteration: [110/782] step: 269900 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0030)\n","\n","epoch: [346/400] iteration: [210/782] step: 270000 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0030)\n","\n","epoch: [346/400] iteration: [310/782] step: 270100 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0030)\n","\n","epoch: [346/400] iteration: [410/782] step: 270200 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0030)\n","\n","epoch: [346/400] iteration: [510/782] step: 270300 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0030)\n","\n","epoch: [346/400] iteration: [610/782] step: 270400 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0030)\n","\n","epoch: [346/400] iteration: [710/782] step: 270500 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0030)\n","\n","epoch: [346/400] iteration: [782/782] step: 270573 Learning rate: 0.0002\n","Loss = 0.0032 (ave = 0.0030)\n","\n","epoch: [347/400] iteration: [28/782] step: 270600 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0030)\n","\n","epoch: [347/400] iteration: [128/782] step: 270700 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0030)\n","\n","epoch: [347/400] iteration: [228/782] step: 270800 Learning rate: 0.0002\n","Loss = 0.0018 (ave = 0.0030)\n","\n","epoch: [347/400] iteration: [328/782] step: 270900 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0030)\n","\n","epoch: [347/400] iteration: [428/782] step: 271000 Learning rate: 0.0002\n","Loss = 0.0018 (ave = 0.0030)\n","\n","epoch: [347/400] iteration: [528/782] step: 271100 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0030)\n","\n","epoch: [347/400] iteration: [628/782] step: 271200 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0030)\n","\n","epoch: [347/400] iteration: [728/782] step: 271300 Learning rate: 0.0002\n","Loss = 0.0018 (ave = 0.0030)\n","\n","epoch: [347/400] iteration: [782/782] step: 271355 Learning rate: 0.0002\n","Loss = 0.0037 (ave = 0.0030)\n","\n","epoch: [348/400] iteration: [46/782] step: 271400 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0030)\n","\n","epoch: [348/400] iteration: [146/782] step: 271500 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0030)\n","\n","epoch: [348/400] iteration: [246/782] step: 271600 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0030)\n","\n","epoch: [348/400] iteration: [346/782] step: 271700 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0030)\n","\n","epoch: [348/400] iteration: [446/782] step: 271800 Learning rate: 0.0002\n","Loss = 0.0018 (ave = 0.0030)\n","\n","epoch: [348/400] iteration: [546/782] step: 271900 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0030)\n","\n","epoch: [348/400] iteration: [646/782] step: 272000 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0030)\n","\n","epoch: [348/400] iteration: [746/782] step: 272100 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0030)\n","\n","epoch: [348/400] iteration: [782/782] step: 272137 Learning rate: 0.0002\n","Loss = 0.0029 (ave = 0.0030)\n","\n","epoch: [349/400] iteration: [64/782] step: 272200 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0030)\n","\n","epoch: [349/400] iteration: [164/782] step: 272300 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0030)\n","\n","epoch: [349/400] iteration: [264/782] step: 272400 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0030)\n","\n","epoch: [349/400] iteration: [364/782] step: 272500 Learning rate: 0.0002\n","Loss = 0.0017 (ave = 0.0030)\n","\n","epoch: [349/400] iteration: [464/782] step: 272600 Learning rate: 0.0002\n","Loss = 0.0018 (ave = 0.0029)\n","\n","epoch: [349/400] iteration: [564/782] step: 272700 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0029)\n","\n","epoch: [349/400] iteration: [664/782] step: 272800 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0029)\n","\n","epoch: [349/400] iteration: [764/782] step: 272900 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0029)\n","\n","epoch: [349/400] iteration: [782/782] step: 272919 Learning rate: 0.0002\n","Loss = 0.0026 (ave = 0.0029)\n","\n","epoch: [350/400] iteration: [82/782] step: 273000 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0029)\n","\n","epoch: [350/400] iteration: [182/782] step: 273100 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0029)\n","\n","epoch: [350/400] iteration: [282/782] step: 273200 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0029)\n","\n","epoch: [350/400] iteration: [382/782] step: 273300 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0029)\n","\n","epoch: [350/400] iteration: [482/782] step: 273400 Learning rate: 0.0002\n","Loss = 0.0018 (ave = 0.0029)\n","\n","epoch: [350/400] iteration: [582/782] step: 273500 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0029)\n","\n","epoch: [350/400] iteration: [682/782] step: 273600 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0029)\n","\n","epoch: [350/400] iteration: [782/782] step: 273700 Learning rate: 0.0002\n","Loss = 0.0025 (ave = 0.0029)\n","\n","epoch: [350/400] iteration: [782/782] step: 273701 Learning rate: 0.0002\n","Loss = 0.0025 (ave = 0.0029)\n","\n","epoch: [351/400] iteration: [100/782] step: 273800 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0029)\n","\n","epoch: [351/400] iteration: [200/782] step: 273900 Learning rate: 0.0002\n","Loss = 0.0018 (ave = 0.0029)\n","\n","epoch: [351/400] iteration: [300/782] step: 274000 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0029)\n","\n","epoch: [351/400] iteration: [400/782] step: 274100 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0029)\n","\n","epoch: [351/400] iteration: [500/782] step: 274200 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0029)\n","\n","epoch: [351/400] iteration: [600/782] step: 274300 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0029)\n","\n","epoch: [351/400] iteration: [700/782] step: 274400 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0029)\n","\n","epoch: [351/400] iteration: [782/782] step: 274483 Learning rate: 0.0002\n","Loss = 0.0025 (ave = 0.0029)\n","\n","epoch: [352/400] iteration: [18/782] step: 274500 Learning rate: 0.0002\n","Loss = 0.0018 (ave = 0.0029)\n","\n","epoch: [352/400] iteration: [118/782] step: 274600 Learning rate: 0.0002\n","Loss = 0.0018 (ave = 0.0029)\n","\n","epoch: [352/400] iteration: [218/782] step: 274700 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0029)\n","\n","epoch: [352/400] iteration: [318/782] step: 274800 Learning rate: 0.0002\n","Loss = 0.0018 (ave = 0.0029)\n","\n","epoch: [352/400] iteration: [418/782] step: 274900 Learning rate: 0.0002\n","Loss = 0.0018 (ave = 0.0029)\n","\n","epoch: [352/400] iteration: [518/782] step: 275000 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0029)\n","\n","epoch: [352/400] iteration: [618/782] step: 275100 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0029)\n","\n","epoch: [352/400] iteration: [718/782] step: 275200 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0029)\n","\n","epoch: [352/400] iteration: [782/782] step: 275265 Learning rate: 0.0002\n","Loss = 0.0029 (ave = 0.0029)\n","\n","epoch: [353/400] iteration: [36/782] step: 275300 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0029)\n","\n","epoch: [353/400] iteration: [136/782] step: 275400 Learning rate: 0.0002\n","Loss = 0.0018 (ave = 0.0029)\n","\n","epoch: [353/400] iteration: [236/782] step: 275500 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0029)\n","\n","epoch: [353/400] iteration: [336/782] step: 275600 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0029)\n","\n","epoch: [353/400] iteration: [436/782] step: 275700 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0029)\n","\n","epoch: [353/400] iteration: [536/782] step: 275800 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0029)\n","\n","epoch: [353/400] iteration: [636/782] step: 275900 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0029)\n","\n","epoch: [353/400] iteration: [736/782] step: 276000 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0029)\n","\n","epoch: [353/400] iteration: [782/782] step: 276047 Learning rate: 0.0002\n","Loss = 0.0026 (ave = 0.0029)\n","\n","epoch: [354/400] iteration: [54/782] step: 276100 Learning rate: 0.0002\n","Loss = 0.0018 (ave = 0.0029)\n","\n","epoch: [354/400] iteration: [154/782] step: 276200 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0029)\n","\n","epoch: [354/400] iteration: [254/782] step: 276300 Learning rate: 0.0002\n","Loss = 0.0018 (ave = 0.0029)\n","\n","epoch: [354/400] iteration: [354/782] step: 276400 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0029)\n","\n","epoch: [354/400] iteration: [454/782] step: 276500 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0029)\n","\n","epoch: [354/400] iteration: [554/782] step: 276600 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0029)\n","\n","epoch: [354/400] iteration: [654/782] step: 276700 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0029)\n","\n","epoch: [354/400] iteration: [754/782] step: 276800 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0029)\n","\n","epoch: [354/400] iteration: [782/782] step: 276829 Learning rate: 0.0002\n","Loss = 0.0036 (ave = 0.0029)\n","\n","epoch: [355/400] iteration: [72/782] step: 276900 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0029)\n","\n","epoch: [355/400] iteration: [172/782] step: 277000 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0029)\n","\n","epoch: [355/400] iteration: [272/782] step: 277100 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0029)\n","\n","epoch: [355/400] iteration: [372/782] step: 277200 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0029)\n","\n","epoch: [355/400] iteration: [472/782] step: 277300 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0029)\n","\n","epoch: [355/400] iteration: [572/782] step: 277400 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0029)\n","\n","epoch: [355/400] iteration: [672/782] step: 277500 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0029)\n","\n","epoch: [355/400] iteration: [772/782] step: 277600 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0029)\n","\n","epoch: [355/400] iteration: [782/782] step: 277611 Learning rate: 0.0002\n","Loss = 0.0026 (ave = 0.0029)\n","\n","epoch: [356/400] iteration: [90/782] step: 277700 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0029)\n","\n","epoch: [356/400] iteration: [190/782] step: 277800 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0029)\n","\n","epoch: [356/400] iteration: [290/782] step: 277900 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0029)\n","\n","epoch: [356/400] iteration: [390/782] step: 278000 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0029)\n","\n","epoch: [356/400] iteration: [490/782] step: 278100 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0029)\n","\n","epoch: [356/400] iteration: [590/782] step: 278200 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0029)\n","\n","epoch: [356/400] iteration: [690/782] step: 278300 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0029)\n","\n","epoch: [356/400] iteration: [782/782] step: 278393 Learning rate: 0.0002\n","Loss = 0.0028 (ave = 0.0029)\n","\n","epoch: [357/400] iteration: [8/782] step: 278400 Learning rate: 0.0002\n","Loss = 0.0018 (ave = 0.0029)\n","\n","epoch: [357/400] iteration: [108/782] step: 278500 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0029)\n","\n","epoch: [357/400] iteration: [208/782] step: 278600 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0029)\n","\n","epoch: [357/400] iteration: [308/782] step: 278700 Learning rate: 0.0002\n","Loss = 0.0018 (ave = 0.0029)\n","\n","epoch: [357/400] iteration: [408/782] step: 278800 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0029)\n","\n","epoch: [357/400] iteration: [508/782] step: 278900 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0029)\n","\n","epoch: [357/400] iteration: [608/782] step: 279000 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0029)\n","\n","epoch: [357/400] iteration: [708/782] step: 279100 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0029)\n","\n","epoch: [357/400] iteration: [782/782] step: 279175 Learning rate: 0.0002\n","Loss = 0.0024 (ave = 0.0029)\n","\n","epoch: [358/400] iteration: [26/782] step: 279200 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0029)\n","\n","epoch: [358/400] iteration: [126/782] step: 279300 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0029)\n","\n","epoch: [358/400] iteration: [226/782] step: 279400 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0029)\n","\n","epoch: [358/400] iteration: [326/782] step: 279500 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0029)\n","\n","epoch: [358/400] iteration: [426/782] step: 279600 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0029)\n","\n","epoch: [358/400] iteration: [526/782] step: 279700 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0029)\n","\n","epoch: [358/400] iteration: [626/782] step: 279800 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0029)\n","\n","epoch: [358/400] iteration: [726/782] step: 279900 Learning rate: 0.0002\n","Loss = 0.0018 (ave = 0.0029)\n","\n","epoch: [358/400] iteration: [782/782] step: 279957 Learning rate: 0.0002\n","Loss = 0.0030 (ave = 0.0029)\n","\n","epoch: [359/400] iteration: [44/782] step: 280000 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0029)\n","\n","epoch: [359/400] iteration: [144/782] step: 280100 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0029)\n","\n","epoch: [359/400] iteration: [244/782] step: 280200 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0029)\n","\n","epoch: [359/400] iteration: [344/782] step: 280300 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0029)\n","\n","epoch: [359/400] iteration: [444/782] step: 280400 Learning rate: 0.0002\n","Loss = 0.0018 (ave = 0.0029)\n","\n","epoch: [359/400] iteration: [544/782] step: 280500 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0029)\n","\n","epoch: [359/400] iteration: [644/782] step: 280600 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0029)\n","\n","epoch: [359/400] iteration: [744/782] step: 280700 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0029)\n","\n","epoch: [359/400] iteration: [782/782] step: 280739 Learning rate: 0.0002\n","Loss = 0.0027 (ave = 0.0029)\n","\n","epoch: [360/400] iteration: [62/782] step: 280800 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0029)\n","\n","epoch: [360/400] iteration: [162/782] step: 280900 Learning rate: 0.0002\n","Loss = 0.0018 (ave = 0.0029)\n","\n","epoch: [360/400] iteration: [262/782] step: 281000 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0029)\n","\n","epoch: [360/400] iteration: [362/782] step: 281100 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0029)\n","\n","epoch: [360/400] iteration: [462/782] step: 281200 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0029)\n","\n","epoch: [360/400] iteration: [562/782] step: 281300 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0029)\n","\n","epoch: [360/400] iteration: [662/782] step: 281400 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0029)\n","\n","epoch: [360/400] iteration: [762/782] step: 281500 Learning rate: 0.0002\n","Loss = 0.0018 (ave = 0.0029)\n","\n","epoch: [360/400] iteration: [782/782] step: 281521 Learning rate: 0.0002\n","Loss = 0.0027 (ave = 0.0029)\n","\n","epoch: [361/400] iteration: [80/782] step: 281600 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0029)\n","\n","epoch: [361/400] iteration: [180/782] step: 281700 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0029)\n","\n","epoch: [361/400] iteration: [280/782] step: 281800 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0029)\n","\n","epoch: [361/400] iteration: [380/782] step: 281900 Learning rate: 0.0002\n","Loss = 0.0018 (ave = 0.0029)\n","\n","epoch: [361/400] iteration: [480/782] step: 282000 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0029)\n","\n","epoch: [361/400] iteration: [580/782] step: 282100 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0029)\n","\n","epoch: [361/400] iteration: [680/782] step: 282200 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0029)\n","\n","epoch: [361/400] iteration: [780/782] step: 282300 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0029)\n","\n","epoch: [361/400] iteration: [782/782] step: 282303 Learning rate: 0.0002\n","Loss = 0.0025 (ave = 0.0029)\n","\n","epoch: [362/400] iteration: [98/782] step: 282400 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0029)\n","\n","epoch: [362/400] iteration: [198/782] step: 282500 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0029)\n","\n","epoch: [362/400] iteration: [298/782] step: 282600 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0029)\n","\n","epoch: [362/400] iteration: [398/782] step: 282700 Learning rate: 0.0002\n","Loss = 0.0018 (ave = 0.0029)\n","\n","epoch: [362/400] iteration: [498/782] step: 282800 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0029)\n","\n","epoch: [362/400] iteration: [598/782] step: 282900 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0029)\n","\n","epoch: [362/400] iteration: [698/782] step: 283000 Learning rate: 0.0002\n","Loss = 0.0018 (ave = 0.0029)\n","\n","epoch: [362/400] iteration: [782/782] step: 283085 Learning rate: 0.0002\n","Loss = 0.0027 (ave = 0.0029)\n","\n","epoch: [363/400] iteration: [16/782] step: 283100 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0029)\n","\n","epoch: [363/400] iteration: [116/782] step: 283200 Learning rate: 0.0002\n","Loss = 0.0018 (ave = 0.0029)\n","\n","epoch: [363/400] iteration: [216/782] step: 283300 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0029)\n","\n","epoch: [363/400] iteration: [316/782] step: 283400 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0029)\n","\n","epoch: [363/400] iteration: [416/782] step: 283500 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0029)\n","\n","epoch: [363/400] iteration: [516/782] step: 283600 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0029)\n","\n","epoch: [363/400] iteration: [616/782] step: 283700 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0029)\n","\n","epoch: [363/400] iteration: [716/782] step: 283800 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0029)\n","\n","epoch: [363/400] iteration: [782/782] step: 283867 Learning rate: 0.0002\n","Loss = 0.0029 (ave = 0.0029)\n","\n","epoch: [364/400] iteration: [34/782] step: 283900 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0029)\n","\n","epoch: [364/400] iteration: [134/782] step: 284000 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0029)\n","\n","epoch: [364/400] iteration: [234/782] step: 284100 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0029)\n","\n","epoch: [364/400] iteration: [334/782] step: 284200 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0029)\n","\n","epoch: [364/400] iteration: [434/782] step: 284300 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0029)\n","\n","epoch: [364/400] iteration: [534/782] step: 284400 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0029)\n","\n","epoch: [364/400] iteration: [634/782] step: 284500 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0029)\n","\n","epoch: [364/400] iteration: [734/782] step: 284600 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0029)\n","\n","epoch: [364/400] iteration: [782/782] step: 284649 Learning rate: 0.0002\n","Loss = 0.0031 (ave = 0.0029)\n","\n","epoch: [365/400] iteration: [52/782] step: 284700 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0029)\n","\n","epoch: [365/400] iteration: [152/782] step: 284800 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0029)\n","\n","epoch: [365/400] iteration: [252/782] step: 284900 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0029)\n","\n","epoch: [365/400] iteration: [352/782] step: 285000 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0029)\n","\n","epoch: [365/400] iteration: [452/782] step: 285100 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0029)\n","\n","epoch: [365/400] iteration: [552/782] step: 285200 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0029)\n","\n","epoch: [365/400] iteration: [652/782] step: 285300 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0029)\n","\n","epoch: [365/400] iteration: [752/782] step: 285400 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0029)\n","\n","epoch: [365/400] iteration: [782/782] step: 285431 Learning rate: 0.0002\n","Loss = 0.0034 (ave = 0.0029)\n","\n","epoch: [366/400] iteration: [70/782] step: 285500 Learning rate: 0.0002\n","Loss = 0.0018 (ave = 0.0029)\n","\n","epoch: [366/400] iteration: [170/782] step: 285600 Learning rate: 0.0002\n","Loss = 0.0018 (ave = 0.0029)\n","\n","epoch: [366/400] iteration: [270/782] step: 285700 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0029)\n","\n","epoch: [366/400] iteration: [370/782] step: 285800 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0029)\n","\n","epoch: [366/400] iteration: [470/782] step: 285900 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0029)\n","\n","epoch: [366/400] iteration: [570/782] step: 286000 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0029)\n","\n","epoch: [366/400] iteration: [670/782] step: 286100 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0029)\n","\n","epoch: [366/400] iteration: [770/782] step: 286200 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0029)\n","\n","epoch: [366/400] iteration: [782/782] step: 286213 Learning rate: 0.0002\n","Loss = 0.0034 (ave = 0.0029)\n","\n","epoch: [367/400] iteration: [88/782] step: 286300 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0029)\n","\n","epoch: [367/400] iteration: [188/782] step: 286400 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0029)\n","\n","epoch: [367/400] iteration: [288/782] step: 286500 Learning rate: 0.0002\n","Loss = 0.0017 (ave = 0.0029)\n","\n","epoch: [367/400] iteration: [388/782] step: 286600 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0029)\n","\n","epoch: [367/400] iteration: [488/782] step: 286700 Learning rate: 0.0002\n","Loss = 0.0018 (ave = 0.0029)\n","\n","epoch: [367/400] iteration: [588/782] step: 286800 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0029)\n","\n","epoch: [367/400] iteration: [688/782] step: 286900 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0029)\n","\n","epoch: [367/400] iteration: [782/782] step: 286995 Learning rate: 0.0002\n","Loss = 0.0025 (ave = 0.0029)\n","\n","epoch: [368/400] iteration: [6/782] step: 287000 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0029)\n","\n","epoch: [368/400] iteration: [106/782] step: 287100 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0029)\n","\n","epoch: [368/400] iteration: [206/782] step: 287200 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0029)\n","\n","epoch: [368/400] iteration: [306/782] step: 287300 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0029)\n","\n","epoch: [368/400] iteration: [406/782] step: 287400 Learning rate: 0.0002\n","Loss = 0.0018 (ave = 0.0029)\n","\n","epoch: [368/400] iteration: [506/782] step: 287500 Learning rate: 0.0002\n","Loss = 0.0018 (ave = 0.0029)\n","\n","epoch: [368/400] iteration: [606/782] step: 287600 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0029)\n","\n","epoch: [368/400] iteration: [706/782] step: 287700 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0029)\n","\n","epoch: [368/400] iteration: [782/782] step: 287777 Learning rate: 0.0002\n","Loss = 0.0032 (ave = 0.0029)\n","\n","epoch: [369/400] iteration: [24/782] step: 287800 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0029)\n","\n","epoch: [369/400] iteration: [124/782] step: 287900 Learning rate: 0.0002\n","Loss = 0.0018 (ave = 0.0029)\n","\n","epoch: [369/400] iteration: [224/782] step: 288000 Learning rate: 0.0002\n","Loss = 0.0018 (ave = 0.0029)\n","\n","epoch: [369/400] iteration: [324/782] step: 288100 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0029)\n","\n","epoch: [369/400] iteration: [424/782] step: 288200 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0029)\n","\n","epoch: [369/400] iteration: [524/782] step: 288300 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0029)\n","\n","epoch: [369/400] iteration: [624/782] step: 288400 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0029)\n","\n","epoch: [369/400] iteration: [724/782] step: 288500 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0029)\n","\n","epoch: [369/400] iteration: [782/782] step: 288559 Learning rate: 0.0002\n","Loss = 0.0038 (ave = 0.0029)\n","\n","epoch: [370/400] iteration: [42/782] step: 288600 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0029)\n","\n","epoch: [370/400] iteration: [142/782] step: 288700 Learning rate: 0.0002\n","Loss = 0.0018 (ave = 0.0029)\n","\n","epoch: [370/400] iteration: [242/782] step: 288800 Learning rate: 0.0002\n","Loss = 0.0018 (ave = 0.0029)\n","\n","epoch: [370/400] iteration: [342/782] step: 288900 Learning rate: 0.0002\n","Loss = 0.0023 (ave = 0.0029)\n","\n","epoch: [370/400] iteration: [442/782] step: 289000 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0029)\n","\n","epoch: [370/400] iteration: [542/782] step: 289100 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0029)\n","\n","epoch: [370/400] iteration: [642/782] step: 289200 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0029)\n","\n","epoch: [370/400] iteration: [742/782] step: 289300 Learning rate: 0.0002\n","Loss = 0.0018 (ave = 0.0029)\n","\n","epoch: [370/400] iteration: [782/782] step: 289341 Learning rate: 0.0002\n","Loss = 0.0038 (ave = 0.0029)\n","\n","epoch: [371/400] iteration: [60/782] step: 289400 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0029)\n","\n","epoch: [371/400] iteration: [160/782] step: 289500 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0029)\n","\n","epoch: [371/400] iteration: [260/782] step: 289600 Learning rate: 0.0002\n","Loss = 0.0018 (ave = 0.0029)\n","\n","epoch: [371/400] iteration: [360/782] step: 289700 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0029)\n","\n","epoch: [371/400] iteration: [460/782] step: 289800 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0029)\n","\n","epoch: [371/400] iteration: [560/782] step: 289900 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0029)\n","\n","epoch: [371/400] iteration: [660/782] step: 290000 Learning rate: 0.0002\n","Loss = 0.0018 (ave = 0.0029)\n","\n","epoch: [371/400] iteration: [760/782] step: 290100 Learning rate: 0.0002\n","Loss = 0.0018 (ave = 0.0029)\n","\n","epoch: [371/400] iteration: [782/782] step: 290123 Learning rate: 0.0002\n","Loss = 0.0030 (ave = 0.0029)\n","\n","epoch: [372/400] iteration: [78/782] step: 290200 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0029)\n","\n","epoch: [372/400] iteration: [178/782] step: 290300 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0029)\n","\n","epoch: [372/400] iteration: [278/782] step: 290400 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0029)\n","\n","epoch: [372/400] iteration: [378/782] step: 290500 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0029)\n","\n","epoch: [372/400] iteration: [478/782] step: 290600 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0029)\n","\n","epoch: [372/400] iteration: [578/782] step: 290700 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0029)\n","\n","epoch: [372/400] iteration: [678/782] step: 290800 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0029)\n","\n","epoch: [372/400] iteration: [778/782] step: 290900 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0029)\n","\n","epoch: [372/400] iteration: [782/782] step: 290905 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0029)\n","\n","epoch: [373/400] iteration: [96/782] step: 291000 Learning rate: 0.0002\n","Loss = 0.0018 (ave = 0.0029)\n","\n","epoch: [373/400] iteration: [196/782] step: 291100 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0029)\n","\n","epoch: [373/400] iteration: [296/782] step: 291200 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0029)\n","\n","epoch: [373/400] iteration: [396/782] step: 291300 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0029)\n","\n","epoch: [373/400] iteration: [496/782] step: 291400 Learning rate: 0.0002\n","Loss = 0.0018 (ave = 0.0029)\n","\n","epoch: [373/400] iteration: [596/782] step: 291500 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0029)\n","\n","epoch: [373/400] iteration: [696/782] step: 291600 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0029)\n","\n","epoch: [373/400] iteration: [782/782] step: 291687 Learning rate: 0.0002\n","Loss = 0.0031 (ave = 0.0029)\n","\n","epoch: [374/400] iteration: [14/782] step: 291700 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0029)\n","\n","epoch: [374/400] iteration: [114/782] step: 291800 Learning rate: 0.0002\n","Loss = 0.0018 (ave = 0.0029)\n","\n","epoch: [374/400] iteration: [214/782] step: 291900 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0029)\n","\n","epoch: [374/400] iteration: [314/782] step: 292000 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0029)\n","\n","epoch: [374/400] iteration: [414/782] step: 292100 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0029)\n","\n","epoch: [374/400] iteration: [514/782] step: 292200 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0029)\n","\n","epoch: [374/400] iteration: [614/782] step: 292300 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0029)\n","\n","epoch: [374/400] iteration: [714/782] step: 292400 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0029)\n","\n","epoch: [374/400] iteration: [782/782] step: 292469 Learning rate: 0.0002\n","Loss = 0.0029 (ave = 0.0029)\n","\n","epoch: [375/400] iteration: [32/782] step: 292500 Learning rate: 0.0002\n","Loss = 0.0018 (ave = 0.0029)\n","\n","epoch: [375/400] iteration: [132/782] step: 292600 Learning rate: 0.0002\n","Loss = 0.0018 (ave = 0.0029)\n","\n","epoch: [375/400] iteration: [232/782] step: 292700 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0029)\n","\n","epoch: [375/400] iteration: [332/782] step: 292800 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0029)\n","\n","epoch: [375/400] iteration: [432/782] step: 292900 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0029)\n","\n","epoch: [375/400] iteration: [532/782] step: 293000 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0029)\n","\n","epoch: [375/400] iteration: [632/782] step: 293100 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0029)\n","\n","epoch: [375/400] iteration: [732/782] step: 293200 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0029)\n","\n","epoch: [375/400] iteration: [782/782] step: 293251 Learning rate: 0.0002\n","Loss = 0.0025 (ave = 0.0029)\n","\n","epoch: [376/400] iteration: [50/782] step: 293300 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0029)\n","\n","epoch: [376/400] iteration: [150/782] step: 293400 Learning rate: 0.0002\n","Loss = 0.0018 (ave = 0.0029)\n","\n","epoch: [376/400] iteration: [250/782] step: 293500 Learning rate: 0.0002\n","Loss = 0.0018 (ave = 0.0029)\n","\n","epoch: [376/400] iteration: [350/782] step: 293600 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0029)\n","\n","epoch: [376/400] iteration: [450/782] step: 293700 Learning rate: 0.0002\n","Loss = 0.0017 (ave = 0.0029)\n","\n","epoch: [376/400] iteration: [550/782] step: 293800 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0029)\n","\n","epoch: [376/400] iteration: [650/782] step: 293900 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0029)\n","\n","epoch: [376/400] iteration: [750/782] step: 294000 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0029)\n","\n","epoch: [376/400] iteration: [782/782] step: 294033 Learning rate: 0.0002\n","Loss = 0.0028 (ave = 0.0029)\n","\n","epoch: [377/400] iteration: [68/782] step: 294100 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0029)\n","\n","epoch: [377/400] iteration: [168/782] step: 294200 Learning rate: 0.0002\n","Loss = 0.0017 (ave = 0.0029)\n","\n","epoch: [377/400] iteration: [268/782] step: 294300 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0029)\n","\n","epoch: [377/400] iteration: [368/782] step: 294400 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0029)\n","\n","epoch: [377/400] iteration: [468/782] step: 294500 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0029)\n","\n","epoch: [377/400] iteration: [568/782] step: 294600 Learning rate: 0.0002\n","Loss = 0.0017 (ave = 0.0029)\n","\n","epoch: [377/400] iteration: [668/782] step: 294700 Learning rate: 0.0002\n","Loss = 0.0018 (ave = 0.0029)\n","\n","epoch: [377/400] iteration: [768/782] step: 294800 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0029)\n","\n","epoch: [377/400] iteration: [782/782] step: 294815 Learning rate: 0.0002\n","Loss = 0.0029 (ave = 0.0029)\n","\n","epoch: [378/400] iteration: [86/782] step: 294900 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0029)\n","\n","epoch: [378/400] iteration: [186/782] step: 295000 Learning rate: 0.0002\n","Loss = 0.0018 (ave = 0.0029)\n","\n","epoch: [378/400] iteration: [286/782] step: 295100 Learning rate: 0.0002\n","Loss = 0.0018 (ave = 0.0029)\n","\n","epoch: [378/400] iteration: [386/782] step: 295200 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0029)\n","\n","epoch: [378/400] iteration: [486/782] step: 295300 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0029)\n","\n","epoch: [378/400] iteration: [586/782] step: 295400 Learning rate: 0.0002\n","Loss = 0.0018 (ave = 0.0029)\n","\n","epoch: [378/400] iteration: [686/782] step: 295500 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0029)\n","\n","epoch: [378/400] iteration: [782/782] step: 295597 Learning rate: 0.0002\n","Loss = 0.0026 (ave = 0.0029)\n","\n","epoch: [379/400] iteration: [4/782] step: 295600 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0029)\n","\n","epoch: [379/400] iteration: [104/782] step: 295700 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0029)\n","\n","epoch: [379/400] iteration: [204/782] step: 295800 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0029)\n","\n","epoch: [379/400] iteration: [304/782] step: 295900 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0029)\n","\n","epoch: [379/400] iteration: [404/782] step: 296000 Learning rate: 0.0002\n","Loss = 0.0018 (ave = 0.0029)\n","\n","epoch: [379/400] iteration: [504/782] step: 296100 Learning rate: 0.0002\n","Loss = 0.0018 (ave = 0.0029)\n","\n","epoch: [379/400] iteration: [604/782] step: 296200 Learning rate: 0.0002\n","Loss = 0.0018 (ave = 0.0029)\n","\n","epoch: [379/400] iteration: [704/782] step: 296300 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0029)\n","\n","epoch: [379/400] iteration: [782/782] step: 296379 Learning rate: 0.0002\n","Loss = 0.0035 (ave = 0.0029)\n","\n","epoch: [380/400] iteration: [22/782] step: 296400 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0029)\n","\n","epoch: [380/400] iteration: [122/782] step: 296500 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0029)\n","\n","epoch: [380/400] iteration: [222/782] step: 296600 Learning rate: 0.0002\n","Loss = 0.0018 (ave = 0.0029)\n","\n","epoch: [380/400] iteration: [322/782] step: 296700 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0029)\n","\n","epoch: [380/400] iteration: [422/782] step: 296800 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0029)\n","\n","epoch: [380/400] iteration: [522/782] step: 296900 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0029)\n","\n","epoch: [380/400] iteration: [622/782] step: 297000 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0029)\n","\n","epoch: [380/400] iteration: [722/782] step: 297100 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0029)\n","\n","epoch: [380/400] iteration: [782/782] step: 297161 Learning rate: 0.0002\n","Loss = 0.0023 (ave = 0.0029)\n","\n","epoch: [381/400] iteration: [40/782] step: 297200 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0029)\n","\n","epoch: [381/400] iteration: [140/782] step: 297300 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0029)\n","\n","epoch: [381/400] iteration: [240/782] step: 297400 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0029)\n","\n","epoch: [381/400] iteration: [340/782] step: 297500 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0029)\n","\n","epoch: [381/400] iteration: [440/782] step: 297600 Learning rate: 0.0002\n","Loss = 0.0017 (ave = 0.0029)\n","\n","epoch: [381/400] iteration: [540/782] step: 297700 Learning rate: 0.0002\n","Loss = 0.0026 (ave = 0.0029)\n","\n","epoch: [381/400] iteration: [640/782] step: 297800 Learning rate: 0.0002\n","Loss = 0.0024 (ave = 0.0029)\n","\n","epoch: [381/400] iteration: [740/782] step: 297900 Learning rate: 0.0002\n","Loss = 0.0018 (ave = 0.0029)\n","\n","epoch: [381/400] iteration: [782/782] step: 297943 Learning rate: 0.0002\n","Loss = 0.0026 (ave = 0.0029)\n","\n","epoch: [382/400] iteration: [58/782] step: 298000 Learning rate: 0.0002\n","Loss = 0.0018 (ave = 0.0029)\n","\n","epoch: [382/400] iteration: [158/782] step: 298100 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0029)\n","\n","epoch: [382/400] iteration: [258/782] step: 298200 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0029)\n","\n","epoch: [382/400] iteration: [358/782] step: 298300 Learning rate: 0.0002\n","Loss = 0.0018 (ave = 0.0029)\n","\n","epoch: [382/400] iteration: [458/782] step: 298400 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0029)\n","\n","epoch: [382/400] iteration: [558/782] step: 298500 Learning rate: 0.0002\n","Loss = 0.0017 (ave = 0.0029)\n","\n","epoch: [382/400] iteration: [658/782] step: 298600 Learning rate: 0.0002\n","Loss = 0.0018 (ave = 0.0029)\n","\n","epoch: [382/400] iteration: [758/782] step: 298700 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0029)\n","\n","epoch: [382/400] iteration: [782/782] step: 298725 Learning rate: 0.0002\n","Loss = 0.0092 (ave = 0.0029)\n","\n","epoch: [383/400] iteration: [76/782] step: 298800 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0029)\n","\n","epoch: [383/400] iteration: [176/782] step: 298900 Learning rate: 0.0002\n","Loss = 0.0018 (ave = 0.0029)\n","\n","epoch: [383/400] iteration: [276/782] step: 299000 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0029)\n","\n","epoch: [383/400] iteration: [376/782] step: 299100 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0029)\n","\n","epoch: [383/400] iteration: [476/782] step: 299200 Learning rate: 0.0002\n","Loss = 0.0018 (ave = 0.0029)\n","\n","epoch: [383/400] iteration: [576/782] step: 299300 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0029)\n","\n","epoch: [383/400] iteration: [676/782] step: 299400 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0029)\n","\n","epoch: [383/400] iteration: [776/782] step: 299500 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0029)\n","\n","epoch: [383/400] iteration: [782/782] step: 299507 Learning rate: 0.0002\n","Loss = 0.0027 (ave = 0.0029)\n","\n","epoch: [384/400] iteration: [94/782] step: 299600 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0029)\n","\n","epoch: [384/400] iteration: [194/782] step: 299700 Learning rate: 0.0002\n","Loss = 0.0018 (ave = 0.0029)\n","\n","epoch: [384/400] iteration: [294/782] step: 299800 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0029)\n","\n","epoch: [384/400] iteration: [394/782] step: 299900 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0029)\n","\n","epoch: [384/400] iteration: [494/782] step: 300000 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0029)\n","\n","epoch: [384/400] iteration: [594/782] step: 300100 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0029)\n","\n","epoch: [384/400] iteration: [694/782] step: 300200 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0029)\n","\n","epoch: [384/400] iteration: [782/782] step: 300289 Learning rate: 0.0002\n","Loss = 0.0024 (ave = 0.0029)\n","\n","epoch: [385/400] iteration: [12/782] step: 300300 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0029)\n","\n","epoch: [385/400] iteration: [112/782] step: 300400 Learning rate: 0.0002\n","Loss = 0.0018 (ave = 0.0029)\n","\n","epoch: [385/400] iteration: [212/782] step: 300500 Learning rate: 0.0002\n","Loss = 0.0018 (ave = 0.0029)\n","\n","epoch: [385/400] iteration: [312/782] step: 300600 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0029)\n","\n","epoch: [385/400] iteration: [412/782] step: 300700 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0029)\n","\n","epoch: [385/400] iteration: [512/782] step: 300800 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0029)\n","\n","epoch: [385/400] iteration: [612/782] step: 300900 Learning rate: 0.0002\n","Loss = 0.0018 (ave = 0.0029)\n","\n","epoch: [385/400] iteration: [712/782] step: 301000 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0029)\n","\n","epoch: [385/400] iteration: [782/782] step: 301071 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0029)\n","\n","epoch: [386/400] iteration: [30/782] step: 301100 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0029)\n","\n","epoch: [386/400] iteration: [130/782] step: 301200 Learning rate: 0.0002\n","Loss = 0.0018 (ave = 0.0029)\n","\n","epoch: [386/400] iteration: [230/782] step: 301300 Learning rate: 0.0002\n","Loss = 0.0018 (ave = 0.0029)\n","\n","epoch: [386/400] iteration: [330/782] step: 301400 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0029)\n","\n","epoch: [386/400] iteration: [430/782] step: 301500 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0029)\n","\n","epoch: [386/400] iteration: [530/782] step: 301600 Learning rate: 0.0002\n","Loss = 0.0018 (ave = 0.0029)\n","\n","epoch: [386/400] iteration: [630/782] step: 301700 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0029)\n","\n","epoch: [386/400] iteration: [730/782] step: 301800 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0029)\n","\n","epoch: [386/400] iteration: [782/782] step: 301853 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0029)\n","\n","epoch: [387/400] iteration: [48/782] step: 301900 Learning rate: 0.0002\n","Loss = 0.0018 (ave = 0.0029)\n","\n","epoch: [387/400] iteration: [148/782] step: 302000 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0029)\n","\n","epoch: [387/400] iteration: [248/782] step: 302100 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0028)\n","\n","epoch: [387/400] iteration: [348/782] step: 302200 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0028)\n","\n","epoch: [387/400] iteration: [448/782] step: 302300 Learning rate: 0.0002\n","Loss = 0.0018 (ave = 0.0028)\n","\n","epoch: [387/400] iteration: [548/782] step: 302400 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0028)\n","\n","epoch: [387/400] iteration: [648/782] step: 302500 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0028)\n","\n","epoch: [387/400] iteration: [748/782] step: 302600 Learning rate: 0.0002\n","Loss = 0.0018 (ave = 0.0028)\n","\n","epoch: [387/400] iteration: [782/782] step: 302635 Learning rate: 0.0002\n","Loss = 0.0027 (ave = 0.0028)\n","\n","epoch: [388/400] iteration: [66/782] step: 302700 Learning rate: 0.0002\n","Loss = 0.0018 (ave = 0.0028)\n","\n","epoch: [388/400] iteration: [166/782] step: 302800 Learning rate: 0.0002\n","Loss = 0.0018 (ave = 0.0028)\n","\n","epoch: [388/400] iteration: [266/782] step: 302900 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0028)\n","\n","epoch: [388/400] iteration: [366/782] step: 303000 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0028)\n","\n","epoch: [388/400] iteration: [466/782] step: 303100 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0028)\n","\n","epoch: [388/400] iteration: [566/782] step: 303200 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0028)\n","\n","epoch: [388/400] iteration: [666/782] step: 303300 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0028)\n","\n","epoch: [388/400] iteration: [766/782] step: 303400 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0028)\n","\n","epoch: [388/400] iteration: [782/782] step: 303417 Learning rate: 0.0002\n","Loss = 0.0023 (ave = 0.0028)\n","\n","epoch: [389/400] iteration: [84/782] step: 303500 Learning rate: 0.0002\n","Loss = 0.0018 (ave = 0.0028)\n","\n","epoch: [389/400] iteration: [184/782] step: 303600 Learning rate: 0.0002\n","Loss = 0.0018 (ave = 0.0028)\n","\n","epoch: [389/400] iteration: [284/782] step: 303700 Learning rate: 0.0002\n","Loss = 0.0018 (ave = 0.0028)\n","\n","epoch: [389/400] iteration: [384/782] step: 303800 Learning rate: 0.0002\n","Loss = 0.0018 (ave = 0.0028)\n","\n","epoch: [389/400] iteration: [484/782] step: 303900 Learning rate: 0.0002\n","Loss = 0.0018 (ave = 0.0028)\n","\n","epoch: [389/400] iteration: [584/782] step: 304000 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0028)\n","\n","epoch: [389/400] iteration: [684/782] step: 304100 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0028)\n","\n","epoch: [389/400] iteration: [782/782] step: 304199 Learning rate: 0.0002\n","Loss = 0.0030 (ave = 0.0028)\n","\n","epoch: [390/400] iteration: [2/782] step: 304200 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0028)\n","\n","epoch: [390/400] iteration: [102/782] step: 304300 Learning rate: 0.0002\n","Loss = 0.0018 (ave = 0.0028)\n","\n","epoch: [390/400] iteration: [202/782] step: 304400 Learning rate: 0.0002\n","Loss = 0.0018 (ave = 0.0028)\n","\n","epoch: [390/400] iteration: [302/782] step: 304500 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0028)\n","\n","epoch: [390/400] iteration: [402/782] step: 304600 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0028)\n","\n","epoch: [390/400] iteration: [502/782] step: 304700 Learning rate: 0.0002\n","Loss = 0.0018 (ave = 0.0028)\n","\n","epoch: [390/400] iteration: [602/782] step: 304800 Learning rate: 0.0002\n","Loss = 0.0018 (ave = 0.0028)\n","\n","epoch: [390/400] iteration: [702/782] step: 304900 Learning rate: 0.0002\n","Loss = 0.0018 (ave = 0.0028)\n","\n","epoch: [390/400] iteration: [782/782] step: 304981 Learning rate: 0.0002\n","Loss = 0.0062 (ave = 0.0028)\n","\n","epoch: [391/400] iteration: [20/782] step: 305000 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0028)\n","\n","epoch: [391/400] iteration: [120/782] step: 305100 Learning rate: 0.0002\n","Loss = 0.0018 (ave = 0.0028)\n","\n","epoch: [391/400] iteration: [220/782] step: 305200 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0028)\n","\n","epoch: [391/400] iteration: [320/782] step: 305300 Learning rate: 0.0002\n","Loss = 0.0017 (ave = 0.0028)\n","\n","epoch: [391/400] iteration: [420/782] step: 305400 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0028)\n","\n","epoch: [391/400] iteration: [520/782] step: 305500 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0028)\n","\n","epoch: [391/400] iteration: [620/782] step: 305600 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0028)\n","\n","epoch: [391/400] iteration: [720/782] step: 305700 Learning rate: 0.0002\n","Loss = 0.0018 (ave = 0.0028)\n","\n","epoch: [391/400] iteration: [782/782] step: 305763 Learning rate: 0.0002\n","Loss = 0.0026 (ave = 0.0028)\n","\n","epoch: [392/400] iteration: [38/782] step: 305800 Learning rate: 0.0002\n","Loss = 0.0018 (ave = 0.0028)\n","\n","epoch: [392/400] iteration: [138/782] step: 305900 Learning rate: 0.0002\n","Loss = 0.0018 (ave = 0.0028)\n","\n","epoch: [392/400] iteration: [238/782] step: 306000 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0028)\n","\n","epoch: [392/400] iteration: [338/782] step: 306100 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0028)\n","\n","epoch: [392/400] iteration: [438/782] step: 306200 Learning rate: 0.0002\n","Loss = 0.0018 (ave = 0.0028)\n","\n","epoch: [392/400] iteration: [538/782] step: 306300 Learning rate: 0.0002\n","Loss = 0.0017 (ave = 0.0028)\n","\n","epoch: [392/400] iteration: [638/782] step: 306400 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0028)\n","\n","epoch: [392/400] iteration: [738/782] step: 306500 Learning rate: 0.0002\n","Loss = 0.0018 (ave = 0.0028)\n","\n","epoch: [392/400] iteration: [782/782] step: 306545 Learning rate: 0.0002\n","Loss = 0.0025 (ave = 0.0028)\n","\n","epoch: [393/400] iteration: [56/782] step: 306600 Learning rate: 0.0002\n","Loss = 0.0018 (ave = 0.0028)\n","\n","epoch: [393/400] iteration: [156/782] step: 306700 Learning rate: 0.0002\n","Loss = 0.0018 (ave = 0.0028)\n","\n","epoch: [393/400] iteration: [256/782] step: 306800 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0028)\n","\n","epoch: [393/400] iteration: [356/782] step: 306900 Learning rate: 0.0002\n","Loss = 0.0018 (ave = 0.0028)\n","\n","epoch: [393/400] iteration: [456/782] step: 307000 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0028)\n","\n","epoch: [393/400] iteration: [556/782] step: 307100 Learning rate: 0.0002\n","Loss = 0.0018 (ave = 0.0028)\n","\n","epoch: [393/400] iteration: [656/782] step: 307200 Learning rate: 0.0002\n","Loss = 0.0018 (ave = 0.0028)\n","\n","epoch: [393/400] iteration: [756/782] step: 307300 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0028)\n","\n","epoch: [393/400] iteration: [782/782] step: 307327 Learning rate: 0.0002\n","Loss = 0.0034 (ave = 0.0028)\n","\n","epoch: [394/400] iteration: [74/782] step: 307400 Learning rate: 0.0002\n","Loss = 0.0018 (ave = 0.0028)\n","\n","epoch: [394/400] iteration: [174/782] step: 307500 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0028)\n","\n","epoch: [394/400] iteration: [274/782] step: 307600 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0028)\n","\n","epoch: [394/400] iteration: [374/782] step: 307700 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0028)\n","\n","epoch: [394/400] iteration: [474/782] step: 307800 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0028)\n","\n","epoch: [394/400] iteration: [574/782] step: 307900 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0028)\n","\n","epoch: [394/400] iteration: [674/782] step: 308000 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0028)\n","\n","epoch: [394/400] iteration: [774/782] step: 308100 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0028)\n","\n","epoch: [394/400] iteration: [782/782] step: 308109 Learning rate: 0.0002\n","Loss = 0.0031 (ave = 0.0028)\n","\n","epoch: [395/400] iteration: [92/782] step: 308200 Learning rate: 0.0002\n","Loss = 0.0018 (ave = 0.0028)\n","\n","epoch: [395/400] iteration: [192/782] step: 308300 Learning rate: 0.0002\n","Loss = 0.0018 (ave = 0.0028)\n","\n","epoch: [395/400] iteration: [292/782] step: 308400 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0028)\n","\n","epoch: [395/400] iteration: [392/782] step: 308500 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0028)\n","\n","epoch: [395/400] iteration: [492/782] step: 308600 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0028)\n","\n","epoch: [395/400] iteration: [592/782] step: 308700 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0028)\n","\n","epoch: [395/400] iteration: [692/782] step: 308800 Learning rate: 0.0002\n","Loss = 0.0018 (ave = 0.0028)\n","\n","epoch: [395/400] iteration: [782/782] step: 308891 Learning rate: 0.0002\n","Loss = 0.0031 (ave = 0.0028)\n","\n","epoch: [396/400] iteration: [10/782] step: 308900 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0028)\n","\n","epoch: [396/400] iteration: [110/782] step: 309000 Learning rate: 0.0002\n","Loss = 0.0018 (ave = 0.0028)\n","\n","epoch: [396/400] iteration: [210/782] step: 309100 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0028)\n","\n","epoch: [396/400] iteration: [310/782] step: 309200 Learning rate: 0.0002\n","Loss = 0.0017 (ave = 0.0028)\n","\n","epoch: [396/400] iteration: [410/782] step: 309300 Learning rate: 0.0002\n","Loss = 0.0017 (ave = 0.0028)\n","\n","epoch: [396/400] iteration: [510/782] step: 309400 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0028)\n","\n","epoch: [396/400] iteration: [610/782] step: 309500 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0028)\n","\n","epoch: [396/400] iteration: [710/782] step: 309600 Learning rate: 0.0002\n","Loss = 0.0018 (ave = 0.0028)\n","\n","epoch: [396/400] iteration: [782/782] step: 309673 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0028)\n","\n","epoch: [397/400] iteration: [28/782] step: 309700 Learning rate: 0.0002\n","Loss = 0.0017 (ave = 0.0028)\n","\n","epoch: [397/400] iteration: [128/782] step: 309800 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0028)\n","\n","epoch: [397/400] iteration: [228/782] step: 309900 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0028)\n","\n","epoch: [397/400] iteration: [328/782] step: 310000 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0028)\n","\n","epoch: [397/400] iteration: [428/782] step: 310100 Learning rate: 0.0002\n","Loss = 0.0018 (ave = 0.0028)\n","\n","epoch: [397/400] iteration: [528/782] step: 310200 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0028)\n","\n","epoch: [397/400] iteration: [628/782] step: 310300 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0028)\n","\n","epoch: [397/400] iteration: [728/782] step: 310400 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0028)\n","\n","epoch: [397/400] iteration: [782/782] step: 310455 Learning rate: 0.0002\n","Loss = 0.0024 (ave = 0.0028)\n","\n","epoch: [398/400] iteration: [46/782] step: 310500 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0028)\n","\n","epoch: [398/400] iteration: [146/782] step: 310600 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0028)\n","\n","epoch: [398/400] iteration: [246/782] step: 310700 Learning rate: 0.0002\n","Loss = 0.0018 (ave = 0.0028)\n","\n","epoch: [398/400] iteration: [346/782] step: 310800 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0028)\n","\n","epoch: [398/400] iteration: [446/782] step: 310900 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0028)\n","\n","epoch: [398/400] iteration: [546/782] step: 311000 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0028)\n","\n","epoch: [398/400] iteration: [646/782] step: 311100 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0028)\n","\n","epoch: [398/400] iteration: [746/782] step: 311200 Learning rate: 0.0002\n","Loss = 0.0018 (ave = 0.0028)\n","\n","epoch: [398/400] iteration: [782/782] step: 311237 Learning rate: 0.0002\n","Loss = 0.0024 (ave = 0.0028)\n","\n","epoch: [399/400] iteration: [64/782] step: 311300 Learning rate: 0.0002\n","Loss = 0.0018 (ave = 0.0028)\n","\n","epoch: [399/400] iteration: [164/782] step: 311400 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0028)\n","\n","epoch: [399/400] iteration: [264/782] step: 311500 Learning rate: 0.0002\n","Loss = 0.0020 (ave = 0.0028)\n","\n","epoch: [399/400] iteration: [364/782] step: 311600 Learning rate: 0.0002\n","Loss = 0.0018 (ave = 0.0028)\n","\n","epoch: [399/400] iteration: [464/782] step: 311700 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0028)\n","\n","epoch: [399/400] iteration: [564/782] step: 311800 Learning rate: 0.0002\n","Loss = 0.0021 (ave = 0.0028)\n","\n","epoch: [399/400] iteration: [664/782] step: 311900 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0028)\n","\n","epoch: [399/400] iteration: [764/782] step: 312000 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0028)\n","\n","epoch: [399/400] iteration: [782/782] step: 312019 Learning rate: 0.0002\n","Loss = 0.0023 (ave = 0.0028)\n","\n","epoch: [400/400] iteration: [82/782] step: 312100 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0028)\n","\n","epoch: [400/400] iteration: [182/782] step: 312200 Learning rate: 0.0002\n","Loss = 0.0018 (ave = 0.0028)\n","\n","epoch: [400/400] iteration: [282/782] step: 312300 Learning rate: 0.0002\n","Loss = 0.0018 (ave = 0.0028)\n","\n","epoch: [400/400] iteration: [382/782] step: 312400 Learning rate: 0.0002\n","Loss = 0.0022 (ave = 0.0028)\n","\n","epoch: [400/400] iteration: [482/782] step: 312500 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0028)\n","\n","epoch: [400/400] iteration: [582/782] step: 312600 Learning rate: 0.0002\n","Loss = 0.0017 (ave = 0.0028)\n","\n","epoch: [400/400] iteration: [682/782] step: 312700 Learning rate: 0.0002\n","Loss = 0.0019 (ave = 0.0028)\n","\n","epoch: [400/400] iteration: [782/782] step: 312800 Learning rate: 0.0002\n","Loss = 0.0023 (ave = 0.0028)\n","\n","epoch: [400/400] iteration: [782/782] step: 312801 Learning rate: 0.0002\n","Loss = 0.0023 (ave = 0.0028)\n","\n"]}],"source":["%cd /content/drive/My Drive/BAGAN\n","\n","! python autoencoder.py \\\n","--batch_size=64 \\\n","--num_workers=2 \\\n","--max_epoches=400 \\\n","--z_dim=100 \\\n","--gf_dim=128 \\\n","--df_dim=128 \\\n","--base_lr=0.0002 \\\n","--board_interval=100 \\\n","--image_interval=1000 \\\n","--save_interval=5000 \\\n","--dataset='cifar10' \\\n","--save_dir='exp_cifar10' \\\n","--train_dir='./datasets/cifar10' \\\n"]},{"cell_type":"markdown","metadata":{"id":"6G2SmIJ2k3Bg"},"source":["# GAN initialization"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25321,"status":"ok","timestamp":1654046129308,"user":{"displayName":"Escano Xiao","userId":"11935749661615477607"},"user_tz":-480},"id":"272sssGHkK8i","outputId":"f5249741-8283-4044-aaa0-429bd6e4e6cd"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/My Drive/BAGAN\n","Files already downloaded and verified\n"]}],"source":["%cd /content/drive/My Drive/BAGAN\n","\n","! python cclvg.py \\\n","--batch_size=64 \\\n","--num_workers=2 \\\n","--z_dim=100 \\\n","--gf_dim=128 \\\n","--df_dim=128 \\\n","--dataset='cifar10' \\\n","--save_dir='./exp_cifar10/20220531_H114137' \\\n","--pretrained_dir='./exp_cifar10/20220531_H114137/autoencoder/BestResult' \\\n","--train_dir='./datasets/cifar10' \\\n"]},{"cell_type":"markdown","metadata":{"id":"UY2HFsRSk-Qf"},"source":["# GAN training"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Nwa5WLMwk9CH","outputId":"40ffa4ab-da48-47e3-8013-a3af946556ac"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/My Drive/BAGAN\n","Files already downloaded and verified\n","/content/drive/My Drive/BAGAN/modules.py:161: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  output = self.softmax(self.fc_aux(h3.view(-1, self.df_dim*8*2*2)))\n","epoch: [1/500] iteration: [100/782] step: 100 Learning rate: 0.0002\n","generator_loss = 2.7441 (ave = 3.1189)\n","discriminator_loss = 2.4901 (ave = 2.5586)\n","\n","epoch: [1/500] iteration: [200/782] step: 200 Learning rate: 0.0002\n","generator_loss = 2.5751 (ave = 2.8317)\n","discriminator_loss = 2.9279 (ave = 2.5367)\n","\n","epoch: [1/500] iteration: [300/782] step: 300 Learning rate: 0.0002\n","generator_loss = 1.6205 (ave = 2.5678)\n","discriminator_loss = 2.5816 (ave = 2.5942)\n","\n","epoch: [1/500] iteration: [400/782] step: 400 Learning rate: 0.0002\n","generator_loss = 1.6161 (ave = 2.4164)\n","discriminator_loss = 2.5645 (ave = 2.7180)\n","\n","epoch: [1/500] iteration: [500/782] step: 500 Learning rate: 0.0002\n","generator_loss = 1.4105 (ave = 2.2799)\n","discriminator_loss = 3.7306 (ave = 2.8730)\n","\n","epoch: [1/500] iteration: [600/782] step: 600 Learning rate: 0.0002\n","generator_loss = 1.5466 (ave = 2.1773)\n","discriminator_loss = 4.9859 (ave = 3.0121)\n","\n","epoch: [1/500] iteration: [700/782] step: 700 Learning rate: 0.0002\n","generator_loss = 1.0239 (ave = 2.0816)\n","discriminator_loss = 6.3120 (ave = 3.1979)\n","\n","epoch: [1/500] iteration: [782/782] step: 783 Learning rate: 0.0002\n","generator_loss = 1.6035 (ave = 2.0155)\n","discriminator_loss = 8.8898 (ave = 3.3459)\n","\n","Real Accuracy : 37.828 Fake Accuracy : 2.972\n","epoch: [2/500] iteration: [18/782] step: 800 Learning rate: 0.0002\n","generator_loss = 1.4907 (ave = 1.9994)\n","discriminator_loss = 4.7552 (ave = 3.3794)\n","\n","epoch: [2/500] iteration: [118/782] step: 900 Learning rate: 0.0002\n","generator_loss = 1.2331 (ave = 1.9303)\n","discriminator_loss = 4.3990 (ave = 3.5254)\n","\n","epoch: [2/500] iteration: [218/782] step: 1000 Learning rate: 0.0002\n","generator_loss = 1.1427 (ave = 1.8688)\n","discriminator_loss = 3.8404 (ave = 3.6766)\n","\n","epoch: [2/500] iteration: [318/782] step: 1100 Learning rate: 0.0002\n","generator_loss = 0.8878 (ave = 1.8160)\n","discriminator_loss = 5.8599 (ave = 3.8809)\n","\n","epoch: [2/500] iteration: [418/782] step: 1200 Learning rate: 0.0002\n","generator_loss = 1.0621 (ave = 1.7630)\n","discriminator_loss = 6.3127 (ave = 4.0169)\n","\n","epoch: [2/500] iteration: [518/782] step: 1300 Learning rate: 0.0002\n","generator_loss = 1.0670 (ave = 1.7256)\n","discriminator_loss = 5.8156 (ave = 4.1536)\n","\n","epoch: [2/500] iteration: [618/782] step: 1400 Learning rate: 0.0002\n","generator_loss = 0.9793 (ave = 1.6886)\n","discriminator_loss = 6.0803 (ave = 4.2661)\n","\n","epoch: [2/500] iteration: [718/782] step: 1500 Learning rate: 0.0002\n","generator_loss = 0.8675 (ave = 1.6506)\n","discriminator_loss = 4.6645 (ave = 4.3974)\n","\n","epoch: [2/500] iteration: [782/782] step: 1565 Learning rate: 0.0002\n","generator_loss = 0.6736 (ave = 1.6291)\n","discriminator_loss = 7.3853 (ave = 4.4714)\n","\n","Real Accuracy : 59.778 Fake Accuracy : 0.926\n","epoch: [3/500] iteration: [36/782] step: 1600 Learning rate: 0.0002\n","generator_loss = 2.0074 (ave = 1.6154)\n","discriminator_loss = 9.8475 (ave = 4.4983)\n","\n","epoch: [3/500] iteration: [136/782] step: 1700 Learning rate: 0.0002\n","generator_loss = 1.0171 (ave = 1.5838)\n","discriminator_loss = 5.3857 (ave = 4.6428)\n","\n","epoch: [3/500] iteration: [236/782] step: 1800 Learning rate: 0.0002\n","generator_loss = 0.9414 (ave = 1.5531)\n","discriminator_loss = 8.7223 (ave = 4.7739)\n","\n","epoch: [3/500] iteration: [336/782] step: 1900 Learning rate: 0.0002\n","generator_loss = 0.9523 (ave = 1.5239)\n","discriminator_loss = 7.2555 (ave = 4.8593)\n","\n","epoch: [3/500] iteration: [436/782] step: 2000 Learning rate: 0.0002\n","generator_loss = 1.0222 (ave = 1.4955)\n","discriminator_loss = 9.0293 (ave = 4.9674)\n","\n","epoch: [3/500] iteration: [536/782] step: 2100 Learning rate: 0.0002\n","generator_loss = 0.8317 (ave = 1.4712)\n","discriminator_loss = 9.8257 (ave = 5.1066)\n","\n","epoch: [3/500] iteration: [636/782] step: 2200 Learning rate: 0.0002\n","generator_loss = 1.7329 (ave = 1.4510)\n","discriminator_loss = 11.3695 (ave = 5.2241)\n","\n","epoch: [3/500] iteration: [736/782] step: 2300 Learning rate: 0.0002\n","generator_loss = 1.1203 (ave = 1.4319)\n","discriminator_loss = 5.3143 (ave = 5.3198)\n","\n","epoch: [3/500] iteration: [782/782] step: 2347 Learning rate: 0.0002\n","generator_loss = 1.0171 (ave = 1.4232)\n","discriminator_loss = 6.9342 (ave = 5.3515)\n","\n","Real Accuracy : 66.916 Fake Accuracy : 0.546\n","epoch: [4/500] iteration: [54/782] step: 2400 Learning rate: 0.0002\n","generator_loss = 0.5735 (ave = 1.4098)\n","discriminator_loss = 6.3445 (ave = 5.4127)\n","\n","epoch: [4/500] iteration: [154/782] step: 2500 Learning rate: 0.0002\n","generator_loss = 0.8710 (ave = 1.3865)\n","discriminator_loss = 7.8636 (ave = 5.5513)\n","\n","epoch: [4/500] iteration: [254/782] step: 2600 Learning rate: 0.0002\n","generator_loss = 1.0556 (ave = 1.3671)\n","discriminator_loss = 6.7399 (ave = 5.6410)\n","\n","epoch: [4/500] iteration: [354/782] step: 2700 Learning rate: 0.0002\n","generator_loss = 1.4931 (ave = 1.3507)\n","discriminator_loss = 12.3713 (ave = 5.7257)\n","\n","epoch: [4/500] iteration: [454/782] step: 2800 Learning rate: 0.0002\n","generator_loss = 0.7765 (ave = 1.3336)\n","discriminator_loss = 5.7902 (ave = 5.8137)\n","\n","epoch: [4/500] iteration: [554/782] step: 2900 Learning rate: 0.0002\n","generator_loss = 0.8335 (ave = 1.3163)\n","discriminator_loss = 7.7072 (ave = 5.8834)\n","\n","epoch: [4/500] iteration: [654/782] step: 3000 Learning rate: 0.0002\n","generator_loss = 0.6960 (ave = 1.3008)\n","discriminator_loss = 7.4468 (ave = 5.9693)\n","\n","epoch: [4/500] iteration: [754/782] step: 3100 Learning rate: 0.0002\n","generator_loss = 0.7984 (ave = 1.2836)\n","discriminator_loss = 10.0254 (ave = 6.0182)\n","\n","epoch: [4/500] iteration: [782/782] step: 3129 Learning rate: 0.0002\n","generator_loss = 1.0920 (ave = 1.2806)\n","discriminator_loss = 9.0442 (ave = 6.0382)\n","\n","Real Accuracy : 72.282 Fake Accuracy : 0.214\n","epoch: [5/500] iteration: [72/782] step: 3200 Learning rate: 0.0002\n","generator_loss = 0.5684 (ave = 1.2668)\n","discriminator_loss = 8.8829 (ave = 6.0966)\n","\n","epoch: [5/500] iteration: [172/782] step: 3300 Learning rate: 0.0002\n","generator_loss = 0.7867 (ave = 1.2505)\n","discriminator_loss = 8.9109 (ave = 6.1793)\n","\n","epoch: [5/500] iteration: [272/782] step: 3400 Learning rate: 0.0002\n","generator_loss = 0.8346 (ave = 1.2347)\n","discriminator_loss = 9.4943 (ave = 6.2317)\n","\n","epoch: [5/500] iteration: [372/782] step: 3500 Learning rate: 0.0002\n","generator_loss = 0.4997 (ave = 1.2197)\n","discriminator_loss = 9.8893 (ave = 6.3152)\n","\n","epoch: [5/500] iteration: [472/782] step: 3600 Learning rate: 0.0002\n","generator_loss = 0.9139 (ave = 1.2065)\n","discriminator_loss = 9.3678 (ave = 6.3883)\n","\n","epoch: [5/500] iteration: [572/782] step: 3700 Learning rate: 0.0002\n","generator_loss = 0.5983 (ave = 1.1928)\n","discriminator_loss = 9.6302 (ave = 6.4525)\n","\n","epoch: [5/500] iteration: [672/782] step: 3800 Learning rate: 0.0002\n","generator_loss = 0.7166 (ave = 1.1793)\n","discriminator_loss = 7.6989 (ave = 6.5101)\n","\n","epoch: [5/500] iteration: [772/782] step: 3900 Learning rate: 0.0002\n","generator_loss = 0.4709 (ave = 1.1660)\n","discriminator_loss = 10.2680 (ave = 6.5846)\n","\n","epoch: [5/500] iteration: [782/782] step: 3911 Learning rate: 0.0002\n","generator_loss = 0.6645 (ave = 1.1646)\n","discriminator_loss = 4.5702 (ave = 6.5898)\n","\n","Real Accuracy : 77.204 Fake Accuracy : 0.216\n","epoch: [6/500] iteration: [90/782] step: 4000 Learning rate: 0.0002\n","generator_loss = 0.5374 (ave = 1.1507)\n","discriminator_loss = 8.3796 (ave = 6.6660)\n","\n","epoch: [6/500] iteration: [190/782] step: 4100 Learning rate: 0.0002\n","generator_loss = 0.4410 (ave = 1.1364)\n","discriminator_loss = 10.1985 (ave = 6.7411)\n","\n","epoch: [6/500] iteration: [290/782] step: 4200 Learning rate: 0.0002\n","generator_loss = 0.4195 (ave = 1.1220)\n","discriminator_loss = 7.8741 (ave = 6.7972)\n","\n","epoch: [6/500] iteration: [390/782] step: 4300 Learning rate: 0.0002\n","generator_loss = 0.6335 (ave = 1.1092)\n","discriminator_loss = 8.3823 (ave = 6.8627)\n","\n","epoch: [6/500] iteration: [490/782] step: 4400 Learning rate: 0.0002\n","generator_loss = 0.5499 (ave = 1.0966)\n","discriminator_loss = 8.8525 (ave = 6.9271)\n","\n","epoch: [6/500] iteration: [590/782] step: 4500 Learning rate: 0.0002\n","generator_loss = 0.6630 (ave = 1.0847)\n","discriminator_loss = 12.4476 (ave = 7.0041)\n","\n","epoch: [6/500] iteration: [690/782] step: 4600 Learning rate: 0.0002\n","generator_loss = 0.6206 (ave = 1.0736)\n","discriminator_loss = 10.1303 (ave = 7.0778)\n","\n","epoch: [6/500] iteration: [782/782] step: 4693 Learning rate: 0.0002\n","generator_loss = 0.5624 (ave = 1.0646)\n","discriminator_loss = 8.0943 (ave = 7.1498)\n","\n","Real Accuracy : 81.846 Fake Accuracy : 0.094\n","epoch: [7/500] iteration: [8/782] step: 4700 Learning rate: 0.0002\n","generator_loss = 0.4069 (ave = 1.0637)\n","discriminator_loss = 8.6933 (ave = 7.1519)\n","\n"]}],"source":["%cd /content/drive/My Drive/BAGAN\n","\n","! python gan.py \\\n","--batch_size=64 \\\n","--num_workers=2 \\\n","--max_epoches=150 \\\n","--z_dim=100 \\\n","--c_dim=3 \\\n","--gf_dim=128 \\\n","--df_dim=128 \\\n","--base_lr=0.0002 \\\n","--board_interval=100 \\\n","--image_interval=1000 \\\n","--save_interval=5000 \\\n","--dataset='cifar10' \\\n","--class_num=10 \\\n","--save_dir='./exp_cifar10/2022-04-04_H11-35-52' \\\n","--pretrained_dir='./exp_cifar10/2022-04-04_H11-35-52/autoencoder/BestResult' \\\n","--distribution_path='./exp_cifar10/2022-04-04_H11-35-52/Distribution/class_distribution.dt' \\\n","--train_dir='./datasets/cifar10' \\\n"]},{"cell_type":"markdown","metadata":{"id":"wxXc10Z1WJzb"},"source":["# Fed_BAGAN training"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5580591,"status":"ok","timestamp":1654177240644,"user":{"displayName":"Escano Xiao","userId":"11935749661615477607"},"user_tz":-480},"id":"_mo5RHpcWIV_","outputId":"598395dd-0128-4092-bbc4-556810745ca6"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1;30;43m流式输出内容被截断，只能显示最后 5000 行内容。\u001b[0m\n","| Global Epoch: 3 | Local Epoch: 4 | Generator Loss: 17.233600 | Discriminator Loss: 0.144442\n","| Global Epoch: 3 | Local Epoch: 5 | Generator Loss: 17.635493 | Discriminator Loss: 0.121602\n","| Global Epoch: 3 | Local Epoch: 6 | Generator Loss: 17.599051 | Discriminator Loss: 0.104872\n","| Global Epoch: 3 | Local Epoch: 7 | Generator Loss: 17.557933 | Discriminator Loss: 0.092697\n","| Global Epoch: 3 | Local Epoch: 8 | Generator Loss: 17.293126 | Discriminator Loss: 0.082593\n","| Global Epoch: 3 | Local Epoch: 9 | Generator Loss: 17.259324 | Discriminator Loss: 0.074522\n","After 4 epoch global training, averged local generator loss is: 14.1887, averged local discrimitor loss is: 0.2741\n","Averged validation loss is: 7.9559\n","| Global Epoch: 4 | Local Epoch: 0 | Generator Loss: 15.659151 | Discriminator Loss: 0.387918\n","| Global Epoch: 4 | Local Epoch: 1 | Generator Loss: 16.195328 | Discriminator Loss: 0.224957\n","| Global Epoch: 4 | Local Epoch: 2 | Generator Loss: 16.403886 | Discriminator Loss: 0.177051\n","| Global Epoch: 4 | Local Epoch: 3 | Generator Loss: 16.689537 | Discriminator Loss: 0.138890\n","| Global Epoch: 4 | Local Epoch: 4 | Generator Loss: 16.764202 | Discriminator Loss: 0.119498\n","| Global Epoch: 4 | Local Epoch: 5 | Generator Loss: 16.606638 | Discriminator Loss: 0.123829\n","| Global Epoch: 4 | Local Epoch: 6 | Generator Loss: 16.591329 | Discriminator Loss: 0.110060\n","| Global Epoch: 4 | Local Epoch: 7 | Generator Loss: 16.671628 | Discriminator Loss: 0.097293\n","| Global Epoch: 4 | Local Epoch: 8 | Generator Loss: 16.846312 | Discriminator Loss: 0.098765\n","| Global Epoch: 4 | Local Epoch: 9 | Generator Loss: 16.986266 | Discriminator Loss: 0.095205\n","| Global Epoch: 4 | Local Epoch: 0 | Generator Loss: 18.242187 | Discriminator Loss: 0.188962\n","| Global Epoch: 4 | Local Epoch: 1 | Generator Loss: 16.904197 | Discriminator Loss: 0.107504\n","| Global Epoch: 4 | Local Epoch: 2 | Generator Loss: 15.895841 | Discriminator Loss: 0.074087\n","| Global Epoch: 4 | Local Epoch: 3 | Generator Loss: 15.685494 | Discriminator Loss: 0.057152\n","| Global Epoch: 4 | Local Epoch: 4 | Generator Loss: 15.810682 | Discriminator Loss: 0.057586\n","| Global Epoch: 4 | Local Epoch: 5 | Generator Loss: 16.432072 | Discriminator Loss: 0.048275\n","| Global Epoch: 4 | Local Epoch: 6 | Generator Loss: 17.395095 | Discriminator Loss: 0.043120\n","| Global Epoch: 4 | Local Epoch: 7 | Generator Loss: 18.129728 | Discriminator Loss: 0.038713\n","| Global Epoch: 4 | Local Epoch: 8 | Generator Loss: 18.805010 | Discriminator Loss: 0.034872\n","| Global Epoch: 4 | Local Epoch: 9 | Generator Loss: 19.196449 | Discriminator Loss: 0.031673\n","| Global Epoch: 4 | Local Epoch: 0 | Generator Loss: 16.740441 | Discriminator Loss: 0.634128\n","| Global Epoch: 4 | Local Epoch: 1 | Generator Loss: 16.908835 | Discriminator Loss: 0.384644\n","| Global Epoch: 4 | Local Epoch: 2 | Generator Loss: 17.139015 | Discriminator Loss: 0.275698\n","| Global Epoch: 4 | Local Epoch: 3 | Generator Loss: 17.604848 | Discriminator Loss: 0.214714\n","| Global Epoch: 4 | Local Epoch: 4 | Generator Loss: 17.576481 | Discriminator Loss: 0.193225\n","| Global Epoch: 4 | Local Epoch: 5 | Generator Loss: 17.612705 | Discriminator Loss: 0.173428\n","| Global Epoch: 4 | Local Epoch: 6 | Generator Loss: 17.379884 | Discriminator Loss: 0.171963\n","| Global Epoch: 4 | Local Epoch: 7 | Generator Loss: 17.295079 | Discriminator Loss: 0.161154\n","| Global Epoch: 4 | Local Epoch: 8 | Generator Loss: 17.293790 | Discriminator Loss: 0.144758\n","| Global Epoch: 4 | Local Epoch: 9 | Generator Loss: 17.430484 | Discriminator Loss: 0.131906\n","| Global Epoch: 4 | Local Epoch: 0 | Generator Loss: 18.556590 | Discriminator Loss: 0.496305\n","| Global Epoch: 4 | Local Epoch: 1 | Generator Loss: 18.807286 | Discriminator Loss: 0.287043\n","| Global Epoch: 4 | Local Epoch: 2 | Generator Loss: 18.400931 | Discriminator Loss: 0.198053\n","| Global Epoch: 4 | Local Epoch: 3 | Generator Loss: 18.628088 | Discriminator Loss: 0.153613\n","| Global Epoch: 4 | Local Epoch: 4 | Generator Loss: 18.534499 | Discriminator Loss: 0.125120\n","| Global Epoch: 4 | Local Epoch: 5 | Generator Loss: 18.524156 | Discriminator Loss: 0.106145\n","| Global Epoch: 4 | Local Epoch: 6 | Generator Loss: 18.379792 | Discriminator Loss: 0.093400\n","| Global Epoch: 4 | Local Epoch: 7 | Generator Loss: 18.378693 | Discriminator Loss: 0.083016\n","| Global Epoch: 4 | Local Epoch: 8 | Generator Loss: 18.450206 | Discriminator Loss: 0.074243\n","| Global Epoch: 4 | Local Epoch: 9 | Generator Loss: 18.495350 | Discriminator Loss: 0.067129\n","| Global Epoch: 4 | Local Epoch: 0 | Generator Loss: 18.771608 | Discriminator Loss: 0.866944\n","| Global Epoch: 4 | Local Epoch: 1 | Generator Loss: 18.800347 | Discriminator Loss: 0.520411\n","| Global Epoch: 4 | Local Epoch: 2 | Generator Loss: 18.692787 | Discriminator Loss: 0.366347\n","| Global Epoch: 4 | Local Epoch: 3 | Generator Loss: 18.743991 | Discriminator Loss: 0.280484\n","| Global Epoch: 4 | Local Epoch: 4 | Generator Loss: 18.716233 | Discriminator Loss: 0.227172\n","| Global Epoch: 4 | Local Epoch: 5 | Generator Loss: 18.930541 | Discriminator Loss: 0.191557\n","| Global Epoch: 4 | Local Epoch: 6 | Generator Loss: 19.048550 | Discriminator Loss: 0.165248\n","| Global Epoch: 4 | Local Epoch: 7 | Generator Loss: 19.168518 | Discriminator Loss: 0.145341\n","| Global Epoch: 4 | Local Epoch: 8 | Generator Loss: 19.209308 | Discriminator Loss: 0.129761\n","| Global Epoch: 4 | Local Epoch: 9 | Generator Loss: 19.220383 | Discriminator Loss: 0.117221\n","After 5 epoch global training, averged local generator loss is: 15.1951, averged local discrimitor loss is: 0.2427\n","Averged validation loss is: 7.4750\n","| Global Epoch: 5 | Local Epoch: 0 | Generator Loss: 16.448591 | Discriminator Loss: 0.313116\n","| Global Epoch: 5 | Local Epoch: 1 | Generator Loss: 16.905814 | Discriminator Loss: 0.211526\n","| Global Epoch: 5 | Local Epoch: 2 | Generator Loss: 17.307469 | Discriminator Loss: 0.150459\n","| Global Epoch: 5 | Local Epoch: 3 | Generator Loss: 17.480687 | Discriminator Loss: 0.115833\n","| Global Epoch: 5 | Local Epoch: 4 | Generator Loss: 17.637058 | Discriminator Loss: 0.094114\n","| Global Epoch: 5 | Local Epoch: 5 | Generator Loss: 17.772915 | Discriminator Loss: 0.079276\n","| Global Epoch: 5 | Local Epoch: 6 | Generator Loss: 17.928238 | Discriminator Loss: 0.068638\n","| Global Epoch: 5 | Local Epoch: 7 | Generator Loss: 17.962238 | Discriminator Loss: 0.065383\n","| Global Epoch: 5 | Local Epoch: 8 | Generator Loss: 18.057016 | Discriminator Loss: 0.062852\n","| Global Epoch: 5 | Local Epoch: 9 | Generator Loss: 18.004458 | Discriminator Loss: 0.069182\n","| Global Epoch: 5 | Local Epoch: 0 | Generator Loss: 16.590949 | Discriminator Loss: 0.266072\n","| Global Epoch: 5 | Local Epoch: 1 | Generator Loss: 17.839972 | Discriminator Loss: 0.146560\n","| Global Epoch: 5 | Local Epoch: 2 | Generator Loss: 18.161579 | Discriminator Loss: 0.101917\n","| Global Epoch: 5 | Local Epoch: 3 | Generator Loss: 18.499746 | Discriminator Loss: 0.078188\n","| Global Epoch: 5 | Local Epoch: 4 | Generator Loss: 18.635110 | Discriminator Loss: 0.063082\n","| Global Epoch: 5 | Local Epoch: 5 | Generator Loss: 18.641943 | Discriminator Loss: 0.053127\n","| Global Epoch: 5 | Local Epoch: 6 | Generator Loss: 18.439939 | Discriminator Loss: 0.046774\n","| Global Epoch: 5 | Local Epoch: 7 | Generator Loss: 18.590283 | Discriminator Loss: 0.041068\n","| Global Epoch: 5 | Local Epoch: 8 | Generator Loss: 18.583432 | Discriminator Loss: 0.036672\n","| Global Epoch: 5 | Local Epoch: 9 | Generator Loss: 18.854870 | Discriminator Loss: 0.033094\n","| Global Epoch: 5 | Local Epoch: 0 | Generator Loss: 18.191896 | Discriminator Loss: 0.338727\n","| Global Epoch: 5 | Local Epoch: 1 | Generator Loss: 18.183874 | Discriminator Loss: 0.187308\n","| Global Epoch: 5 | Local Epoch: 2 | Generator Loss: 18.091802 | Discriminator Loss: 0.128663\n","| Global Epoch: 5 | Local Epoch: 3 | Generator Loss: 18.093583 | Discriminator Loss: 0.100266\n","| Global Epoch: 5 | Local Epoch: 4 | Generator Loss: 17.568981 | Discriminator Loss: 0.111702\n","| Global Epoch: 5 | Local Epoch: 5 | Generator Loss: 17.702364 | Discriminator Loss: 0.099571\n","| Global Epoch: 5 | Local Epoch: 6 | Generator Loss: 17.958116 | Discriminator Loss: 0.087365\n","| Global Epoch: 5 | Local Epoch: 7 | Generator Loss: 18.111562 | Discriminator Loss: 0.077174\n","| Global Epoch: 5 | Local Epoch: 8 | Generator Loss: 18.250643 | Discriminator Loss: 0.068947\n","| Global Epoch: 5 | Local Epoch: 9 | Generator Loss: 18.439182 | Discriminator Loss: 0.063093\n","| Global Epoch: 5 | Local Epoch: 0 | Generator Loss: 19.646480 | Discriminator Loss: 0.348559\n","| Global Epoch: 5 | Local Epoch: 1 | Generator Loss: 18.139386 | Discriminator Loss: 0.197427\n","| Global Epoch: 5 | Local Epoch: 2 | Generator Loss: 18.524424 | Discriminator Loss: 0.137919\n","| Global Epoch: 5 | Local Epoch: 3 | Generator Loss: 18.466742 | Discriminator Loss: 0.110407\n","| Global Epoch: 5 | Local Epoch: 4 | Generator Loss: 18.528213 | Discriminator Loss: 0.103323\n","| Global Epoch: 5 | Local Epoch: 5 | Generator Loss: 18.466887 | Discriminator Loss: 0.091414\n","| Global Epoch: 5 | Local Epoch: 6 | Generator Loss: 18.686983 | Discriminator Loss: 0.080928\n","| Global Epoch: 5 | Local Epoch: 7 | Generator Loss: 18.664753 | Discriminator Loss: 0.071549\n","| Global Epoch: 5 | Local Epoch: 8 | Generator Loss: 18.873587 | Discriminator Loss: 0.065105\n","| Global Epoch: 5 | Local Epoch: 9 | Generator Loss: 18.888469 | Discriminator Loss: 0.059087\n","| Global Epoch: 5 | Local Epoch: 0 | Generator Loss: 18.660619 | Discriminator Loss: 0.775590\n","| Global Epoch: 5 | Local Epoch: 1 | Generator Loss: 18.810417 | Discriminator Loss: 0.455428\n","| Global Epoch: 5 | Local Epoch: 2 | Generator Loss: 18.456126 | Discriminator Loss: 0.317668\n","| Global Epoch: 5 | Local Epoch: 3 | Generator Loss: 18.473560 | Discriminator Loss: 0.243129\n","| Global Epoch: 5 | Local Epoch: 4 | Generator Loss: 18.448929 | Discriminator Loss: 0.196656\n","| Global Epoch: 5 | Local Epoch: 5 | Generator Loss: 18.570495 | Discriminator Loss: 0.179067\n","| Global Epoch: 5 | Local Epoch: 6 | Generator Loss: 18.033313 | Discriminator Loss: 0.211184\n","| Global Epoch: 5 | Local Epoch: 7 | Generator Loss: 18.064623 | Discriminator Loss: 0.192105\n","| Global Epoch: 5 | Local Epoch: 8 | Generator Loss: 17.979881 | Discriminator Loss: 0.172068\n","| Global Epoch: 5 | Local Epoch: 9 | Generator Loss: 18.003781 | Discriminator Loss: 0.155525\n","After 6 epoch global training, averged local generator loss is: 15.6632, averged local discrimitor loss is: 0.2282\n","Averged validation loss is: 9.3937\n","| Global Epoch: 6 | Local Epoch: 0 | Generator Loss: 16.042770 | Discriminator Loss: 0.120523\n","| Global Epoch: 6 | Local Epoch: 1 | Generator Loss: 16.458203 | Discriminator Loss: 0.078737\n","| Global Epoch: 6 | Local Epoch: 2 | Generator Loss: 16.483757 | Discriminator Loss: 0.060733\n","| Global Epoch: 6 | Local Epoch: 3 | Generator Loss: 16.258967 | Discriminator Loss: 0.047264\n","| Global Epoch: 6 | Local Epoch: 4 | Generator Loss: 16.616761 | Discriminator Loss: 0.048802\n","| Global Epoch: 6 | Local Epoch: 5 | Generator Loss: 16.661084 | Discriminator Loss: 0.063761\n","| Global Epoch: 6 | Local Epoch: 6 | Generator Loss: 17.057608 | Discriminator Loss: 0.065000\n","| Global Epoch: 6 | Local Epoch: 7 | Generator Loss: 17.253044 | Discriminator Loss: 0.058574\n","| Global Epoch: 6 | Local Epoch: 8 | Generator Loss: 17.695857 | Discriminator Loss: 0.053683\n","| Global Epoch: 6 | Local Epoch: 9 | Generator Loss: 17.859382 | Discriminator Loss: 0.055051\n","| Global Epoch: 6 | Local Epoch: 0 | Generator Loss: 17.646797 | Discriminator Loss: 0.523320\n","| Global Epoch: 6 | Local Epoch: 1 | Generator Loss: 17.624119 | Discriminator Loss: 0.305735\n","| Global Epoch: 6 | Local Epoch: 2 | Generator Loss: 18.042860 | Discriminator Loss: 0.221098\n","| Global Epoch: 6 | Local Epoch: 3 | Generator Loss: 18.296185 | Discriminator Loss: 0.191562\n","| Global Epoch: 6 | Local Epoch: 4 | Generator Loss: 17.963640 | Discriminator Loss: 0.186669\n","| Global Epoch: 6 | Local Epoch: 5 | Generator Loss: 18.078648 | Discriminator Loss: 0.160461\n","| Global Epoch: 6 | Local Epoch: 6 | Generator Loss: 18.244108 | Discriminator Loss: 0.139650\n","| Global Epoch: 6 | Local Epoch: 7 | Generator Loss: 18.354915 | Discriminator Loss: 0.124490\n","| Global Epoch: 6 | Local Epoch: 8 | Generator Loss: 18.302104 | Discriminator Loss: 0.137806\n","| Global Epoch: 6 | Local Epoch: 9 | Generator Loss: 18.513294 | Discriminator Loss: 0.129958\n","| Global Epoch: 6 | Local Epoch: 0 | Generator Loss: 19.348558 | Discriminator Loss: 1.612061\n","| Global Epoch: 6 | Local Epoch: 1 | Generator Loss: 18.995064 | Discriminator Loss: 1.126358\n","| Global Epoch: 6 | Local Epoch: 2 | Generator Loss: 17.940241 | Discriminator Loss: 0.854242\n","| Global Epoch: 6 | Local Epoch: 3 | Generator Loss: 17.879544 | Discriminator Loss: 0.754546\n","| Global Epoch: 6 | Local Epoch: 4 | Generator Loss: 17.602014 | Discriminator Loss: 0.641913\n","| Global Epoch: 6 | Local Epoch: 5 | Generator Loss: 17.407256 | Discriminator Loss: 0.580684\n","| Global Epoch: 6 | Local Epoch: 6 | Generator Loss: 17.184454 | Discriminator Loss: 0.529690\n","| Global Epoch: 6 | Local Epoch: 7 | Generator Loss: 17.207976 | Discriminator Loss: 0.487001\n","| Global Epoch: 6 | Local Epoch: 8 | Generator Loss: 17.174377 | Discriminator Loss: 0.447490\n","| Global Epoch: 6 | Local Epoch: 9 | Generator Loss: 17.123400 | Discriminator Loss: 0.423329\n","| Global Epoch: 6 | Local Epoch: 0 | Generator Loss: 16.540148 | Discriminator Loss: 0.397703\n","| Global Epoch: 6 | Local Epoch: 1 | Generator Loss: 17.051888 | Discriminator Loss: 0.232490\n","| Global Epoch: 6 | Local Epoch: 2 | Generator Loss: 17.002651 | Discriminator Loss: 0.162481\n","| Global Epoch: 6 | Local Epoch: 3 | Generator Loss: 17.126305 | Discriminator Loss: 0.125721\n","| Global Epoch: 6 | Local Epoch: 4 | Generator Loss: 17.305580 | Discriminator Loss: 0.116305\n","| Global Epoch: 6 | Local Epoch: 5 | Generator Loss: 17.466292 | Discriminator Loss: 0.110325\n","| Global Epoch: 6 | Local Epoch: 6 | Generator Loss: 17.435203 | Discriminator Loss: 0.137468\n","| Global Epoch: 6 | Local Epoch: 7 | Generator Loss: 17.451397 | Discriminator Loss: 0.128384\n","| Global Epoch: 6 | Local Epoch: 8 | Generator Loss: 17.358968 | Discriminator Loss: 0.115169\n","| Global Epoch: 6 | Local Epoch: 9 | Generator Loss: 17.339507 | Discriminator Loss: 0.104117\n","| Global Epoch: 6 | Local Epoch: 0 | Generator Loss: 16.719240 | Discriminator Loss: 0.630793\n","| Global Epoch: 6 | Local Epoch: 1 | Generator Loss: 16.908389 | Discriminator Loss: 0.382639\n","| Global Epoch: 6 | Local Epoch: 2 | Generator Loss: 17.113566 | Discriminator Loss: 0.276946\n","| Global Epoch: 6 | Local Epoch: 3 | Generator Loss: 17.113402 | Discriminator Loss: 0.212972\n","| Global Epoch: 6 | Local Epoch: 4 | Generator Loss: 17.178479 | Discriminator Loss: 0.172593\n","| Global Epoch: 6 | Local Epoch: 5 | Generator Loss: 17.515096 | Discriminator Loss: 0.145690\n","| Global Epoch: 6 | Local Epoch: 6 | Generator Loss: 17.563914 | Discriminator Loss: 0.125846\n","| Global Epoch: 6 | Local Epoch: 7 | Generator Loss: 17.679832 | Discriminator Loss: 0.110806\n","| Global Epoch: 6 | Local Epoch: 8 | Generator Loss: 17.669833 | Discriminator Loss: 0.098980\n","| Global Epoch: 6 | Local Epoch: 9 | Generator Loss: 17.563344 | Discriminator Loss: 0.091335\n","After 7 epoch global training, averged local generator loss is: 15.9346, averged local discrimitor loss is: 0.2087\n","Averged validation loss is: 5.7248\n","| Global Epoch: 7 | Local Epoch: 0 | Generator Loss: 18.586885 | Discriminator Loss: 0.462829\n","| Global Epoch: 7 | Local Epoch: 1 | Generator Loss: 18.090591 | Discriminator Loss: 0.264079\n","| Global Epoch: 7 | Local Epoch: 2 | Generator Loss: 17.706524 | Discriminator Loss: 0.181503\n","| Global Epoch: 7 | Local Epoch: 3 | Generator Loss: 17.548708 | Discriminator Loss: 0.144841\n","| Global Epoch: 7 | Local Epoch: 4 | Generator Loss: 17.467743 | Discriminator Loss: 0.121391\n","| Global Epoch: 7 | Local Epoch: 5 | Generator Loss: 17.528736 | Discriminator Loss: 0.102150\n","| Global Epoch: 7 | Local Epoch: 6 | Generator Loss: 17.653879 | Discriminator Loss: 0.116702\n","| Global Epoch: 7 | Local Epoch: 7 | Generator Loss: 17.721659 | Discriminator Loss: 0.107939\n","| Global Epoch: 7 | Local Epoch: 8 | Generator Loss: 17.829760 | Discriminator Loss: 0.099756\n","| Global Epoch: 7 | Local Epoch: 9 | Generator Loss: 17.931819 | Discriminator Loss: 0.090819\n","| Global Epoch: 7 | Local Epoch: 0 | Generator Loss: 18.894334 | Discriminator Loss: 0.313286\n","| Global Epoch: 7 | Local Epoch: 1 | Generator Loss: 18.095334 | Discriminator Loss: 0.188454\n","| Global Epoch: 7 | Local Epoch: 2 | Generator Loss: 18.144893 | Discriminator Loss: 0.130913\n","| Global Epoch: 7 | Local Epoch: 3 | Generator Loss: 18.270028 | Discriminator Loss: 0.099559\n","| Global Epoch: 7 | Local Epoch: 4 | Generator Loss: 18.264369 | Discriminator Loss: 0.080550\n","| Global Epoch: 7 | Local Epoch: 5 | Generator Loss: 18.263629 | Discriminator Loss: 0.067709\n","| Global Epoch: 7 | Local Epoch: 6 | Generator Loss: 18.511519 | Discriminator Loss: 0.059054\n","| Global Epoch: 7 | Local Epoch: 7 | Generator Loss: 18.297507 | Discriminator Loss: 0.052063\n","| Global Epoch: 7 | Local Epoch: 8 | Generator Loss: 18.444205 | Discriminator Loss: 0.046613\n","| Global Epoch: 7 | Local Epoch: 9 | Generator Loss: 18.522467 | Discriminator Loss: 0.042166\n","| Global Epoch: 7 | Local Epoch: 0 | Generator Loss: 17.985061 | Discriminator Loss: 0.228105\n","| Global Epoch: 7 | Local Epoch: 1 | Generator Loss: 17.960555 | Discriminator Loss: 0.151093\n","| Global Epoch: 7 | Local Epoch: 2 | Generator Loss: 17.528100 | Discriminator Loss: 0.107951\n","| Global Epoch: 7 | Local Epoch: 3 | Generator Loss: 17.453357 | Discriminator Loss: 0.083553\n","| Global Epoch: 7 | Local Epoch: 4 | Generator Loss: 17.705342 | Discriminator Loss: 0.067916\n","| Global Epoch: 7 | Local Epoch: 5 | Generator Loss: 17.851474 | Discriminator Loss: 0.057177\n","| Global Epoch: 7 | Local Epoch: 6 | Generator Loss: 18.054872 | Discriminator Loss: 0.049546\n","| Global Epoch: 7 | Local Epoch: 7 | Generator Loss: 18.003760 | Discriminator Loss: 0.049458\n","| Global Epoch: 7 | Local Epoch: 8 | Generator Loss: 18.015376 | Discriminator Loss: 0.059060\n","| Global Epoch: 7 | Local Epoch: 9 | Generator Loss: 18.047338 | Discriminator Loss: 0.079496\n","| Global Epoch: 7 | Local Epoch: 0 | Generator Loss: 16.694348 | Discriminator Loss: 0.617573\n","| Global Epoch: 7 | Local Epoch: 1 | Generator Loss: 17.615200 | Discriminator Loss: 0.368804\n","| Global Epoch: 7 | Local Epoch: 2 | Generator Loss: 17.271799 | Discriminator Loss: 0.254039\n","| Global Epoch: 7 | Local Epoch: 3 | Generator Loss: 17.419449 | Discriminator Loss: 0.192994\n","| Global Epoch: 7 | Local Epoch: 4 | Generator Loss: 17.442396 | Discriminator Loss: 0.155975\n","| Global Epoch: 7 | Local Epoch: 5 | Generator Loss: 17.524589 | Discriminator Loss: 0.131256\n","| Global Epoch: 7 | Local Epoch: 6 | Generator Loss: 17.826746 | Discriminator Loss: 0.114163\n","| Global Epoch: 7 | Local Epoch: 7 | Generator Loss: 17.881332 | Discriminator Loss: 0.108600\n","| Global Epoch: 7 | Local Epoch: 8 | Generator Loss: 18.179108 | Discriminator Loss: 0.098439\n","| Global Epoch: 7 | Local Epoch: 9 | Generator Loss: 18.249211 | Discriminator Loss: 0.090605\n","| Global Epoch: 7 | Local Epoch: 0 | Generator Loss: 18.701064 | Discriminator Loss: 0.799057\n","| Global Epoch: 7 | Local Epoch: 1 | Generator Loss: 18.940729 | Discriminator Loss: 0.521119\n","| Global Epoch: 7 | Local Epoch: 2 | Generator Loss: 18.534122 | Discriminator Loss: 0.444265\n","| Global Epoch: 7 | Local Epoch: 3 | Generator Loss: 17.828066 | Discriminator Loss: 0.370160\n","| Global Epoch: 7 | Local Epoch: 4 | Generator Loss: 17.592705 | Discriminator Loss: 0.365593\n","| Global Epoch: 7 | Local Epoch: 5 | Generator Loss: 17.260753 | Discriminator Loss: 0.342193\n","| Global Epoch: 7 | Local Epoch: 6 | Generator Loss: 16.914702 | Discriminator Loss: 0.323405\n","| Global Epoch: 7 | Local Epoch: 7 | Generator Loss: 16.888058 | Discriminator Loss: 0.289809\n","| Global Epoch: 7 | Local Epoch: 8 | Generator Loss: 16.734095 | Discriminator Loss: 0.269477\n","| Global Epoch: 7 | Local Epoch: 9 | Generator Loss: 16.687704 | Discriminator Loss: 0.250924\n","After 8 epoch global training, averged local generator loss is: 16.0288, averged local discrimitor loss is: 0.2139\n","Averged validation loss is: 7.6677\n","| Global Epoch: 8 | Local Epoch: 0 | Generator Loss: 16.273056 | Discriminator Loss: 0.949017\n","| Global Epoch: 8 | Local Epoch: 1 | Generator Loss: 16.444648 | Discriminator Loss: 0.584458\n","| Global Epoch: 8 | Local Epoch: 2 | Generator Loss: 16.310348 | Discriminator Loss: 0.411880\n","| Global Epoch: 8 | Local Epoch: 3 | Generator Loss: 16.362251 | Discriminator Loss: 0.317377\n","| Global Epoch: 8 | Local Epoch: 4 | Generator Loss: 16.538642 | Discriminator Loss: 0.257192\n","| Global Epoch: 8 | Local Epoch: 5 | Generator Loss: 16.753218 | Discriminator Loss: 0.216226\n","| Global Epoch: 8 | Local Epoch: 6 | Generator Loss: 16.945487 | Discriminator Loss: 0.189147\n","| Global Epoch: 8 | Local Epoch: 7 | Generator Loss: 16.762867 | Discriminator Loss: 0.171654\n","| Global Epoch: 8 | Local Epoch: 8 | Generator Loss: 16.735190 | Discriminator Loss: 0.154099\n","| Global Epoch: 8 | Local Epoch: 9 | Generator Loss: 16.962341 | Discriminator Loss: 0.143488\n","| Global Epoch: 8 | Local Epoch: 0 | Generator Loss: 18.535917 | Discriminator Loss: 1.077338\n","| Global Epoch: 8 | Local Epoch: 1 | Generator Loss: 17.584049 | Discriminator Loss: 0.650207\n","| Global Epoch: 8 | Local Epoch: 2 | Generator Loss: 17.734364 | Discriminator Loss: 0.460313\n","| Global Epoch: 8 | Local Epoch: 3 | Generator Loss: 17.671719 | Discriminator Loss: 0.352085\n","| Global Epoch: 8 | Local Epoch: 4 | Generator Loss: 18.033891 | Discriminator Loss: 0.285462\n","| Global Epoch: 8 | Local Epoch: 5 | Generator Loss: 18.438475 | Discriminator Loss: 0.239706\n","| Global Epoch: 8 | Local Epoch: 6 | Generator Loss: 18.469048 | Discriminator Loss: 0.207949\n","| Global Epoch: 8 | Local Epoch: 7 | Generator Loss: 18.484593 | Discriminator Loss: 0.186353\n","| Global Epoch: 8 | Local Epoch: 8 | Generator Loss: 18.351839 | Discriminator Loss: 0.197253\n","| Global Epoch: 8 | Local Epoch: 9 | Generator Loss: 18.340673 | Discriminator Loss: 0.191409\n","| Global Epoch: 8 | Local Epoch: 0 | Generator Loss: 18.618325 | Discriminator Loss: 0.262027\n","| Global Epoch: 8 | Local Epoch: 1 | Generator Loss: 18.669946 | Discriminator Loss: 0.148767\n","| Global Epoch: 8 | Local Epoch: 2 | Generator Loss: 18.893510 | Discriminator Loss: 0.110830\n","| Global Epoch: 8 | Local Epoch: 3 | Generator Loss: 19.015640 | Discriminator Loss: 0.088165\n","| Global Epoch: 8 | Local Epoch: 4 | Generator Loss: 19.138360 | Discriminator Loss: 0.071775\n","| Global Epoch: 8 | Local Epoch: 5 | Generator Loss: 19.225511 | Discriminator Loss: 0.060320\n","| Global Epoch: 8 | Local Epoch: 6 | Generator Loss: 19.337658 | Discriminator Loss: 0.052226\n","| Global Epoch: 8 | Local Epoch: 7 | Generator Loss: 19.392210 | Discriminator Loss: 0.045944\n","| Global Epoch: 8 | Local Epoch: 8 | Generator Loss: 19.434634 | Discriminator Loss: 0.041636\n","| Global Epoch: 8 | Local Epoch: 9 | Generator Loss: 19.446286 | Discriminator Loss: 0.041408\n","| Global Epoch: 8 | Local Epoch: 0 | Generator Loss: 19.417799 | Discriminator Loss: 0.411305\n","| Global Epoch: 8 | Local Epoch: 1 | Generator Loss: 18.313373 | Discriminator Loss: 0.228194\n","| Global Epoch: 8 | Local Epoch: 2 | Generator Loss: 18.388341 | Discriminator Loss: 0.155255\n","| Global Epoch: 8 | Local Epoch: 3 | Generator Loss: 18.312556 | Discriminator Loss: 0.117504\n","| Global Epoch: 8 | Local Epoch: 4 | Generator Loss: 18.754012 | Discriminator Loss: 0.094912\n","| Global Epoch: 8 | Local Epoch: 5 | Generator Loss: 18.737794 | Discriminator Loss: 0.079575\n","| Global Epoch: 8 | Local Epoch: 6 | Generator Loss: 18.750659 | Discriminator Loss: 0.068565\n","| Global Epoch: 8 | Local Epoch: 7 | Generator Loss: 18.772802 | Discriminator Loss: 0.060236\n","| Global Epoch: 8 | Local Epoch: 8 | Generator Loss: 18.640161 | Discriminator Loss: 0.054873\n","| Global Epoch: 8 | Local Epoch: 9 | Generator Loss: 18.697819 | Discriminator Loss: 0.059288\n","| Global Epoch: 8 | Local Epoch: 0 | Generator Loss: 19.862054 | Discriminator Loss: 0.855868\n","| Global Epoch: 8 | Local Epoch: 1 | Generator Loss: 18.957025 | Discriminator Loss: 0.455149\n","| Global Epoch: 8 | Local Epoch: 2 | Generator Loss: 18.336241 | Discriminator Loss: 0.311027\n","| Global Epoch: 8 | Local Epoch: 3 | Generator Loss: 18.444798 | Discriminator Loss: 0.331752\n","| Global Epoch: 8 | Local Epoch: 4 | Generator Loss: 19.820241 | Discriminator Loss: 0.270957\n","| Global Epoch: 8 | Local Epoch: 5 | Generator Loss: 20.618874 | Discriminator Loss: 0.248499\n","| Global Epoch: 8 | Local Epoch: 6 | Generator Loss: 20.955903 | Discriminator Loss: 0.213889\n","| Global Epoch: 8 | Local Epoch: 7 | Generator Loss: 20.896891 | Discriminator Loss: 0.187380\n","| Global Epoch: 8 | Local Epoch: 8 | Generator Loss: 20.461054 | Discriminator Loss: 0.166732\n","| Global Epoch: 8 | Local Epoch: 9 | Generator Loss: 20.058316 | Discriminator Loss: 0.150265\n","After 9 epoch global training, averged local generator loss is: 16.4765, averged local discrimitor loss is: 0.2069\n","Averged validation loss is: 6.5498\n","| Global Epoch: 9 | Local Epoch: 0 | Generator Loss: 18.598643 | Discriminator Loss: 0.422237\n","| Global Epoch: 9 | Local Epoch: 1 | Generator Loss: 17.936342 | Discriminator Loss: 0.231575\n","| Global Epoch: 9 | Local Epoch: 2 | Generator Loss: 18.155993 | Discriminator Loss: 0.157800\n","| Global Epoch: 9 | Local Epoch: 3 | Generator Loss: 18.428845 | Discriminator Loss: 0.119718\n","| Global Epoch: 9 | Local Epoch: 4 | Generator Loss: 18.533821 | Discriminator Loss: 0.096613\n","| Global Epoch: 9 | Local Epoch: 5 | Generator Loss: 18.591118 | Discriminator Loss: 0.088049\n","| Global Epoch: 9 | Local Epoch: 6 | Generator Loss: 18.517106 | Discriminator Loss: 0.112105\n","| Global Epoch: 9 | Local Epoch: 7 | Generator Loss: 18.372401 | Discriminator Loss: 0.104190\n","| Global Epoch: 9 | Local Epoch: 8 | Generator Loss: 18.337540 | Discriminator Loss: 0.093694\n","| Global Epoch: 9 | Local Epoch: 9 | Generator Loss: 18.313503 | Discriminator Loss: 0.084738\n","| Global Epoch: 9 | Local Epoch: 0 | Generator Loss: 18.562215 | Discriminator Loss: 0.418701\n","| Global Epoch: 9 | Local Epoch: 1 | Generator Loss: 18.528041 | Discriminator Loss: 0.251390\n","| Global Epoch: 9 | Local Epoch: 2 | Generator Loss: 18.228886 | Discriminator Loss: 0.185659\n","| Global Epoch: 9 | Local Epoch: 3 | Generator Loss: 18.567820 | Discriminator Loss: 0.150075\n","| Global Epoch: 9 | Local Epoch: 4 | Generator Loss: 18.781087 | Discriminator Loss: 0.121569\n","| Global Epoch: 9 | Local Epoch: 5 | Generator Loss: 19.071584 | Discriminator Loss: 0.104712\n","| Global Epoch: 9 | Local Epoch: 6 | Generator Loss: 19.315687 | Discriminator Loss: 0.090484\n","| Global Epoch: 9 | Local Epoch: 7 | Generator Loss: 19.462019 | Discriminator Loss: 0.079580\n","| Global Epoch: 9 | Local Epoch: 8 | Generator Loss: 19.574293 | Discriminator Loss: 0.071280\n","| Global Epoch: 9 | Local Epoch: 9 | Generator Loss: 19.643556 | Discriminator Loss: 0.070162\n","| Global Epoch: 9 | Local Epoch: 0 | Generator Loss: 18.671593 | Discriminator Loss: 0.446050\n","| Global Epoch: 9 | Local Epoch: 1 | Generator Loss: 18.119814 | Discriminator Loss: 0.251183\n","| Global Epoch: 9 | Local Epoch: 2 | Generator Loss: 17.962149 | Discriminator Loss: 0.172319\n","| Global Epoch: 9 | Local Epoch: 3 | Generator Loss: 18.129158 | Discriminator Loss: 0.131035\n","| Global Epoch: 9 | Local Epoch: 4 | Generator Loss: 18.269153 | Discriminator Loss: 0.106149\n","| Global Epoch: 9 | Local Epoch: 5 | Generator Loss: 18.579514 | Discriminator Loss: 0.089147\n","| Global Epoch: 9 | Local Epoch: 6 | Generator Loss: 18.754674 | Discriminator Loss: 0.089264\n","| Global Epoch: 9 | Local Epoch: 7 | Generator Loss: 18.694715 | Discriminator Loss: 0.091723\n","| Global Epoch: 9 | Local Epoch: 8 | Generator Loss: 18.608947 | Discriminator Loss: 0.143812\n","| Global Epoch: 9 | Local Epoch: 9 | Generator Loss: 18.733732 | Discriminator Loss: 0.138690\n","| Global Epoch: 9 | Local Epoch: 0 | Generator Loss: 18.622280 | Discriminator Loss: 0.237818\n","| Global Epoch: 9 | Local Epoch: 1 | Generator Loss: 18.394824 | Discriminator Loss: 0.134959\n","| Global Epoch: 9 | Local Epoch: 2 | Generator Loss: 18.578423 | Discriminator Loss: 0.102238\n","| Global Epoch: 9 | Local Epoch: 3 | Generator Loss: 18.364530 | Discriminator Loss: 0.089037\n","| Global Epoch: 9 | Local Epoch: 4 | Generator Loss: 18.749125 | Discriminator Loss: 0.073376\n","| Global Epoch: 9 | Local Epoch: 5 | Generator Loss: 19.006800 | Discriminator Loss: 0.072236\n","| Global Epoch: 9 | Local Epoch: 6 | Generator Loss: 18.726335 | Discriminator Loss: 0.119744\n","| Global Epoch: 9 | Local Epoch: 7 | Generator Loss: 18.615547 | Discriminator Loss: 0.113738\n","| Global Epoch: 9 | Local Epoch: 8 | Generator Loss: 18.483756 | Discriminator Loss: 0.108346\n","| Global Epoch: 9 | Local Epoch: 9 | Generator Loss: 18.512731 | Discriminator Loss: 0.098197\n","| Global Epoch: 9 | Local Epoch: 0 | Generator Loss: 16.505483 | Discriminator Loss: 0.831346\n","| Global Epoch: 9 | Local Epoch: 1 | Generator Loss: 16.220645 | Discriminator Loss: 0.517707\n","| Global Epoch: 9 | Local Epoch: 2 | Generator Loss: 16.025659 | Discriminator Loss: 0.369893\n","| Global Epoch: 9 | Local Epoch: 3 | Generator Loss: 16.429629 | Discriminator Loss: 0.323843\n","| Global Epoch: 9 | Local Epoch: 4 | Generator Loss: 16.434055 | Discriminator Loss: 0.327215\n","| Global Epoch: 9 | Local Epoch: 5 | Generator Loss: 16.242222 | Discriminator Loss: 0.300501\n","| Global Epoch: 9 | Local Epoch: 6 | Generator Loss: 16.333445 | Discriminator Loss: 0.267648\n","| Global Epoch: 9 | Local Epoch: 7 | Generator Loss: 16.362842 | Discriminator Loss: 0.259730\n","| Global Epoch: 9 | Local Epoch: 8 | Generator Loss: 16.140782 | Discriminator Loss: 0.246215\n","| Global Epoch: 9 | Local Epoch: 9 | Generator Loss: 15.963011 | Discriminator Loss: 0.235965\n","After 10 epoch global training, averged local generator loss is: 16.4251, averged local discrimitor loss is: 0.2098\n","Averged validation loss is: 12.2423\n","| Global Epoch: 10 | Local Epoch: 0 | Generator Loss: 16.015127 | Discriminator Loss: 0.389720\n","| Global Epoch: 10 | Local Epoch: 1 | Generator Loss: 15.775421 | Discriminator Loss: 0.235649\n","| Global Epoch: 10 | Local Epoch: 2 | Generator Loss: 16.435565 | Discriminator Loss: 0.165643\n","| Global Epoch: 10 | Local Epoch: 3 | Generator Loss: 16.711774 | Discriminator Loss: 0.126825\n","| Global Epoch: 10 | Local Epoch: 4 | Generator Loss: 16.887432 | Discriminator Loss: 0.103055\n","| Global Epoch: 10 | Local Epoch: 5 | Generator Loss: 16.963166 | Discriminator Loss: 0.086586\n","| Global Epoch: 10 | Local Epoch: 6 | Generator Loss: 17.019171 | Discriminator Loss: 0.074693\n","| Global Epoch: 10 | Local Epoch: 7 | Generator Loss: 17.104608 | Discriminator Loss: 0.067035\n","| Global Epoch: 10 | Local Epoch: 8 | Generator Loss: 17.061797 | Discriminator Loss: 0.087948\n","| Global Epoch: 10 | Local Epoch: 9 | Generator Loss: 17.021009 | Discriminator Loss: 0.085031\n","| Global Epoch: 10 | Local Epoch: 0 | Generator Loss: 16.891697 | Discriminator Loss: 0.876862\n","| Global Epoch: 10 | Local Epoch: 1 | Generator Loss: 16.968006 | Discriminator Loss: 0.524886\n","| Global Epoch: 10 | Local Epoch: 2 | Generator Loss: 17.098910 | Discriminator Loss: 0.368562\n","| Global Epoch: 10 | Local Epoch: 3 | Generator Loss: 17.134214 | Discriminator Loss: 0.282140\n","| Global Epoch: 10 | Local Epoch: 4 | Generator Loss: 17.231079 | Discriminator Loss: 0.228832\n","| Global Epoch: 10 | Local Epoch: 5 | Generator Loss: 17.156683 | Discriminator Loss: 0.192365\n","| Global Epoch: 10 | Local Epoch: 6 | Generator Loss: 17.221238 | Discriminator Loss: 0.165909\n","| Global Epoch: 10 | Local Epoch: 7 | Generator Loss: 17.239715 | Discriminator Loss: 0.145892\n","| Global Epoch: 10 | Local Epoch: 8 | Generator Loss: 17.244217 | Discriminator Loss: 0.130542\n","| Global Epoch: 10 | Local Epoch: 9 | Generator Loss: 17.248611 | Discriminator Loss: 0.122232\n","| Global Epoch: 10 | Local Epoch: 0 | Generator Loss: 18.692024 | Discriminator Loss: 0.402358\n","| Global Epoch: 10 | Local Epoch: 1 | Generator Loss: 18.739280 | Discriminator Loss: 0.225916\n","| Global Epoch: 10 | Local Epoch: 2 | Generator Loss: 18.383410 | Discriminator Loss: 0.154935\n","| Global Epoch: 10 | Local Epoch: 3 | Generator Loss: 18.307973 | Discriminator Loss: 0.117659\n","| Global Epoch: 10 | Local Epoch: 4 | Generator Loss: 18.466443 | Discriminator Loss: 0.094947\n","| Global Epoch: 10 | Local Epoch: 5 | Generator Loss: 18.543453 | Discriminator Loss: 0.079662\n","| Global Epoch: 10 | Local Epoch: 6 | Generator Loss: 18.585076 | Discriminator Loss: 0.072612\n","| Global Epoch: 10 | Local Epoch: 7 | Generator Loss: 18.233760 | Discriminator Loss: 0.071955\n","| Global Epoch: 10 | Local Epoch: 8 | Generator Loss: 18.175236 | Discriminator Loss: 0.066359\n","| Global Epoch: 10 | Local Epoch: 9 | Generator Loss: 18.115048 | Discriminator Loss: 0.068540\n","| Global Epoch: 10 | Local Epoch: 0 | Generator Loss: 17.523447 | Discriminator Loss: 0.197347\n","| Global Epoch: 10 | Local Epoch: 1 | Generator Loss: 17.701689 | Discriminator Loss: 0.146914\n","| Global Epoch: 10 | Local Epoch: 2 | Generator Loss: 18.157102 | Discriminator Loss: 0.101451\n","| Global Epoch: 10 | Local Epoch: 3 | Generator Loss: 18.121402 | Discriminator Loss: 0.077268\n","| Global Epoch: 10 | Local Epoch: 4 | Generator Loss: 17.733609 | Discriminator Loss: 0.087378\n","| Global Epoch: 10 | Local Epoch: 5 | Generator Loss: 17.767573 | Discriminator Loss: 0.078272\n","| Global Epoch: 10 | Local Epoch: 6 | Generator Loss: 17.897333 | Discriminator Loss: 0.070242\n","| Global Epoch: 10 | Local Epoch: 7 | Generator Loss: 17.920969 | Discriminator Loss: 0.062036\n","| Global Epoch: 10 | Local Epoch: 8 | Generator Loss: 18.089173 | Discriminator Loss: 0.055782\n","| Global Epoch: 10 | Local Epoch: 9 | Generator Loss: 18.229424 | Discriminator Loss: 0.050431\n","| Global Epoch: 10 | Local Epoch: 0 | Generator Loss: 16.594568 | Discriminator Loss: 0.260197\n","| Global Epoch: 10 | Local Epoch: 1 | Generator Loss: 15.963728 | Discriminator Loss: 0.155167\n","| Global Epoch: 10 | Local Epoch: 2 | Generator Loss: 16.401113 | Discriminator Loss: 0.137040\n","| Global Epoch: 10 | Local Epoch: 3 | Generator Loss: 16.426132 | Discriminator Loss: 0.119191\n","| Global Epoch: 10 | Local Epoch: 4 | Generator Loss: 16.807845 | Discriminator Loss: 0.101559\n","| Global Epoch: 10 | Local Epoch: 5 | Generator Loss: 17.223193 | Discriminator Loss: 0.087971\n","| Global Epoch: 10 | Local Epoch: 6 | Generator Loss: 17.527755 | Discriminator Loss: 0.115420\n","| Global Epoch: 10 | Local Epoch: 7 | Generator Loss: 17.295098 | Discriminator Loss: 0.144942\n","| Global Epoch: 10 | Local Epoch: 8 | Generator Loss: 16.960650 | Discriminator Loss: 0.153525\n","| Global Epoch: 10 | Local Epoch: 9 | Generator Loss: 16.928332 | Discriminator Loss: 0.140322\n","After 11 epoch global training, averged local generator loss is: 16.4709, averged local discrimitor loss is: 0.2035\n","Averged validation loss is: 11.4836\n","| Global Epoch: 11 | Local Epoch: 0 | Generator Loss: 15.067494 | Discriminator Loss: 0.782125\n","| Global Epoch: 11 | Local Epoch: 1 | Generator Loss: 15.452789 | Discriminator Loss: 0.467621\n","| Global Epoch: 11 | Local Epoch: 2 | Generator Loss: 15.526387 | Discriminator Loss: 0.326172\n","| Global Epoch: 11 | Local Epoch: 3 | Generator Loss: 15.612497 | Discriminator Loss: 0.249179\n","| Global Epoch: 11 | Local Epoch: 4 | Generator Loss: 15.647787 | Discriminator Loss: 0.201533\n","| Global Epoch: 11 | Local Epoch: 5 | Generator Loss: 15.807371 | Discriminator Loss: 0.169378\n","| Global Epoch: 11 | Local Epoch: 6 | Generator Loss: 15.792221 | Discriminator Loss: 0.148098\n","| Global Epoch: 11 | Local Epoch: 7 | Generator Loss: 15.821062 | Discriminator Loss: 0.139412\n","| Global Epoch: 11 | Local Epoch: 8 | Generator Loss: 15.969400 | Discriminator Loss: 0.127061\n","| Global Epoch: 11 | Local Epoch: 9 | Generator Loss: 16.005479 | Discriminator Loss: 0.117915\n","| Global Epoch: 11 | Local Epoch: 0 | Generator Loss: 17.178823 | Discriminator Loss: 0.481447\n","| Global Epoch: 11 | Local Epoch: 1 | Generator Loss: 17.155123 | Discriminator Loss: 0.287066\n","| Global Epoch: 11 | Local Epoch: 2 | Generator Loss: 16.978554 | Discriminator Loss: 0.198356\n","| Global Epoch: 11 | Local Epoch: 3 | Generator Loss: 16.809676 | Discriminator Loss: 0.150925\n","| Global Epoch: 11 | Local Epoch: 4 | Generator Loss: 16.924123 | Discriminator Loss: 0.123214\n","| Global Epoch: 11 | Local Epoch: 5 | Generator Loss: 16.605062 | Discriminator Loss: 0.154923\n","| Global Epoch: 11 | Local Epoch: 6 | Generator Loss: 16.611093 | Discriminator Loss: 0.142124\n","| Global Epoch: 11 | Local Epoch: 7 | Generator Loss: 16.339585 | Discriminator Loss: 0.141260\n","| Global Epoch: 11 | Local Epoch: 8 | Generator Loss: 16.271441 | Discriminator Loss: 0.126999\n","| Global Epoch: 11 | Local Epoch: 9 | Generator Loss: 16.261766 | Discriminator Loss: 0.114890\n","| Global Epoch: 11 | Local Epoch: 0 | Generator Loss: 15.725040 | Discriminator Loss: 0.504824\n","| Global Epoch: 11 | Local Epoch: 1 | Generator Loss: 15.960892 | Discriminator Loss: 0.287192\n","| Global Epoch: 11 | Local Epoch: 2 | Generator Loss: 16.115946 | Discriminator Loss: 0.202079\n","| Global Epoch: 11 | Local Epoch: 3 | Generator Loss: 16.093855 | Discriminator Loss: 0.154081\n","| Global Epoch: 11 | Local Epoch: 4 | Generator Loss: 16.107240 | Discriminator Loss: 0.128184\n","| Global Epoch: 11 | Local Epoch: 5 | Generator Loss: 16.128594 | Discriminator Loss: 0.117344\n","| Global Epoch: 11 | Local Epoch: 6 | Generator Loss: 16.124281 | Discriminator Loss: 0.107324\n","| Global Epoch: 11 | Local Epoch: 7 | Generator Loss: 16.342611 | Discriminator Loss: 0.097094\n","| Global Epoch: 11 | Local Epoch: 8 | Generator Loss: 16.509564 | Discriminator Loss: 0.088889\n","| Global Epoch: 11 | Local Epoch: 9 | Generator Loss: 16.701595 | Discriminator Loss: 0.080609\n","| Global Epoch: 11 | Local Epoch: 0 | Generator Loss: 19.056171 | Discriminator Loss: 0.764248\n","| Global Epoch: 11 | Local Epoch: 1 | Generator Loss: 18.043976 | Discriminator Loss: 0.446656\n","| Global Epoch: 11 | Local Epoch: 2 | Generator Loss: 17.541012 | Discriminator Loss: 0.307876\n","| Global Epoch: 11 | Local Epoch: 3 | Generator Loss: 17.332839 | Discriminator Loss: 0.233937\n","| Global Epoch: 11 | Local Epoch: 4 | Generator Loss: 17.053901 | Discriminator Loss: 0.188700\n","| Global Epoch: 11 | Local Epoch: 5 | Generator Loss: 16.986462 | Discriminator Loss: 0.158193\n","| Global Epoch: 11 | Local Epoch: 6 | Generator Loss: 16.847645 | Discriminator Loss: 0.136240\n","| Global Epoch: 11 | Local Epoch: 7 | Generator Loss: 16.822709 | Discriminator Loss: 0.119697\n","| Global Epoch: 11 | Local Epoch: 8 | Generator Loss: 16.929923 | Discriminator Loss: 0.106814\n","| Global Epoch: 11 | Local Epoch: 9 | Generator Loss: 17.000643 | Discriminator Loss: 0.096462\n","| Global Epoch: 11 | Local Epoch: 0 | Generator Loss: 15.840125 | Discriminator Loss: 0.395252\n","| Global Epoch: 11 | Local Epoch: 1 | Generator Loss: 15.356529 | Discriminator Loss: 0.325089\n","| Global Epoch: 11 | Local Epoch: 2 | Generator Loss: 15.001167 | Discriminator Loss: 0.256000\n","| Global Epoch: 11 | Local Epoch: 3 | Generator Loss: 15.192073 | Discriminator Loss: 0.211336\n","| Global Epoch: 11 | Local Epoch: 4 | Generator Loss: 15.291561 | Discriminator Loss: 0.170770\n","| Global Epoch: 11 | Local Epoch: 5 | Generator Loss: 15.480542 | Discriminator Loss: 0.153855\n","| Global Epoch: 11 | Local Epoch: 6 | Generator Loss: 15.362376 | Discriminator Loss: 0.152934\n","| Global Epoch: 11 | Local Epoch: 7 | Generator Loss: 15.345594 | Discriminator Loss: 0.153558\n","| Global Epoch: 11 | Local Epoch: 8 | Generator Loss: 15.209712 | Discriminator Loss: 0.161317\n","| Global Epoch: 11 | Local Epoch: 9 | Generator Loss: 15.061756 | Discriminator Loss: 0.173854\n","After 12 epoch global training, averged local generator loss is: 16.3535, averged local discrimitor loss is: 0.2010\n","Averged validation loss is: 10.4008\n","| Global Epoch: 12 | Local Epoch: 0 | Generator Loss: 14.945675 | Discriminator Loss: 0.311167\n","| Global Epoch: 12 | Local Epoch: 1 | Generator Loss: 15.032946 | Discriminator Loss: 0.176515\n","| Global Epoch: 12 | Local Epoch: 2 | Generator Loss: 15.304134 | Discriminator Loss: 0.122846\n","| Global Epoch: 12 | Local Epoch: 3 | Generator Loss: 15.401000 | Discriminator Loss: 0.093597\n","| Global Epoch: 12 | Local Epoch: 4 | Generator Loss: 15.449710 | Discriminator Loss: 0.075695\n","| Global Epoch: 12 | Local Epoch: 5 | Generator Loss: 15.539407 | Discriminator Loss: 0.063596\n","| Global Epoch: 12 | Local Epoch: 6 | Generator Loss: 15.588608 | Discriminator Loss: 0.054886\n","| Global Epoch: 12 | Local Epoch: 7 | Generator Loss: 15.703703 | Discriminator Loss: 0.048934\n","| Global Epoch: 12 | Local Epoch: 8 | Generator Loss: 15.877904 | Discriminator Loss: 0.045271\n","| Global Epoch: 12 | Local Epoch: 9 | Generator Loss: 16.108310 | Discriminator Loss: 0.041111\n","| Global Epoch: 12 | Local Epoch: 0 | Generator Loss: 16.410168 | Discriminator Loss: 0.088718\n","| Global Epoch: 12 | Local Epoch: 1 | Generator Loss: 16.420275 | Discriminator Loss: 0.071088\n","| Global Epoch: 12 | Local Epoch: 2 | Generator Loss: 17.042050 | Discriminator Loss: 0.049989\n","| Global Epoch: 12 | Local Epoch: 3 | Generator Loss: 17.095947 | Discriminator Loss: 0.038372\n","| Global Epoch: 12 | Local Epoch: 4 | Generator Loss: 17.151427 | Discriminator Loss: 0.032686\n","| Global Epoch: 12 | Local Epoch: 5 | Generator Loss: 17.126803 | Discriminator Loss: 0.027782\n","| Global Epoch: 12 | Local Epoch: 6 | Generator Loss: 17.172393 | Discriminator Loss: 0.028664\n","| Global Epoch: 12 | Local Epoch: 7 | Generator Loss: 17.121967 | Discriminator Loss: 0.036797\n","| Global Epoch: 12 | Local Epoch: 8 | Generator Loss: 17.162792 | Discriminator Loss: 0.042805\n","| Global Epoch: 12 | Local Epoch: 9 | Generator Loss: 17.069399 | Discriminator Loss: 0.059774\n","| Global Epoch: 12 | Local Epoch: 0 | Generator Loss: 17.472993 | Discriminator Loss: 0.918395\n","| Global Epoch: 12 | Local Epoch: 1 | Generator Loss: 17.066295 | Discriminator Loss: 0.533315\n","| Global Epoch: 12 | Local Epoch: 2 | Generator Loss: 17.367068 | Discriminator Loss: 0.369555\n","| Global Epoch: 12 | Local Epoch: 3 | Generator Loss: 17.288691 | Discriminator Loss: 0.281134\n","| Global Epoch: 12 | Local Epoch: 4 | Generator Loss: 17.427732 | Discriminator Loss: 0.226884\n","| Global Epoch: 12 | Local Epoch: 5 | Generator Loss: 17.552563 | Discriminator Loss: 0.190671\n","| Global Epoch: 12 | Local Epoch: 6 | Generator Loss: 17.660595 | Discriminator Loss: 0.164571\n","| Global Epoch: 12 | Local Epoch: 7 | Generator Loss: 17.747346 | Discriminator Loss: 0.144632\n","| Global Epoch: 12 | Local Epoch: 8 | Generator Loss: 17.920353 | Discriminator Loss: 0.129070\n","| Global Epoch: 12 | Local Epoch: 9 | Generator Loss: 18.055191 | Discriminator Loss: 0.116535\n","| Global Epoch: 12 | Local Epoch: 0 | Generator Loss: 18.362688 | Discriminator Loss: 0.291576\n","| Global Epoch: 12 | Local Epoch: 1 | Generator Loss: 17.629968 | Discriminator Loss: 0.172754\n","| Global Epoch: 12 | Local Epoch: 2 | Generator Loss: 17.880542 | Discriminator Loss: 0.128545\n","| Global Epoch: 12 | Local Epoch: 3 | Generator Loss: 18.205714 | Discriminator Loss: 0.103743\n","| Global Epoch: 12 | Local Epoch: 4 | Generator Loss: 17.982350 | Discriminator Loss: 0.103151\n","| Global Epoch: 12 | Local Epoch: 5 | Generator Loss: 18.126399 | Discriminator Loss: 0.092779\n","| Global Epoch: 12 | Local Epoch: 6 | Generator Loss: 18.314030 | Discriminator Loss: 0.088102\n","| Global Epoch: 12 | Local Epoch: 7 | Generator Loss: 18.412801 | Discriminator Loss: 0.082192\n","| Global Epoch: 12 | Local Epoch: 8 | Generator Loss: 18.509241 | Discriminator Loss: 0.073673\n","| Global Epoch: 12 | Local Epoch: 9 | Generator Loss: 18.581440 | Discriminator Loss: 0.066567\n","| Global Epoch: 12 | Local Epoch: 0 | Generator Loss: 18.692555 | Discriminator Loss: 0.329115\n","| Global Epoch: 12 | Local Epoch: 1 | Generator Loss: 17.999695 | Discriminator Loss: 0.185389\n","| Global Epoch: 12 | Local Epoch: 2 | Generator Loss: 17.690282 | Discriminator Loss: 0.127179\n","| Global Epoch: 12 | Local Epoch: 3 | Generator Loss: 17.658609 | Discriminator Loss: 0.096640\n","| Global Epoch: 12 | Local Epoch: 4 | Generator Loss: 17.673754 | Discriminator Loss: 0.078030\n","| Global Epoch: 12 | Local Epoch: 5 | Generator Loss: 17.910858 | Discriminator Loss: 0.069533\n","| Global Epoch: 12 | Local Epoch: 6 | Generator Loss: 17.961273 | Discriminator Loss: 0.060474\n","| Global Epoch: 12 | Local Epoch: 7 | Generator Loss: 17.896732 | Discriminator Loss: 0.055178\n","| Global Epoch: 12 | Local Epoch: 8 | Generator Loss: 17.930674 | Discriminator Loss: 0.050554\n","| Global Epoch: 12 | Local Epoch: 9 | Generator Loss: 18.142540 | Discriminator Loss: 0.046168\n","After 13 epoch global training, averged local generator loss is: 16.4911, averged local discrimitor loss is: 0.1891\n","Averged validation loss is: 12.7630\n","| Global Epoch: 13 | Local Epoch: 0 | Generator Loss: 20.861230 | Discriminator Loss: 0.645117\n","| Global Epoch: 13 | Local Epoch: 1 | Generator Loss: 19.440336 | Discriminator Loss: 0.339384\n","| Global Epoch: 13 | Local Epoch: 2 | Generator Loss: 18.208930 | Discriminator Loss: 0.235559\n","| Global Epoch: 13 | Local Epoch: 3 | Generator Loss: 17.610701 | Discriminator Loss: 0.194233\n","| Global Epoch: 13 | Local Epoch: 4 | Generator Loss: 18.022596 | Discriminator Loss: 0.156364\n","| Global Epoch: 13 | Local Epoch: 5 | Generator Loss: 18.667739 | Discriminator Loss: 0.131119\n","| Global Epoch: 13 | Local Epoch: 6 | Generator Loss: 19.053208 | Discriminator Loss: 0.114227\n","| Global Epoch: 13 | Local Epoch: 7 | Generator Loss: 19.341371 | Discriminator Loss: 0.100451\n","| Global Epoch: 13 | Local Epoch: 8 | Generator Loss: 19.552642 | Discriminator Loss: 0.089512\n","| Global Epoch: 13 | Local Epoch: 9 | Generator Loss: 19.869771 | Discriminator Loss: 0.080670\n","| Global Epoch: 13 | Local Epoch: 0 | Generator Loss: 19.953431 | Discriminator Loss: 0.649555\n","| Global Epoch: 13 | Local Epoch: 1 | Generator Loss: 19.038637 | Discriminator Loss: 0.362840\n","| Global Epoch: 13 | Local Epoch: 2 | Generator Loss: 18.394339 | Discriminator Loss: 0.246594\n","| Global Epoch: 13 | Local Epoch: 3 | Generator Loss: 18.745588 | Discriminator Loss: 0.191503\n","| Global Epoch: 13 | Local Epoch: 4 | Generator Loss: 18.186494 | Discriminator Loss: 0.154706\n","| Global Epoch: 13 | Local Epoch: 5 | Generator Loss: 18.022889 | Discriminator Loss: 0.129619\n","| Global Epoch: 13 | Local Epoch: 6 | Generator Loss: 18.179933 | Discriminator Loss: 0.111618\n","| Global Epoch: 13 | Local Epoch: 7 | Generator Loss: 18.353436 | Discriminator Loss: 0.097986\n","| Global Epoch: 13 | Local Epoch: 8 | Generator Loss: 18.452176 | Discriminator Loss: 0.087380\n","| Global Epoch: 13 | Local Epoch: 9 | Generator Loss: 18.626256 | Discriminator Loss: 0.078857\n","| Global Epoch: 13 | Local Epoch: 0 | Generator Loss: 18.549525 | Discriminator Loss: 0.283883\n","| Global Epoch: 13 | Local Epoch: 1 | Generator Loss: 16.971473 | Discriminator Loss: 0.162794\n","| Global Epoch: 13 | Local Epoch: 2 | Generator Loss: 16.970653 | Discriminator Loss: 0.111803\n","| Global Epoch: 13 | Local Epoch: 3 | Generator Loss: 17.075926 | Discriminator Loss: 0.090802\n","| Global Epoch: 13 | Local Epoch: 4 | Generator Loss: 17.333065 | Discriminator Loss: 0.073939\n","| Global Epoch: 13 | Local Epoch: 5 | Generator Loss: 17.507660 | Discriminator Loss: 0.070434\n","| Global Epoch: 13 | Local Epoch: 6 | Generator Loss: 17.350117 | Discriminator Loss: 0.063054\n","| Global Epoch: 13 | Local Epoch: 7 | Generator Loss: 17.312103 | Discriminator Loss: 0.055756\n","| Global Epoch: 13 | Local Epoch: 8 | Generator Loss: 17.469715 | Discriminator Loss: 0.050156\n","| Global Epoch: 13 | Local Epoch: 9 | Generator Loss: 17.593261 | Discriminator Loss: 0.046023\n","| Global Epoch: 13 | Local Epoch: 0 | Generator Loss: 19.332253 | Discriminator Loss: 0.340301\n","| Global Epoch: 13 | Local Epoch: 1 | Generator Loss: 19.076441 | Discriminator Loss: 0.197481\n","| Global Epoch: 13 | Local Epoch: 2 | Generator Loss: 18.600976 | Discriminator Loss: 0.143135\n","| Global Epoch: 13 | Local Epoch: 3 | Generator Loss: 18.482604 | Discriminator Loss: 0.109271\n","| Global Epoch: 13 | Local Epoch: 4 | Generator Loss: 18.667798 | Discriminator Loss: 0.088781\n","| Global Epoch: 13 | Local Epoch: 5 | Generator Loss: 18.279741 | Discriminator Loss: 0.099950\n","| Global Epoch: 13 | Local Epoch: 6 | Generator Loss: 18.300229 | Discriminator Loss: 0.091965\n","| Global Epoch: 13 | Local Epoch: 7 | Generator Loss: 18.321948 | Discriminator Loss: 0.087554\n","| Global Epoch: 13 | Local Epoch: 8 | Generator Loss: 18.217262 | Discriminator Loss: 0.091399\n","| Global Epoch: 13 | Local Epoch: 9 | Generator Loss: 18.288861 | Discriminator Loss: 0.084854\n","| Global Epoch: 13 | Local Epoch: 0 | Generator Loss: 17.849954 | Discriminator Loss: 0.189709\n","| Global Epoch: 13 | Local Epoch: 1 | Generator Loss: 18.164936 | Discriminator Loss: 0.114081\n","| Global Epoch: 13 | Local Epoch: 2 | Generator Loss: 17.787017 | Discriminator Loss: 0.094437\n","| Global Epoch: 13 | Local Epoch: 3 | Generator Loss: 17.963197 | Discriminator Loss: 0.078624\n","| Global Epoch: 13 | Local Epoch: 4 | Generator Loss: 18.078047 | Discriminator Loss: 0.065846\n","| Global Epoch: 13 | Local Epoch: 5 | Generator Loss: 18.189701 | Discriminator Loss: 0.060633\n","| Global Epoch: 13 | Local Epoch: 6 | Generator Loss: 18.269131 | Discriminator Loss: 0.061350\n","| Global Epoch: 13 | Local Epoch: 7 | Generator Loss: 18.259970 | Discriminator Loss: 0.061407\n","| Global Epoch: 13 | Local Epoch: 8 | Generator Loss: 18.312200 | Discriminator Loss: 0.055636\n","| Global Epoch: 13 | Local Epoch: 9 | Generator Loss: 18.457721 | Discriminator Loss: 0.050390\n","After 14 epoch global training, averged local generator loss is: 16.6316, averged local discrimitor loss is: 0.1792\n","Averged validation loss is: 9.1051\n","| Global Epoch: 14 | Local Epoch: 0 | Generator Loss: 18.031623 | Discriminator Loss: 0.498637\n","| Global Epoch: 14 | Local Epoch: 1 | Generator Loss: 17.789523 | Discriminator Loss: 0.279589\n","| Global Epoch: 14 | Local Epoch: 2 | Generator Loss: 17.753029 | Discriminator Loss: 0.191541\n","| Global Epoch: 14 | Local Epoch: 3 | Generator Loss: 17.664842 | Discriminator Loss: 0.145150\n","| Global Epoch: 14 | Local Epoch: 4 | Generator Loss: 17.578107 | Discriminator Loss: 0.116956\n","| Global Epoch: 14 | Local Epoch: 5 | Generator Loss: 17.469565 | Discriminator Loss: 0.097995\n","| Global Epoch: 14 | Local Epoch: 6 | Generator Loss: 17.380893 | Discriminator Loss: 0.084397\n","| Global Epoch: 14 | Local Epoch: 7 | Generator Loss: 17.420416 | Discriminator Loss: 0.074137\n","| Global Epoch: 14 | Local Epoch: 8 | Generator Loss: 17.544141 | Discriminator Loss: 0.066172\n","| Global Epoch: 14 | Local Epoch: 9 | Generator Loss: 17.736847 | Discriminator Loss: 0.059910\n","| Global Epoch: 14 | Local Epoch: 0 | Generator Loss: 18.222997 | Discriminator Loss: 0.035467\n","| Global Epoch: 14 | Local Epoch: 1 | Generator Loss: 17.945627 | Discriminator Loss: 0.033033\n","| Global Epoch: 14 | Local Epoch: 2 | Generator Loss: 17.541119 | Discriminator Loss: 0.042149\n","| Global Epoch: 14 | Local Epoch: 3 | Generator Loss: 17.460425 | Discriminator Loss: 0.073480\n","| Global Epoch: 14 | Local Epoch: 4 | Generator Loss: 17.543685 | Discriminator Loss: 0.074785\n","| Global Epoch: 14 | Local Epoch: 5 | Generator Loss: 17.496525 | Discriminator Loss: 0.064413\n","| Global Epoch: 14 | Local Epoch: 6 | Generator Loss: 17.431649 | Discriminator Loss: 0.069330\n","| Global Epoch: 14 | Local Epoch: 7 | Generator Loss: 17.602841 | Discriminator Loss: 0.065823\n","| Global Epoch: 14 | Local Epoch: 8 | Generator Loss: 17.692017 | Discriminator Loss: 0.058860\n","| Global Epoch: 14 | Local Epoch: 9 | Generator Loss: 17.821178 | Discriminator Loss: 0.053576\n","| Global Epoch: 14 | Local Epoch: 0 | Generator Loss: 18.747796 | Discriminator Loss: 0.201517\n","| Global Epoch: 14 | Local Epoch: 1 | Generator Loss: 18.749278 | Discriminator Loss: 0.114612\n","| Global Epoch: 14 | Local Epoch: 2 | Generator Loss: 18.944231 | Discriminator Loss: 0.081356\n","| Global Epoch: 14 | Local Epoch: 3 | Generator Loss: 18.022951 | Discriminator Loss: 0.114244\n","| Global Epoch: 14 | Local Epoch: 4 | Generator Loss: 18.053395 | Discriminator Loss: 0.098544\n","| Global Epoch: 14 | Local Epoch: 5 | Generator Loss: 18.038755 | Discriminator Loss: 0.083009\n","| Global Epoch: 14 | Local Epoch: 6 | Generator Loss: 18.143619 | Discriminator Loss: 0.072290\n","| Global Epoch: 14 | Local Epoch: 7 | Generator Loss: 18.022852 | Discriminator Loss: 0.063939\n","| Global Epoch: 14 | Local Epoch: 8 | Generator Loss: 18.065585 | Discriminator Loss: 0.059632\n","| Global Epoch: 14 | Local Epoch: 9 | Generator Loss: 18.144262 | Discriminator Loss: 0.056893\n","| Global Epoch: 14 | Local Epoch: 0 | Generator Loss: 17.997464 | Discriminator Loss: 0.734883\n","| Global Epoch: 14 | Local Epoch: 1 | Generator Loss: 17.524038 | Discriminator Loss: 0.383151\n","| Global Epoch: 14 | Local Epoch: 2 | Generator Loss: 17.734736 | Discriminator Loss: 0.261928\n","| Global Epoch: 14 | Local Epoch: 3 | Generator Loss: 17.869754 | Discriminator Loss: 0.197884\n","| Global Epoch: 14 | Local Epoch: 4 | Generator Loss: 18.044446 | Discriminator Loss: 0.158894\n","| Global Epoch: 14 | Local Epoch: 5 | Generator Loss: 18.156832 | Discriminator Loss: 0.132738\n","| Global Epoch: 14 | Local Epoch: 6 | Generator Loss: 18.160727 | Discriminator Loss: 0.113983\n","| Global Epoch: 14 | Local Epoch: 7 | Generator Loss: 18.027245 | Discriminator Loss: 0.099963\n","| Global Epoch: 14 | Local Epoch: 8 | Generator Loss: 18.105503 | Discriminator Loss: 0.088956\n","| Global Epoch: 14 | Local Epoch: 9 | Generator Loss: 18.146325 | Discriminator Loss: 0.080328\n","| Global Epoch: 14 | Local Epoch: 0 | Generator Loss: 16.905971 | Discriminator Loss: 0.497831\n","| Global Epoch: 14 | Local Epoch: 1 | Generator Loss: 16.670418 | Discriminator Loss: 0.277275\n","| Global Epoch: 14 | Local Epoch: 2 | Generator Loss: 16.563048 | Discriminator Loss: 0.196975\n","| Global Epoch: 14 | Local Epoch: 3 | Generator Loss: 17.029179 | Discriminator Loss: 0.152444\n","| Global Epoch: 14 | Local Epoch: 4 | Generator Loss: 16.946796 | Discriminator Loss: 0.128315\n","| Global Epoch: 14 | Local Epoch: 5 | Generator Loss: 17.036112 | Discriminator Loss: 0.108245\n","| Global Epoch: 14 | Local Epoch: 6 | Generator Loss: 17.126664 | Discriminator Loss: 0.093262\n","| Global Epoch: 14 | Local Epoch: 7 | Generator Loss: 17.217636 | Discriminator Loss: 0.081951\n","| Global Epoch: 14 | Local Epoch: 8 | Generator Loss: 17.296232 | Discriminator Loss: 0.073090\n","| Global Epoch: 14 | Local Epoch: 9 | Generator Loss: 17.319575 | Discriminator Loss: 0.065970\n","After 15 epoch global training, averged local generator loss is: 16.6774, averged local discrimitor loss is: 0.1716\n","Averged validation loss is: 7.4905\n","| Global Epoch: 15 | Local Epoch: 0 | Generator Loss: 16.857240 | Discriminator Loss: 0.052896\n","| Global Epoch: 15 | Local Epoch: 1 | Generator Loss: 16.832667 | Discriminator Loss: 0.040741\n","| Global Epoch: 15 | Local Epoch: 2 | Generator Loss: 16.924878 | Discriminator Loss: 0.056573\n","| Global Epoch: 15 | Local Epoch: 3 | Generator Loss: 16.949344 | Discriminator Loss: 0.044906\n","| Global Epoch: 15 | Local Epoch: 4 | Generator Loss: 17.048284 | Discriminator Loss: 0.037493\n","| Global Epoch: 15 | Local Epoch: 5 | Generator Loss: 17.438898 | Discriminator Loss: 0.031691\n","| Global Epoch: 15 | Local Epoch: 6 | Generator Loss: 17.609190 | Discriminator Loss: 0.027379\n","| Global Epoch: 15 | Local Epoch: 7 | Generator Loss: 17.683319 | Discriminator Loss: 0.024082\n","| Global Epoch: 15 | Local Epoch: 8 | Generator Loss: 17.743311 | Discriminator Loss: 0.021515\n","| Global Epoch: 15 | Local Epoch: 9 | Generator Loss: 17.828903 | Discriminator Loss: 0.020737\n","| Global Epoch: 15 | Local Epoch: 0 | Generator Loss: 16.719200 | Discriminator Loss: 0.526225\n","| Global Epoch: 15 | Local Epoch: 1 | Generator Loss: 15.702474 | Discriminator Loss: 0.400723\n","| Global Epoch: 15 | Local Epoch: 2 | Generator Loss: 15.057010 | Discriminator Loss: 0.330710\n","| Global Epoch: 15 | Local Epoch: 3 | Generator Loss: 15.064457 | Discriminator Loss: 0.265774\n","| Global Epoch: 15 | Local Epoch: 4 | Generator Loss: 15.236220 | Discriminator Loss: 0.215658\n","| Global Epoch: 15 | Local Epoch: 5 | Generator Loss: 15.274955 | Discriminator Loss: 0.184435\n","| Global Epoch: 15 | Local Epoch: 6 | Generator Loss: 15.296424 | Discriminator Loss: 0.159908\n","| Global Epoch: 15 | Local Epoch: 7 | Generator Loss: 15.337397 | Discriminator Loss: 0.140569\n","| Global Epoch: 15 | Local Epoch: 8 | Generator Loss: 15.464370 | Discriminator Loss: 0.139284\n","| Global Epoch: 15 | Local Epoch: 9 | Generator Loss: 15.447333 | Discriminator Loss: 0.143634\n","| Global Epoch: 15 | Local Epoch: 0 | Generator Loss: 16.032969 | Discriminator Loss: 0.237626\n","| Global Epoch: 15 | Local Epoch: 1 | Generator Loss: 15.899903 | Discriminator Loss: 0.131756\n","| Global Epoch: 15 | Local Epoch: 2 | Generator Loss: 16.057543 | Discriminator Loss: 0.101116\n","| Global Epoch: 15 | Local Epoch: 3 | Generator Loss: 15.959235 | Discriminator Loss: 0.086242\n","| Global Epoch: 15 | Local Epoch: 4 | Generator Loss: 15.996446 | Discriminator Loss: 0.073420\n","| Global Epoch: 15 | Local Epoch: 5 | Generator Loss: 16.104334 | Discriminator Loss: 0.062137\n","| Global Epoch: 15 | Local Epoch: 6 | Generator Loss: 16.204359 | Discriminator Loss: 0.053754\n","| Global Epoch: 15 | Local Epoch: 7 | Generator Loss: 16.349237 | Discriminator Loss: 0.047285\n","| Global Epoch: 15 | Local Epoch: 8 | Generator Loss: 16.475236 | Discriminator Loss: 0.042208\n","| Global Epoch: 15 | Local Epoch: 9 | Generator Loss: 16.490870 | Discriminator Loss: 0.038119\n","| Global Epoch: 15 | Local Epoch: 0 | Generator Loss: 16.063324 | Discriminator Loss: 0.465684\n","| Global Epoch: 15 | Local Epoch: 1 | Generator Loss: 15.777461 | Discriminator Loss: 0.262368\n","| Global Epoch: 15 | Local Epoch: 2 | Generator Loss: 15.696557 | Discriminator Loss: 0.182697\n","| Global Epoch: 15 | Local Epoch: 3 | Generator Loss: 15.586381 | Discriminator Loss: 0.138642\n","| Global Epoch: 15 | Local Epoch: 4 | Generator Loss: 15.604655 | Discriminator Loss: 0.111698\n","| Global Epoch: 15 | Local Epoch: 5 | Generator Loss: 15.605846 | Discriminator Loss: 0.093624\n","| Global Epoch: 15 | Local Epoch: 6 | Generator Loss: 15.656790 | Discriminator Loss: 0.080656\n","| Global Epoch: 15 | Local Epoch: 7 | Generator Loss: 15.747802 | Discriminator Loss: 0.070892\n","| Global Epoch: 15 | Local Epoch: 8 | Generator Loss: 15.814426 | Discriminator Loss: 0.063253\n","| Global Epoch: 15 | Local Epoch: 9 | Generator Loss: 15.876984 | Discriminator Loss: 0.057107\n","| Global Epoch: 15 | Local Epoch: 0 | Generator Loss: 15.992540 | Discriminator Loss: 0.105619\n","| Global Epoch: 15 | Local Epoch: 1 | Generator Loss: 15.933341 | Discriminator Loss: 0.061999\n","| Global Epoch: 15 | Local Epoch: 2 | Generator Loss: 15.755871 | Discriminator Loss: 0.044602\n","| Global Epoch: 15 | Local Epoch: 3 | Generator Loss: 15.852850 | Discriminator Loss: 0.037925\n","| Global Epoch: 15 | Local Epoch: 4 | Generator Loss: 15.864331 | Discriminator Loss: 0.036415\n","| Global Epoch: 15 | Local Epoch: 5 | Generator Loss: 15.961502 | Discriminator Loss: 0.033495\n","| Global Epoch: 15 | Local Epoch: 6 | Generator Loss: 15.912320 | Discriminator Loss: 0.071180\n","| Global Epoch: 15 | Local Epoch: 7 | Generator Loss: 16.077228 | Discriminator Loss: 0.066697\n","| Global Epoch: 15 | Local Epoch: 8 | Generator Loss: 16.029951 | Discriminator Loss: 0.060100\n","| Global Epoch: 15 | Local Epoch: 9 | Generator Loss: 16.038164 | Discriminator Loss: 0.056270\n","After 16 epoch global training, averged local generator loss is: 16.6375, averged local discrimitor loss is: 0.1644\n","Averged validation loss is: 7.8639\n","| Global Epoch: 16 | Local Epoch: 0 | Generator Loss: 16.101574 | Discriminator Loss: 0.029624\n","| Global Epoch: 16 | Local Epoch: 1 | Generator Loss: 16.467600 | Discriminator Loss: 0.037733\n","| Global Epoch: 16 | Local Epoch: 2 | Generator Loss: 16.258457 | Discriminator Loss: 0.060240\n","| Global Epoch: 16 | Local Epoch: 3 | Generator Loss: 16.522121 | Discriminator Loss: 0.055812\n","| Global Epoch: 16 | Local Epoch: 4 | Generator Loss: 16.724642 | Discriminator Loss: 0.045653\n","| Global Epoch: 16 | Local Epoch: 5 | Generator Loss: 16.878131 | Discriminator Loss: 0.038359\n","| Global Epoch: 16 | Local Epoch: 6 | Generator Loss: 17.011010 | Discriminator Loss: 0.033065\n","| Global Epoch: 16 | Local Epoch: 7 | Generator Loss: 17.161166 | Discriminator Loss: 0.038961\n","| Global Epoch: 16 | Local Epoch: 8 | Generator Loss: 17.209679 | Discriminator Loss: 0.037447\n","| Global Epoch: 16 | Local Epoch: 9 | Generator Loss: 17.185953 | Discriminator Loss: 0.038857\n","| Global Epoch: 16 | Local Epoch: 0 | Generator Loss: 17.367848 | Discriminator Loss: 0.388591\n","| Global Epoch: 16 | Local Epoch: 1 | Generator Loss: 17.245108 | Discriminator Loss: 0.214715\n","| Global Epoch: 16 | Local Epoch: 2 | Generator Loss: 17.250689 | Discriminator Loss: 0.146932\n","| Global Epoch: 16 | Local Epoch: 3 | Generator Loss: 17.568884 | Discriminator Loss: 0.113162\n","| Global Epoch: 16 | Local Epoch: 4 | Generator Loss: 17.467466 | Discriminator Loss: 0.091385\n","| Global Epoch: 16 | Local Epoch: 5 | Generator Loss: 17.491570 | Discriminator Loss: 0.076682\n","| Global Epoch: 16 | Local Epoch: 6 | Generator Loss: 17.672508 | Discriminator Loss: 0.066178\n","| Global Epoch: 16 | Local Epoch: 7 | Generator Loss: 17.771003 | Discriminator Loss: 0.058163\n","| Global Epoch: 16 | Local Epoch: 8 | Generator Loss: 17.833327 | Discriminator Loss: 0.051897\n","| Global Epoch: 16 | Local Epoch: 9 | Generator Loss: 17.863830 | Discriminator Loss: 0.046862\n","| Global Epoch: 16 | Local Epoch: 0 | Generator Loss: 17.650506 | Discriminator Loss: 0.334718\n","| Global Epoch: 16 | Local Epoch: 1 | Generator Loss: 17.138523 | Discriminator Loss: 0.189737\n","| Global Epoch: 16 | Local Epoch: 2 | Generator Loss: 16.952465 | Discriminator Loss: 0.129928\n","| Global Epoch: 16 | Local Epoch: 3 | Generator Loss: 17.150197 | Discriminator Loss: 0.098693\n","| Global Epoch: 16 | Local Epoch: 4 | Generator Loss: 17.277236 | Discriminator Loss: 0.079673\n","| Global Epoch: 16 | Local Epoch: 5 | Generator Loss: 17.283585 | Discriminator Loss: 0.066871\n","| Global Epoch: 16 | Local Epoch: 6 | Generator Loss: 17.351255 | Discriminator Loss: 0.057627\n","| Global Epoch: 16 | Local Epoch: 7 | Generator Loss: 17.435763 | Discriminator Loss: 0.050812\n","| Global Epoch: 16 | Local Epoch: 8 | Generator Loss: 17.502738 | Discriminator Loss: 0.045861\n","| Global Epoch: 16 | Local Epoch: 9 | Generator Loss: 17.503864 | Discriminator Loss: 0.041700\n","| Global Epoch: 16 | Local Epoch: 0 | Generator Loss: 19.397709 | Discriminator Loss: 0.328361\n","| Global Epoch: 16 | Local Epoch: 1 | Generator Loss: 19.248957 | Discriminator Loss: 0.169988\n","| Global Epoch: 16 | Local Epoch: 2 | Generator Loss: 18.658537 | Discriminator Loss: 0.116003\n","| Global Epoch: 16 | Local Epoch: 3 | Generator Loss: 18.605287 | Discriminator Loss: 0.088541\n","| Global Epoch: 16 | Local Epoch: 4 | Generator Loss: 18.511162 | Discriminator Loss: 0.071257\n","| Global Epoch: 16 | Local Epoch: 5 | Generator Loss: 18.175988 | Discriminator Loss: 0.059575\n","| Global Epoch: 16 | Local Epoch: 6 | Generator Loss: 17.841184 | Discriminator Loss: 0.051175\n","| Global Epoch: 16 | Local Epoch: 7 | Generator Loss: 17.567330 | Discriminator Loss: 0.044858\n","| Global Epoch: 16 | Local Epoch: 8 | Generator Loss: 17.427204 | Discriminator Loss: 0.040013\n","| Global Epoch: 16 | Local Epoch: 9 | Generator Loss: 17.405326 | Discriminator Loss: 0.036120\n","| Global Epoch: 16 | Local Epoch: 0 | Generator Loss: 17.068462 | Discriminator Loss: 0.380532\n","| Global Epoch: 16 | Local Epoch: 1 | Generator Loss: 17.385093 | Discriminator Loss: 0.214217\n","| Global Epoch: 16 | Local Epoch: 2 | Generator Loss: 16.889589 | Discriminator Loss: 0.148431\n","| Global Epoch: 16 | Local Epoch: 3 | Generator Loss: 17.120604 | Discriminator Loss: 0.139436\n","| Global Epoch: 16 | Local Epoch: 4 | Generator Loss: 16.783815 | Discriminator Loss: 0.156616\n","| Global Epoch: 16 | Local Epoch: 5 | Generator Loss: 16.792529 | Discriminator Loss: 0.147388\n","| Global Epoch: 16 | Local Epoch: 6 | Generator Loss: 16.862719 | Discriminator Loss: 0.128865\n","| Global Epoch: 16 | Local Epoch: 7 | Generator Loss: 16.607685 | Discriminator Loss: 0.123058\n","| Global Epoch: 16 | Local Epoch: 8 | Generator Loss: 16.635654 | Discriminator Loss: 0.122907\n","| Global Epoch: 16 | Local Epoch: 9 | Generator Loss: 16.621503 | Discriminator Loss: 0.135358\n","After 17 epoch global training, averged local generator loss is: 16.6365, averged local discrimitor loss is: 0.1627\n","Averged validation loss is: 8.4231\n","| Global Epoch: 17 | Local Epoch: 0 | Generator Loss: 17.052498 | Discriminator Loss: 0.835908\n","| Global Epoch: 17 | Local Epoch: 1 | Generator Loss: 17.185305 | Discriminator Loss: 0.476639\n","| Global Epoch: 17 | Local Epoch: 2 | Generator Loss: 17.007171 | Discriminator Loss: 0.325809\n","| Global Epoch: 17 | Local Epoch: 3 | Generator Loss: 16.936391 | Discriminator Loss: 0.247260\n","| Global Epoch: 17 | Local Epoch: 4 | Generator Loss: 16.919056 | Discriminator Loss: 0.199357\n","| Global Epoch: 17 | Local Epoch: 5 | Generator Loss: 16.927159 | Discriminator Loss: 0.167080\n","| Global Epoch: 17 | Local Epoch: 6 | Generator Loss: 16.913791 | Discriminator Loss: 0.143871\n","| Global Epoch: 17 | Local Epoch: 7 | Generator Loss: 16.882974 | Discriminator Loss: 0.126398\n","| Global Epoch: 17 | Local Epoch: 8 | Generator Loss: 16.896820 | Discriminator Loss: 0.113413\n","| Global Epoch: 17 | Local Epoch: 9 | Generator Loss: 16.770021 | Discriminator Loss: 0.102502\n","| Global Epoch: 17 | Local Epoch: 0 | Generator Loss: 16.180451 | Discriminator Loss: 0.842004\n","| Global Epoch: 17 | Local Epoch: 1 | Generator Loss: 14.884890 | Discriminator Loss: 0.508994\n","| Global Epoch: 17 | Local Epoch: 2 | Generator Loss: 14.670757 | Discriminator Loss: 0.359726\n","| Global Epoch: 17 | Local Epoch: 3 | Generator Loss: 14.761637 | Discriminator Loss: 0.276177\n","| Global Epoch: 17 | Local Epoch: 4 | Generator Loss: 14.957003 | Discriminator Loss: 0.223644\n","| Global Epoch: 17 | Local Epoch: 5 | Generator Loss: 15.196485 | Discriminator Loss: 0.188007\n","| Global Epoch: 17 | Local Epoch: 6 | Generator Loss: 15.333903 | Discriminator Loss: 0.162049\n","| Global Epoch: 17 | Local Epoch: 7 | Generator Loss: 15.522846 | Discriminator Loss: 0.142568\n","| Global Epoch: 17 | Local Epoch: 8 | Generator Loss: 15.560087 | Discriminator Loss: 0.130289\n","| Global Epoch: 17 | Local Epoch: 9 | Generator Loss: 15.590152 | Discriminator Loss: 0.138054\n","| Global Epoch: 17 | Local Epoch: 0 | Generator Loss: 15.613960 | Discriminator Loss: 0.167985\n","| Global Epoch: 17 | Local Epoch: 1 | Generator Loss: 15.906743 | Discriminator Loss: 0.102185\n","| Global Epoch: 17 | Local Epoch: 2 | Generator Loss: 16.130643 | Discriminator Loss: 0.073383\n","| Global Epoch: 17 | Local Epoch: 3 | Generator Loss: 16.210069 | Discriminator Loss: 0.057701\n","| Global Epoch: 17 | Local Epoch: 4 | Generator Loss: 16.326338 | Discriminator Loss: 0.046725\n","| Global Epoch: 17 | Local Epoch: 5 | Generator Loss: 16.542452 | Discriminator Loss: 0.039321\n","| Global Epoch: 17 | Local Epoch: 6 | Generator Loss: 16.822952 | Discriminator Loss: 0.034073\n","| Global Epoch: 17 | Local Epoch: 7 | Generator Loss: 16.904963 | Discriminator Loss: 0.030006\n","| Global Epoch: 17 | Local Epoch: 8 | Generator Loss: 17.024268 | Discriminator Loss: 0.026796\n","| Global Epoch: 17 | Local Epoch: 9 | Generator Loss: 17.058707 | Discriminator Loss: 0.024357\n","| Global Epoch: 17 | Local Epoch: 0 | Generator Loss: 17.944601 | Discriminator Loss: 0.778922\n","| Global Epoch: 17 | Local Epoch: 1 | Generator Loss: 17.013929 | Discriminator Loss: 0.455012\n","| Global Epoch: 17 | Local Epoch: 2 | Generator Loss: 15.988693 | Discriminator Loss: 0.328744\n","| Global Epoch: 17 | Local Epoch: 3 | Generator Loss: 15.388291 | Discriminator Loss: 0.253572\n","| Global Epoch: 17 | Local Epoch: 4 | Generator Loss: 15.274868 | Discriminator Loss: 0.212989\n","| Global Epoch: 17 | Local Epoch: 5 | Generator Loss: 15.447461 | Discriminator Loss: 0.181329\n","| Global Epoch: 17 | Local Epoch: 6 | Generator Loss: 15.361115 | Discriminator Loss: 0.156630\n","| Global Epoch: 17 | Local Epoch: 7 | Generator Loss: 15.261345 | Discriminator Loss: 0.137787\n","| Global Epoch: 17 | Local Epoch: 8 | Generator Loss: 15.348056 | Discriminator Loss: 0.126220\n","| Global Epoch: 17 | Local Epoch: 9 | Generator Loss: 15.412499 | Discriminator Loss: 0.115548\n","| Global Epoch: 17 | Local Epoch: 0 | Generator Loss: 15.963633 | Discriminator Loss: 0.341706\n","| Global Epoch: 17 | Local Epoch: 1 | Generator Loss: 16.426827 | Discriminator Loss: 0.179086\n","| Global Epoch: 17 | Local Epoch: 2 | Generator Loss: 16.768756 | Discriminator Loss: 0.121418\n","| Global Epoch: 17 | Local Epoch: 3 | Generator Loss: 17.069642 | Discriminator Loss: 0.091889\n","| Global Epoch: 17 | Local Epoch: 4 | Generator Loss: 17.095705 | Discriminator Loss: 0.073936\n","| Global Epoch: 17 | Local Epoch: 5 | Generator Loss: 17.311543 | Discriminator Loss: 0.062232\n","| Global Epoch: 17 | Local Epoch: 6 | Generator Loss: 17.455086 | Discriminator Loss: 0.053508\n","| Global Epoch: 17 | Local Epoch: 7 | Generator Loss: 17.537121 | Discriminator Loss: 0.047032\n","| Global Epoch: 17 | Local Epoch: 8 | Generator Loss: 17.501831 | Discriminator Loss: 0.041891\n","| Global Epoch: 17 | Local Epoch: 9 | Generator Loss: 17.551086 | Discriminator Loss: 0.037755\n","After 18 epoch global training, averged local generator loss is: 16.6873, averged local discrimitor loss is: 0.1558\n","Averged validation loss is: 9.7244\n","| Global Epoch: 18 | Local Epoch: 0 | Generator Loss: 16.467918 | Discriminator Loss: 0.408140\n","| Global Epoch: 18 | Local Epoch: 1 | Generator Loss: 16.801389 | Discriminator Loss: 0.222164\n","| Global Epoch: 18 | Local Epoch: 2 | Generator Loss: 16.841384 | Discriminator Loss: 0.151317\n","| Global Epoch: 18 | Local Epoch: 3 | Generator Loss: 16.879481 | Discriminator Loss: 0.114575\n","| Global Epoch: 18 | Local Epoch: 4 | Generator Loss: 17.199897 | Discriminator Loss: 0.092778\n","| Global Epoch: 18 | Local Epoch: 5 | Generator Loss: 17.364725 | Discriminator Loss: 0.077911\n","| Global Epoch: 18 | Local Epoch: 6 | Generator Loss: 17.482008 | Discriminator Loss: 0.067152\n","| Global Epoch: 18 | Local Epoch: 7 | Generator Loss: 17.519960 | Discriminator Loss: 0.059003\n","| Global Epoch: 18 | Local Epoch: 8 | Generator Loss: 17.526695 | Discriminator Loss: 0.052647\n","| Global Epoch: 18 | Local Epoch: 9 | Generator Loss: 17.502757 | Discriminator Loss: 0.047538\n","| Global Epoch: 18 | Local Epoch: 0 | Generator Loss: 15.921553 | Discriminator Loss: 0.164046\n","| Global Epoch: 18 | Local Epoch: 1 | Generator Loss: 15.660263 | Discriminator Loss: 0.145993\n","| Global Epoch: 18 | Local Epoch: 2 | Generator Loss: 15.563160 | Discriminator Loss: 0.137472\n","| Global Epoch: 18 | Local Epoch: 3 | Generator Loss: 15.491270 | Discriminator Loss: 0.111089\n","| Global Epoch: 18 | Local Epoch: 4 | Generator Loss: 15.762559 | Discriminator Loss: 0.098049\n","| Global Epoch: 18 | Local Epoch: 5 | Generator Loss: 15.552309 | Discriminator Loss: 0.101525\n","| Global Epoch: 18 | Local Epoch: 6 | Generator Loss: 15.890803 | Discriminator Loss: 0.093199\n","| Global Epoch: 18 | Local Epoch: 7 | Generator Loss: 16.008609 | Discriminator Loss: 0.098255\n","| Global Epoch: 18 | Local Epoch: 8 | Generator Loss: 16.168198 | Discriminator Loss: 0.089431\n","| Global Epoch: 18 | Local Epoch: 9 | Generator Loss: 16.284844 | Discriminator Loss: 0.083487\n","| Global Epoch: 18 | Local Epoch: 0 | Generator Loss: 16.862879 | Discriminator Loss: 0.380394\n","| Global Epoch: 18 | Local Epoch: 1 | Generator Loss: 16.611773 | Discriminator Loss: 0.210149\n","| Global Epoch: 18 | Local Epoch: 2 | Generator Loss: 16.587379 | Discriminator Loss: 0.143820\n","| Global Epoch: 18 | Local Epoch: 3 | Generator Loss: 16.637310 | Discriminator Loss: 0.109295\n","| Global Epoch: 18 | Local Epoch: 4 | Generator Loss: 16.572839 | Discriminator Loss: 0.088256\n","| Global Epoch: 18 | Local Epoch: 5 | Generator Loss: 16.548082 | Discriminator Loss: 0.074087\n","| Global Epoch: 18 | Local Epoch: 6 | Generator Loss: 16.482339 | Discriminator Loss: 0.063901\n","| Global Epoch: 18 | Local Epoch: 7 | Generator Loss: 16.552838 | Discriminator Loss: 0.056227\n","| Global Epoch: 18 | Local Epoch: 8 | Generator Loss: 16.567580 | Discriminator Loss: 0.050223\n","| Global Epoch: 18 | Local Epoch: 9 | Generator Loss: 16.578584 | Discriminator Loss: 0.045391\n","| Global Epoch: 18 | Local Epoch: 0 | Generator Loss: 18.872759 | Discriminator Loss: 0.064341\n","| Global Epoch: 18 | Local Epoch: 1 | Generator Loss: 18.516570 | Discriminator Loss: 0.036366\n","| Global Epoch: 18 | Local Epoch: 2 | Generator Loss: 17.994434 | Discriminator Loss: 0.024949\n","| Global Epoch: 18 | Local Epoch: 3 | Generator Loss: 17.787826 | Discriminator Loss: 0.019094\n","| Global Epoch: 18 | Local Epoch: 4 | Generator Loss: 17.690954 | Discriminator Loss: 0.015470\n","| Global Epoch: 18 | Local Epoch: 5 | Generator Loss: 17.660824 | Discriminator Loss: 0.012982\n","| Global Epoch: 18 | Local Epoch: 6 | Generator Loss: 17.558495 | Discriminator Loss: 0.011202\n","| Global Epoch: 18 | Local Epoch: 7 | Generator Loss: 17.457059 | Discriminator Loss: 0.009840\n","| Global Epoch: 18 | Local Epoch: 8 | Generator Loss: 17.518108 | Discriminator Loss: 0.008786\n","| Global Epoch: 18 | Local Epoch: 9 | Generator Loss: 17.552967 | Discriminator Loss: 0.007936\n","| Global Epoch: 18 | Local Epoch: 0 | Generator Loss: 16.614239 | Discriminator Loss: 0.300375\n","| Global Epoch: 18 | Local Epoch: 1 | Generator Loss: 16.498938 | Discriminator Loss: 0.169619\n","| Global Epoch: 18 | Local Epoch: 2 | Generator Loss: 16.312203 | Discriminator Loss: 0.116355\n","| Global Epoch: 18 | Local Epoch: 3 | Generator Loss: 16.476836 | Discriminator Loss: 0.088764\n","| Global Epoch: 18 | Local Epoch: 4 | Generator Loss: 16.558327 | Discriminator Loss: 0.071729\n","| Global Epoch: 18 | Local Epoch: 5 | Generator Loss: 16.619945 | Discriminator Loss: 0.060225\n","| Global Epoch: 18 | Local Epoch: 6 | Generator Loss: 16.764245 | Discriminator Loss: 0.051985\n","| Global Epoch: 18 | Local Epoch: 7 | Generator Loss: 16.976201 | Discriminator Loss: 0.045740\n","| Global Epoch: 18 | Local Epoch: 8 | Generator Loss: 17.104561 | Discriminator Loss: 0.040852\n","| Global Epoch: 18 | Local Epoch: 9 | Generator Loss: 17.162767 | Discriminator Loss: 0.036916\n","After 19 epoch global training, averged local generator loss is: 16.7124, averged local discrimitor loss is: 0.1495\n","Averged validation loss is: 6.7432\n","| Global Epoch: 19 | Local Epoch: 0 | Generator Loss: 17.846200 | Discriminator Loss: 0.621919\n","| Global Epoch: 19 | Local Epoch: 1 | Generator Loss: 16.970058 | Discriminator Loss: 0.351222\n","| Global Epoch: 19 | Local Epoch: 2 | Generator Loss: 16.863192 | Discriminator Loss: 0.241426\n","| Global Epoch: 19 | Local Epoch: 3 | Generator Loss: 16.679185 | Discriminator Loss: 0.183192\n","| Global Epoch: 19 | Local Epoch: 4 | Generator Loss: 16.669478 | Discriminator Loss: 0.148725\n","| Global Epoch: 19 | Local Epoch: 5 | Generator Loss: 16.829301 | Discriminator Loss: 0.125279\n","| Global Epoch: 19 | Local Epoch: 6 | Generator Loss: 16.819902 | Discriminator Loss: 0.107949\n","| Global Epoch: 19 | Local Epoch: 7 | Generator Loss: 16.889609 | Discriminator Loss: 0.094865\n","| Global Epoch: 19 | Local Epoch: 8 | Generator Loss: 17.009240 | Discriminator Loss: 0.084650\n","| Global Epoch: 19 | Local Epoch: 9 | Generator Loss: 17.120964 | Discriminator Loss: 0.076515\n","| Global Epoch: 19 | Local Epoch: 0 | Generator Loss: 16.189453 | Discriminator Loss: 0.052603\n","| Global Epoch: 19 | Local Epoch: 1 | Generator Loss: 16.354505 | Discriminator Loss: 0.058234\n","| Global Epoch: 19 | Local Epoch: 2 | Generator Loss: 16.169197 | Discriminator Loss: 0.049176\n","| Global Epoch: 19 | Local Epoch: 3 | Generator Loss: 16.127885 | Discriminator Loss: 0.045890\n","| Global Epoch: 19 | Local Epoch: 4 | Generator Loss: 16.148887 | Discriminator Loss: 0.041521\n","| Global Epoch: 19 | Local Epoch: 5 | Generator Loss: 16.208690 | Discriminator Loss: 0.035328\n","| Global Epoch: 19 | Local Epoch: 6 | Generator Loss: 16.278190 | Discriminator Loss: 0.030531\n","| Global Epoch: 19 | Local Epoch: 7 | Generator Loss: 16.450977 | Discriminator Loss: 0.027198\n","| Global Epoch: 19 | Local Epoch: 8 | Generator Loss: 16.537865 | Discriminator Loss: 0.024620\n","| Global Epoch: 19 | Local Epoch: 9 | Generator Loss: 16.622739 | Discriminator Loss: 0.022714\n","| Global Epoch: 19 | Local Epoch: 0 | Generator Loss: 16.339495 | Discriminator Loss: 0.428814\n","| Global Epoch: 19 | Local Epoch: 1 | Generator Loss: 16.441913 | Discriminator Loss: 0.243689\n","| Global Epoch: 19 | Local Epoch: 2 | Generator Loss: 16.347419 | Discriminator Loss: 0.167846\n","| Global Epoch: 19 | Local Epoch: 3 | Generator Loss: 16.364600 | Discriminator Loss: 0.127423\n","| Global Epoch: 19 | Local Epoch: 4 | Generator Loss: 16.331110 | Discriminator Loss: 0.104986\n","| Global Epoch: 19 | Local Epoch: 5 | Generator Loss: 16.342175 | Discriminator Loss: 0.088319\n","| Global Epoch: 19 | Local Epoch: 6 | Generator Loss: 16.321966 | Discriminator Loss: 0.099346\n","| Global Epoch: 19 | Local Epoch: 7 | Generator Loss: 16.326730 | Discriminator Loss: 0.094659\n","| Global Epoch: 19 | Local Epoch: 8 | Generator Loss: 16.327079 | Discriminator Loss: 0.088377\n","| Global Epoch: 19 | Local Epoch: 9 | Generator Loss: 16.406861 | Discriminator Loss: 0.080164\n","| Global Epoch: 19 | Local Epoch: 0 | Generator Loss: 16.381399 | Discriminator Loss: 0.481684\n","| Global Epoch: 19 | Local Epoch: 1 | Generator Loss: 16.228191 | Discriminator Loss: 0.271159\n","| Global Epoch: 19 | Local Epoch: 2 | Generator Loss: 16.230564 | Discriminator Loss: 0.193394\n","| Global Epoch: 19 | Local Epoch: 3 | Generator Loss: 15.911379 | Discriminator Loss: 0.148433\n","| Global Epoch: 19 | Local Epoch: 4 | Generator Loss: 16.098966 | Discriminator Loss: 0.120252\n","| Global Epoch: 19 | Local Epoch: 5 | Generator Loss: 16.166907 | Discriminator Loss: 0.100903\n","| Global Epoch: 19 | Local Epoch: 6 | Generator Loss: 16.198499 | Discriminator Loss: 0.086967\n","| Global Epoch: 19 | Local Epoch: 7 | Generator Loss: 16.215452 | Discriminator Loss: 0.078229\n","| Global Epoch: 19 | Local Epoch: 8 | Generator Loss: 16.111242 | Discriminator Loss: 0.089110\n","| Global Epoch: 19 | Local Epoch: 9 | Generator Loss: 15.956094 | Discriminator Loss: 0.083108\n","| Global Epoch: 19 | Local Epoch: 0 | Generator Loss: 15.567231 | Discriminator Loss: 0.468312\n","| Global Epoch: 19 | Local Epoch: 1 | Generator Loss: 15.222844 | Discriminator Loss: 0.270039\n","| Global Epoch: 19 | Local Epoch: 2 | Generator Loss: 15.395829 | Discriminator Loss: 0.185969\n","| Global Epoch: 19 | Local Epoch: 3 | Generator Loss: 15.536646 | Discriminator Loss: 0.158600\n","| Global Epoch: 19 | Local Epoch: 4 | Generator Loss: 15.666893 | Discriminator Loss: 0.131770\n","| Global Epoch: 19 | Local Epoch: 5 | Generator Loss: 15.964295 | Discriminator Loss: 0.111168\n","| Global Epoch: 19 | Local Epoch: 6 | Generator Loss: 16.067461 | Discriminator Loss: 0.095823\n","| Global Epoch: 19 | Local Epoch: 7 | Generator Loss: 16.282610 | Discriminator Loss: 0.084351\n","| Global Epoch: 19 | Local Epoch: 8 | Generator Loss: 16.374109 | Discriminator Loss: 0.076308\n","| Global Epoch: 19 | Local Epoch: 9 | Generator Loss: 16.533401 | Discriminator Loss: 0.069329\n","After 20 epoch global training, averged local generator loss is: 16.7034, averged local discrimitor loss is: 0.1455\n","Averged validation loss is: 7.9426\n","| Global Epoch: 20 | Local Epoch: 0 | Generator Loss: 16.593228 | Discriminator Loss: 0.090914\n","| Global Epoch: 20 | Local Epoch: 1 | Generator Loss: 16.726078 | Discriminator Loss: 0.056325\n","| Global Epoch: 20 | Local Epoch: 2 | Generator Loss: 16.750118 | Discriminator Loss: 0.042901\n","| Global Epoch: 20 | Local Epoch: 3 | Generator Loss: 17.051751 | Discriminator Loss: 0.033121\n","| Global Epoch: 20 | Local Epoch: 4 | Generator Loss: 17.148351 | Discriminator Loss: 0.026825\n","| Global Epoch: 20 | Local Epoch: 5 | Generator Loss: 17.185008 | Discriminator Loss: 0.022575\n","| Global Epoch: 20 | Local Epoch: 6 | Generator Loss: 17.238759 | Discriminator Loss: 0.019527\n","| Global Epoch: 20 | Local Epoch: 7 | Generator Loss: 17.124067 | Discriminator Loss: 0.017227\n","| Global Epoch: 20 | Local Epoch: 8 | Generator Loss: 17.256909 | Discriminator Loss: 0.015397\n","| Global Epoch: 20 | Local Epoch: 9 | Generator Loss: 17.351901 | Discriminator Loss: 0.013970\n","| Global Epoch: 20 | Local Epoch: 0 | Generator Loss: 17.066894 | Discriminator Loss: 0.143696\n","| Global Epoch: 20 | Local Epoch: 1 | Generator Loss: 17.309397 | Discriminator Loss: 0.082751\n","| Global Epoch: 20 | Local Epoch: 2 | Generator Loss: 17.209208 | Discriminator Loss: 0.056828\n","| Global Epoch: 20 | Local Epoch: 3 | Generator Loss: 17.340038 | Discriminator Loss: 0.044029\n","| Global Epoch: 20 | Local Epoch: 4 | Generator Loss: 17.195643 | Discriminator Loss: 0.035884\n","| Global Epoch: 20 | Local Epoch: 5 | Generator Loss: 17.302142 | Discriminator Loss: 0.030141\n","| Global Epoch: 20 | Local Epoch: 6 | Generator Loss: 17.219604 | Discriminator Loss: 0.036016\n","| Global Epoch: 20 | Local Epoch: 7 | Generator Loss: 16.993225 | Discriminator Loss: 0.047681\n","| Global Epoch: 20 | Local Epoch: 8 | Generator Loss: 16.860640 | Discriminator Loss: 0.045447\n","| Global Epoch: 20 | Local Epoch: 9 | Generator Loss: 16.776012 | Discriminator Loss: 0.043430\n","| Global Epoch: 20 | Local Epoch: 0 | Generator Loss: 13.896500 | Discriminator Loss: 0.345696\n","| Global Epoch: 20 | Local Epoch: 1 | Generator Loss: 13.586574 | Discriminator Loss: 0.192104\n","| Global Epoch: 20 | Local Epoch: 2 | Generator Loss: 14.180274 | Discriminator Loss: 0.131066\n","| Global Epoch: 20 | Local Epoch: 3 | Generator Loss: 14.615820 | Discriminator Loss: 0.099936\n","| Global Epoch: 20 | Local Epoch: 4 | Generator Loss: 15.086522 | Discriminator Loss: 0.080792\n","| Global Epoch: 20 | Local Epoch: 5 | Generator Loss: 15.455790 | Discriminator Loss: 0.067637\n","| Global Epoch: 20 | Local Epoch: 6 | Generator Loss: 15.745213 | Discriminator Loss: 0.058202\n","| Global Epoch: 20 | Local Epoch: 7 | Generator Loss: 15.880699 | Discriminator Loss: 0.051110\n","| Global Epoch: 20 | Local Epoch: 8 | Generator Loss: 16.011804 | Discriminator Loss: 0.045512\n","| Global Epoch: 20 | Local Epoch: 9 | Generator Loss: 16.058558 | Discriminator Loss: 0.041022\n","| Global Epoch: 20 | Local Epoch: 0 | Generator Loss: 16.927306 | Discriminator Loss: 0.217667\n","| Global Epoch: 20 | Local Epoch: 1 | Generator Loss: 16.742334 | Discriminator Loss: 0.121823\n","| Global Epoch: 20 | Local Epoch: 2 | Generator Loss: 16.835031 | Discriminator Loss: 0.083623\n","| Global Epoch: 20 | Local Epoch: 3 | Generator Loss: 16.679278 | Discriminator Loss: 0.064185\n","| Global Epoch: 20 | Local Epoch: 4 | Generator Loss: 16.778464 | Discriminator Loss: 0.058443\n","| Global Epoch: 20 | Local Epoch: 5 | Generator Loss: 16.580807 | Discriminator Loss: 0.060910\n","| Global Epoch: 20 | Local Epoch: 6 | Generator Loss: 16.631767 | Discriminator Loss: 0.069637\n","| Global Epoch: 20 | Local Epoch: 7 | Generator Loss: 16.754122 | Discriminator Loss: 0.064475\n","| Global Epoch: 20 | Local Epoch: 8 | Generator Loss: 16.902181 | Discriminator Loss: 0.057947\n","| Global Epoch: 20 | Local Epoch: 9 | Generator Loss: 17.074550 | Discriminator Loss: 0.052430\n","| Global Epoch: 20 | Local Epoch: 0 | Generator Loss: 17.489017 | Discriminator Loss: 0.260836\n","| Global Epoch: 20 | Local Epoch: 1 | Generator Loss: 17.008048 | Discriminator Loss: 0.152432\n","| Global Epoch: 20 | Local Epoch: 2 | Generator Loss: 17.033335 | Discriminator Loss: 0.104946\n","| Global Epoch: 20 | Local Epoch: 3 | Generator Loss: 16.987136 | Discriminator Loss: 0.083281\n","| Global Epoch: 20 | Local Epoch: 4 | Generator Loss: 16.820610 | Discriminator Loss: 0.072702\n","| Global Epoch: 20 | Local Epoch: 5 | Generator Loss: 16.771788 | Discriminator Loss: 0.068316\n","| Global Epoch: 20 | Local Epoch: 6 | Generator Loss: 16.702892 | Discriminator Loss: 0.063236\n","| Global Epoch: 20 | Local Epoch: 7 | Generator Loss: 16.759812 | Discriminator Loss: 0.062428\n","| Global Epoch: 20 | Local Epoch: 8 | Generator Loss: 16.799203 | Discriminator Loss: 0.066556\n","| Global Epoch: 20 | Local Epoch: 9 | Generator Loss: 16.874692 | Discriminator Loss: 0.062082\n","After 21 epoch global training, averged local generator loss is: 16.7116, averged local discrimitor loss is: 0.1415\n","Averged validation loss is: 6.2384\n","| Global Epoch: 21 | Local Epoch: 0 | Generator Loss: 17.194212 | Discriminator Loss: 0.033226\n","| Global Epoch: 21 | Local Epoch: 1 | Generator Loss: 17.070488 | Discriminator Loss: 0.035042\n","| Global Epoch: 21 | Local Epoch: 2 | Generator Loss: 16.820233 | Discriminator Loss: 0.060058\n","| Global Epoch: 21 | Local Epoch: 3 | Generator Loss: 16.675467 | Discriminator Loss: 0.046649\n","| Global Epoch: 21 | Local Epoch: 4 | Generator Loss: 16.870913 | Discriminator Loss: 0.041005\n","| Global Epoch: 21 | Local Epoch: 5 | Generator Loss: 16.705174 | Discriminator Loss: 0.050730\n","| Global Epoch: 21 | Local Epoch: 6 | Generator Loss: 16.567301 | Discriminator Loss: 0.044439\n","| Global Epoch: 21 | Local Epoch: 7 | Generator Loss: 16.484744 | Discriminator Loss: 0.039591\n","| Global Epoch: 21 | Local Epoch: 8 | Generator Loss: 16.600479 | Discriminator Loss: 0.037021\n","| Global Epoch: 21 | Local Epoch: 9 | Generator Loss: 16.691188 | Discriminator Loss: 0.033657\n","| Global Epoch: 21 | Local Epoch: 0 | Generator Loss: 17.002590 | Discriminator Loss: 0.473746\n","| Global Epoch: 21 | Local Epoch: 1 | Generator Loss: 15.777384 | Discriminator Loss: 0.285687\n","| Global Epoch: 21 | Local Epoch: 2 | Generator Loss: 15.586859 | Discriminator Loss: 0.196346\n","| Global Epoch: 21 | Local Epoch: 3 | Generator Loss: 15.582111 | Discriminator Loss: 0.149329\n","| Global Epoch: 21 | Local Epoch: 4 | Generator Loss: 15.766436 | Discriminator Loss: 0.120751\n","| Global Epoch: 21 | Local Epoch: 5 | Generator Loss: 15.798693 | Discriminator Loss: 0.112247\n","| Global Epoch: 21 | Local Epoch: 6 | Generator Loss: 15.392571 | Discriminator Loss: 0.136456\n","| Global Epoch: 21 | Local Epoch: 7 | Generator Loss: 15.392152 | Discriminator Loss: 0.125379\n","| Global Epoch: 21 | Local Epoch: 8 | Generator Loss: 15.447777 | Discriminator Loss: 0.112938\n","| Global Epoch: 21 | Local Epoch: 9 | Generator Loss: 15.501217 | Discriminator Loss: 0.104433\n","| Global Epoch: 21 | Local Epoch: 0 | Generator Loss: 15.973443 | Discriminator Loss: 0.463534\n","| Global Epoch: 21 | Local Epoch: 1 | Generator Loss: 15.438628 | Discriminator Loss: 0.259460\n","| Global Epoch: 21 | Local Epoch: 2 | Generator Loss: 15.441571 | Discriminator Loss: 0.178222\n","| Global Epoch: 21 | Local Epoch: 3 | Generator Loss: 15.549840 | Discriminator Loss: 0.136431\n","| Global Epoch: 21 | Local Epoch: 4 | Generator Loss: 15.673297 | Discriminator Loss: 0.110794\n","| Global Epoch: 21 | Local Epoch: 5 | Generator Loss: 15.471594 | Discriminator Loss: 0.098246\n","| Global Epoch: 21 | Local Epoch: 6 | Generator Loss: 15.481619 | Discriminator Loss: 0.085728\n","| Global Epoch: 21 | Local Epoch: 7 | Generator Loss: 15.661904 | Discriminator Loss: 0.076196\n","| Global Epoch: 21 | Local Epoch: 8 | Generator Loss: 15.731805 | Discriminator Loss: 0.068102\n","| Global Epoch: 21 | Local Epoch: 9 | Generator Loss: 15.895426 | Discriminator Loss: 0.061534\n","| Global Epoch: 21 | Local Epoch: 0 | Generator Loss: 15.860493 | Discriminator Loss: 0.165952\n","| Global Epoch: 21 | Local Epoch: 1 | Generator Loss: 15.300358 | Discriminator Loss: 0.096579\n","| Global Epoch: 21 | Local Epoch: 2 | Generator Loss: 15.503677 | Discriminator Loss: 0.067564\n","| Global Epoch: 21 | Local Epoch: 3 | Generator Loss: 15.792707 | Discriminator Loss: 0.053096\n","| Global Epoch: 21 | Local Epoch: 4 | Generator Loss: 16.251313 | Discriminator Loss: 0.043210\n","| Global Epoch: 21 | Local Epoch: 5 | Generator Loss: 16.505928 | Discriminator Loss: 0.036311\n","| Global Epoch: 21 | Local Epoch: 6 | Generator Loss: 16.691870 | Discriminator Loss: 0.032405\n","| Global Epoch: 21 | Local Epoch: 7 | Generator Loss: 16.433381 | Discriminator Loss: 0.069604\n","| Global Epoch: 21 | Local Epoch: 8 | Generator Loss: 16.454397 | Discriminator Loss: 0.069356\n","| Global Epoch: 21 | Local Epoch: 9 | Generator Loss: 16.438682 | Discriminator Loss: 0.065178\n","| Global Epoch: 21 | Local Epoch: 0 | Generator Loss: 16.720688 | Discriminator Loss: 0.712163\n","| Global Epoch: 21 | Local Epoch: 1 | Generator Loss: 16.055504 | Discriminator Loss: 0.411533\n","| Global Epoch: 21 | Local Epoch: 2 | Generator Loss: 15.833314 | Discriminator Loss: 0.284510\n","| Global Epoch: 21 | Local Epoch: 3 | Generator Loss: 15.754218 | Discriminator Loss: 0.220239\n","| Global Epoch: 21 | Local Epoch: 4 | Generator Loss: 15.604864 | Discriminator Loss: 0.178482\n","| Global Epoch: 21 | Local Epoch: 5 | Generator Loss: 15.658613 | Discriminator Loss: 0.149953\n","| Global Epoch: 21 | Local Epoch: 6 | Generator Loss: 15.757406 | Discriminator Loss: 0.129353\n","| Global Epoch: 21 | Local Epoch: 7 | Generator Loss: 15.769673 | Discriminator Loss: 0.113769\n","| Global Epoch: 21 | Local Epoch: 8 | Generator Loss: 15.794735 | Discriminator Loss: 0.101548\n","| Global Epoch: 21 | Local Epoch: 9 | Generator Loss: 15.895967 | Discriminator Loss: 0.091807\n","After 22 epoch global training, averged local generator loss is: 16.6745, averged local discrimitor loss is: 0.1393\n","Averged validation loss is: 7.0545\n","| Global Epoch: 22 | Local Epoch: 0 | Generator Loss: 15.634178 | Discriminator Loss: 0.481159\n","| Global Epoch: 22 | Local Epoch: 1 | Generator Loss: 14.809991 | Discriminator Loss: 0.276669\n","| Global Epoch: 22 | Local Epoch: 2 | Generator Loss: 14.776607 | Discriminator Loss: 0.191542\n","| Global Epoch: 22 | Local Epoch: 3 | Generator Loss: 15.036825 | Discriminator Loss: 0.145932\n","| Global Epoch: 22 | Local Epoch: 4 | Generator Loss: 15.237719 | Discriminator Loss: 0.118027\n","| Global Epoch: 22 | Local Epoch: 5 | Generator Loss: 15.254679 | Discriminator Loss: 0.099098\n","| Global Epoch: 22 | Local Epoch: 6 | Generator Loss: 15.244446 | Discriminator Loss: 0.085428\n","| Global Epoch: 22 | Local Epoch: 7 | Generator Loss: 15.262776 | Discriminator Loss: 0.075118\n","| Global Epoch: 22 | Local Epoch: 8 | Generator Loss: 15.308652 | Discriminator Loss: 0.067046\n","| Global Epoch: 22 | Local Epoch: 9 | Generator Loss: 15.405479 | Discriminator Loss: 0.060565\n","| Global Epoch: 22 | Local Epoch: 0 | Generator Loss: 15.964416 | Discriminator Loss: 0.235339\n","| Global Epoch: 22 | Local Epoch: 1 | Generator Loss: 15.438949 | Discriminator Loss: 0.126579\n","| Global Epoch: 22 | Local Epoch: 2 | Generator Loss: 15.368541 | Discriminator Loss: 0.086619\n","| Global Epoch: 22 | Local Epoch: 3 | Generator Loss: 15.406510 | Discriminator Loss: 0.065923\n","| Global Epoch: 22 | Local Epoch: 4 | Generator Loss: 15.384670 | Discriminator Loss: 0.053191\n","| Global Epoch: 22 | Local Epoch: 5 | Generator Loss: 15.356268 | Discriminator Loss: 0.044528\n","| Global Epoch: 22 | Local Epoch: 6 | Generator Loss: 15.410705 | Discriminator Loss: 0.038302\n","| Global Epoch: 22 | Local Epoch: 7 | Generator Loss: 15.433627 | Discriminator Loss: 0.033655\n","| Global Epoch: 22 | Local Epoch: 8 | Generator Loss: 15.422299 | Discriminator Loss: 0.029980\n","| Global Epoch: 22 | Local Epoch: 9 | Generator Loss: 15.489138 | Discriminator Loss: 0.027035\n","| Global Epoch: 22 | Local Epoch: 0 | Generator Loss: 16.695965 | Discriminator Loss: 0.602550\n","| Global Epoch: 22 | Local Epoch: 1 | Generator Loss: 15.518191 | Discriminator Loss: 0.340227\n","| Global Epoch: 22 | Local Epoch: 2 | Generator Loss: 15.137554 | Discriminator Loss: 0.232949\n","| Global Epoch: 22 | Local Epoch: 3 | Generator Loss: 15.035705 | Discriminator Loss: 0.176945\n","| Global Epoch: 22 | Local Epoch: 4 | Generator Loss: 15.126173 | Discriminator Loss: 0.142710\n","| Global Epoch: 22 | Local Epoch: 5 | Generator Loss: 15.040928 | Discriminator Loss: 0.119682\n","| Global Epoch: 22 | Local Epoch: 6 | Generator Loss: 15.001113 | Discriminator Loss: 0.103108\n","| Global Epoch: 22 | Local Epoch: 7 | Generator Loss: 15.049260 | Discriminator Loss: 0.090631\n","| Global Epoch: 22 | Local Epoch: 8 | Generator Loss: 15.156106 | Discriminator Loss: 0.080888\n","| Global Epoch: 22 | Local Epoch: 9 | Generator Loss: 15.241481 | Discriminator Loss: 0.073038\n","| Global Epoch: 22 | Local Epoch: 0 | Generator Loss: 15.885958 | Discriminator Loss: 0.162496\n","| Global Epoch: 22 | Local Epoch: 1 | Generator Loss: 15.882730 | Discriminator Loss: 0.106590\n","| Global Epoch: 22 | Local Epoch: 2 | Generator Loss: 15.641139 | Discriminator Loss: 0.074080\n","| Global Epoch: 22 | Local Epoch: 3 | Generator Loss: 15.673487 | Discriminator Loss: 0.086741\n","| Global Epoch: 22 | Local Epoch: 4 | Generator Loss: 15.456406 | Discriminator Loss: 0.095922\n","| Global Epoch: 22 | Local Epoch: 5 | Generator Loss: 15.469288 | Discriminator Loss: 0.084417\n","| Global Epoch: 22 | Local Epoch: 6 | Generator Loss: 15.288333 | Discriminator Loss: 0.094480\n","| Global Epoch: 22 | Local Epoch: 7 | Generator Loss: 15.264793 | Discriminator Loss: 0.091354\n","| Global Epoch: 22 | Local Epoch: 8 | Generator Loss: 15.292119 | Discriminator Loss: 0.081979\n","| Global Epoch: 22 | Local Epoch: 9 | Generator Loss: 15.338112 | Discriminator Loss: 0.081541\n","| Global Epoch: 22 | Local Epoch: 0 | Generator Loss: 15.611798 | Discriminator Loss: 0.277560\n","| Global Epoch: 22 | Local Epoch: 1 | Generator Loss: 14.989999 | Discriminator Loss: 0.156740\n","| Global Epoch: 22 | Local Epoch: 2 | Generator Loss: 15.527611 | Discriminator Loss: 0.108326\n","| Global Epoch: 22 | Local Epoch: 3 | Generator Loss: 15.470682 | Discriminator Loss: 0.082518\n","| Global Epoch: 22 | Local Epoch: 4 | Generator Loss: 15.496295 | Discriminator Loss: 0.067020\n","| Global Epoch: 22 | Local Epoch: 5 | Generator Loss: 15.584978 | Discriminator Loss: 0.056330\n","| Global Epoch: 22 | Local Epoch: 6 | Generator Loss: 15.614637 | Discriminator Loss: 0.048597\n","| Global Epoch: 22 | Local Epoch: 7 | Generator Loss: 15.722735 | Discriminator Loss: 0.042788\n","| Global Epoch: 22 | Local Epoch: 8 | Generator Loss: 15.894300 | Discriminator Loss: 0.038703\n","| Global Epoch: 22 | Local Epoch: 9 | Generator Loss: 15.816169 | Discriminator Loss: 0.044030\n","After 23 epoch global training, averged local generator loss is: 16.6372, averged local discrimitor loss is: 0.1351\n","Averged validation loss is: 9.7272\n","| Global Epoch: 23 | Local Epoch: 0 | Generator Loss: 16.995407 | Discriminator Loss: 0.328697\n","| Global Epoch: 23 | Local Epoch: 1 | Generator Loss: 16.553177 | Discriminator Loss: 0.182287\n","| Global Epoch: 23 | Local Epoch: 2 | Generator Loss: 16.328260 | Discriminator Loss: 0.124741\n","| Global Epoch: 23 | Local Epoch: 3 | Generator Loss: 16.110395 | Discriminator Loss: 0.098745\n","| Global Epoch: 23 | Local Epoch: 4 | Generator Loss: 16.340568 | Discriminator Loss: 0.081798\n","| Global Epoch: 23 | Local Epoch: 5 | Generator Loss: 16.562084 | Discriminator Loss: 0.068895\n","| Global Epoch: 23 | Local Epoch: 6 | Generator Loss: 16.683287 | Discriminator Loss: 0.059422\n","| Global Epoch: 23 | Local Epoch: 7 | Generator Loss: 16.725169 | Discriminator Loss: 0.052251\n","| Global Epoch: 23 | Local Epoch: 8 | Generator Loss: 16.756078 | Discriminator Loss: 0.046637\n","| Global Epoch: 23 | Local Epoch: 9 | Generator Loss: 16.779491 | Discriminator Loss: 0.042126\n","| Global Epoch: 23 | Local Epoch: 0 | Generator Loss: 16.307246 | Discriminator Loss: 0.090158\n","| Global Epoch: 23 | Local Epoch: 1 | Generator Loss: 16.018952 | Discriminator Loss: 0.099673\n","| Global Epoch: 23 | Local Epoch: 2 | Generator Loss: 16.075558 | Discriminator Loss: 0.069405\n","| Global Epoch: 23 | Local Epoch: 3 | Generator Loss: 16.052062 | Discriminator Loss: 0.053267\n","| Global Epoch: 23 | Local Epoch: 4 | Generator Loss: 15.986121 | Discriminator Loss: 0.053190\n","| Global Epoch: 23 | Local Epoch: 5 | Generator Loss: 15.876850 | Discriminator Loss: 0.056110\n","| Global Epoch: 23 | Local Epoch: 6 | Generator Loss: 15.846615 | Discriminator Loss: 0.049066\n","| Global Epoch: 23 | Local Epoch: 7 | Generator Loss: 15.828843 | Discriminator Loss: 0.043808\n","| Global Epoch: 23 | Local Epoch: 8 | Generator Loss: 15.905570 | Discriminator Loss: 0.039680\n","| Global Epoch: 23 | Local Epoch: 9 | Generator Loss: 15.864808 | Discriminator Loss: 0.038897\n","| Global Epoch: 23 | Local Epoch: 0 | Generator Loss: 16.181836 | Discriminator Loss: 0.360358\n","| Global Epoch: 23 | Local Epoch: 1 | Generator Loss: 15.888939 | Discriminator Loss: 0.202938\n","| Global Epoch: 23 | Local Epoch: 2 | Generator Loss: 16.132269 | Discriminator Loss: 0.139413\n","| Global Epoch: 23 | Local Epoch: 3 | Generator Loss: 16.170648 | Discriminator Loss: 0.106151\n","| Global Epoch: 23 | Local Epoch: 4 | Generator Loss: 16.186497 | Discriminator Loss: 0.085785\n","| Global Epoch: 23 | Local Epoch: 5 | Generator Loss: 16.149047 | Discriminator Loss: 0.072057\n","| Global Epoch: 23 | Local Epoch: 6 | Generator Loss: 16.274900 | Discriminator Loss: 0.062190\n","| Global Epoch: 23 | Local Epoch: 7 | Generator Loss: 16.418095 | Discriminator Loss: 0.054762\n","| Global Epoch: 23 | Local Epoch: 8 | Generator Loss: 16.398436 | Discriminator Loss: 0.050050\n","| Global Epoch: 23 | Local Epoch: 9 | Generator Loss: 16.247599 | Discriminator Loss: 0.054333\n","| Global Epoch: 23 | Local Epoch: 0 | Generator Loss: 15.862634 | Discriminator Loss: 0.222608\n","| Global Epoch: 23 | Local Epoch: 1 | Generator Loss: 15.980289 | Discriminator Loss: 0.122630\n","| Global Epoch: 23 | Local Epoch: 2 | Generator Loss: 16.179503 | Discriminator Loss: 0.089070\n","| Global Epoch: 23 | Local Epoch: 3 | Generator Loss: 16.198062 | Discriminator Loss: 0.068351\n","| Global Epoch: 23 | Local Epoch: 4 | Generator Loss: 16.247768 | Discriminator Loss: 0.055351\n","| Global Epoch: 23 | Local Epoch: 5 | Generator Loss: 16.333916 | Discriminator Loss: 0.046454\n","| Global Epoch: 23 | Local Epoch: 6 | Generator Loss: 16.363183 | Discriminator Loss: 0.040048\n","| Global Epoch: 23 | Local Epoch: 7 | Generator Loss: 16.511943 | Discriminator Loss: 0.035434\n","| Global Epoch: 23 | Local Epoch: 8 | Generator Loss: 16.390377 | Discriminator Loss: 0.046543\n","| Global Epoch: 23 | Local Epoch: 9 | Generator Loss: 16.328806 | Discriminator Loss: 0.044894\n","| Global Epoch: 23 | Local Epoch: 0 | Generator Loss: 16.466182 | Discriminator Loss: 0.746595\n","| Global Epoch: 23 | Local Epoch: 1 | Generator Loss: 15.578657 | Discriminator Loss: 0.421454\n","| Global Epoch: 23 | Local Epoch: 2 | Generator Loss: 15.389307 | Discriminator Loss: 0.289048\n","| Global Epoch: 23 | Local Epoch: 3 | Generator Loss: 15.538991 | Discriminator Loss: 0.224189\n","| Global Epoch: 23 | Local Epoch: 4 | Generator Loss: 15.314212 | Discriminator Loss: 0.182223\n","| Global Epoch: 23 | Local Epoch: 5 | Generator Loss: 15.288874 | Discriminator Loss: 0.152933\n","| Global Epoch: 23 | Local Epoch: 6 | Generator Loss: 15.305779 | Discriminator Loss: 0.131769\n","| Global Epoch: 23 | Local Epoch: 7 | Generator Loss: 15.366814 | Discriminator Loss: 0.115776\n","| Global Epoch: 23 | Local Epoch: 8 | Generator Loss: 15.395294 | Discriminator Loss: 0.103274\n","| Global Epoch: 23 | Local Epoch: 9 | Generator Loss: 15.598692 | Discriminator Loss: 0.093415\n","After 24 epoch global training, averged local generator loss is: 16.5939, averged local discrimitor loss is: 0.1334\n","Averged validation loss is: 7.6111\n","| Global Epoch: 24 | Local Epoch: 0 | Generator Loss: 16.293302 | Discriminator Loss: 0.177263\n","| Global Epoch: 24 | Local Epoch: 1 | Generator Loss: 16.071249 | Discriminator Loss: 0.099428\n","| Global Epoch: 24 | Local Epoch: 2 | Generator Loss: 16.306838 | Discriminator Loss: 0.068289\n","| Global Epoch: 24 | Local Epoch: 3 | Generator Loss: 16.239572 | Discriminator Loss: 0.051916\n","| Global Epoch: 24 | Local Epoch: 4 | Generator Loss: 16.327692 | Discriminator Loss: 0.041953\n","| Global Epoch: 24 | Local Epoch: 5 | Generator Loss: 16.427100 | Discriminator Loss: 0.036121\n","| Global Epoch: 24 | Local Epoch: 6 | Generator Loss: 16.537376 | Discriminator Loss: 0.032630\n","| Global Epoch: 24 | Local Epoch: 7 | Generator Loss: 16.589573 | Discriminator Loss: 0.030081\n","| Global Epoch: 24 | Local Epoch: 8 | Generator Loss: 16.653868 | Discriminator Loss: 0.028623\n","| Global Epoch: 24 | Local Epoch: 9 | Generator Loss: 16.589459 | Discriminator Loss: 0.026273\n","| Global Epoch: 24 | Local Epoch: 0 | Generator Loss: 16.361700 | Discriminator Loss: 0.284355\n","| Global Epoch: 24 | Local Epoch: 1 | Generator Loss: 16.097084 | Discriminator Loss: 0.148913\n","| Global Epoch: 24 | Local Epoch: 2 | Generator Loss: 15.940004 | Discriminator Loss: 0.101526\n","| Global Epoch: 24 | Local Epoch: 3 | Generator Loss: 16.019431 | Discriminator Loss: 0.076739\n","| Global Epoch: 24 | Local Epoch: 4 | Generator Loss: 16.078342 | Discriminator Loss: 0.061609\n","| Global Epoch: 24 | Local Epoch: 5 | Generator Loss: 16.049810 | Discriminator Loss: 0.051502\n","| Global Epoch: 24 | Local Epoch: 6 | Generator Loss: 16.100465 | Discriminator Loss: 0.044310\n","| Global Epoch: 24 | Local Epoch: 7 | Generator Loss: 16.185979 | Discriminator Loss: 0.038873\n","| Global Epoch: 24 | Local Epoch: 8 | Generator Loss: 16.259697 | Discriminator Loss: 0.034669\n","| Global Epoch: 24 | Local Epoch: 9 | Generator Loss: 16.366907 | Discriminator Loss: 0.031254\n","| Global Epoch: 24 | Local Epoch: 0 | Generator Loss: 16.899561 | Discriminator Loss: 0.300921\n","| Global Epoch: 24 | Local Epoch: 1 | Generator Loss: 16.360333 | Discriminator Loss: 0.185682\n","| Global Epoch: 24 | Local Epoch: 2 | Generator Loss: 16.441320 | Discriminator Loss: 0.128534\n","| Global Epoch: 24 | Local Epoch: 3 | Generator Loss: 16.521001 | Discriminator Loss: 0.097650\n","| Global Epoch: 24 | Local Epoch: 4 | Generator Loss: 16.492295 | Discriminator Loss: 0.078731\n","| Global Epoch: 24 | Local Epoch: 5 | Generator Loss: 16.516888 | Discriminator Loss: 0.066021\n","| Global Epoch: 24 | Local Epoch: 6 | Generator Loss: 16.663976 | Discriminator Loss: 0.056934\n","| Global Epoch: 24 | Local Epoch: 7 | Generator Loss: 16.749246 | Discriminator Loss: 0.050059\n","| Global Epoch: 24 | Local Epoch: 8 | Generator Loss: 16.753419 | Discriminator Loss: 0.046586\n","| Global Epoch: 24 | Local Epoch: 9 | Generator Loss: 16.671259 | Discriminator Loss: 0.042276\n","| Global Epoch: 24 | Local Epoch: 0 | Generator Loss: 18.107645 | Discriminator Loss: 0.173307\n","| Global Epoch: 24 | Local Epoch: 1 | Generator Loss: 17.384977 | Discriminator Loss: 0.095180\n","| Global Epoch: 24 | Local Epoch: 2 | Generator Loss: 17.251261 | Discriminator Loss: 0.065311\n","| Global Epoch: 24 | Local Epoch: 3 | Generator Loss: 17.249178 | Discriminator Loss: 0.049824\n","| Global Epoch: 24 | Local Epoch: 4 | Generator Loss: 17.266861 | Discriminator Loss: 0.040323\n","| Global Epoch: 24 | Local Epoch: 5 | Generator Loss: 17.259832 | Discriminator Loss: 0.033885\n","| Global Epoch: 24 | Local Epoch: 6 | Generator Loss: 17.378620 | Discriminator Loss: 0.029389\n","| Global Epoch: 24 | Local Epoch: 7 | Generator Loss: 17.398553 | Discriminator Loss: 0.025902\n","| Global Epoch: 24 | Local Epoch: 8 | Generator Loss: 17.387988 | Discriminator Loss: 0.023164\n","| Global Epoch: 24 | Local Epoch: 9 | Generator Loss: 17.402046 | Discriminator Loss: 0.020955\n","| Global Epoch: 24 | Local Epoch: 0 | Generator Loss: 16.284295 | Discriminator Loss: 0.080761\n","| Global Epoch: 24 | Local Epoch: 1 | Generator Loss: 16.042186 | Discriminator Loss: 0.080303\n","| Global Epoch: 24 | Local Epoch: 2 | Generator Loss: 16.172906 | Discriminator Loss: 0.058379\n","| Global Epoch: 24 | Local Epoch: 3 | Generator Loss: 16.329705 | Discriminator Loss: 0.044583\n","| Global Epoch: 24 | Local Epoch: 4 | Generator Loss: 16.512909 | Discriminator Loss: 0.036054\n","| Global Epoch: 24 | Local Epoch: 5 | Generator Loss: 16.593182 | Discriminator Loss: 0.030279\n","| Global Epoch: 24 | Local Epoch: 6 | Generator Loss: 16.721736 | Discriminator Loss: 0.026167\n","| Global Epoch: 24 | Local Epoch: 7 | Generator Loss: 16.806562 | Discriminator Loss: 0.023084\n","| Global Epoch: 24 | Local Epoch: 8 | Generator Loss: 16.885536 | Discriminator Loss: 0.021624\n","| Global Epoch: 24 | Local Epoch: 9 | Generator Loss: 16.887156 | Discriminator Loss: 0.020742\n","After 25 epoch global training, averged local generator loss is: 16.6056, averged local discrimitor loss is: 0.1289\n","Averged validation loss is: 8.0641\n","| Global Epoch: 25 | Local Epoch: 0 | Generator Loss: 16.476173 | Discriminator Loss: 0.098718\n","| Global Epoch: 25 | Local Epoch: 1 | Generator Loss: 16.252932 | Discriminator Loss: 0.055574\n","| Global Epoch: 25 | Local Epoch: 2 | Generator Loss: 16.424446 | Discriminator Loss: 0.038304\n","| Global Epoch: 25 | Local Epoch: 3 | Generator Loss: 16.260824 | Discriminator Loss: 0.046279\n","| Global Epoch: 25 | Local Epoch: 4 | Generator Loss: 16.079806 | Discriminator Loss: 0.041710\n","| Global Epoch: 25 | Local Epoch: 5 | Generator Loss: 16.077001 | Discriminator Loss: 0.035555\n","| Global Epoch: 25 | Local Epoch: 6 | Generator Loss: 16.104970 | Discriminator Loss: 0.030746\n","| Global Epoch: 25 | Local Epoch: 7 | Generator Loss: 16.154263 | Discriminator Loss: 0.027102\n","| Global Epoch: 25 | Local Epoch: 8 | Generator Loss: 16.215094 | Discriminator Loss: 0.024212\n","| Global Epoch: 25 | Local Epoch: 9 | Generator Loss: 16.258408 | Discriminator Loss: 0.021877\n","| Global Epoch: 25 | Local Epoch: 0 | Generator Loss: 15.460803 | Discriminator Loss: 0.150654\n","| Global Epoch: 25 | Local Epoch: 1 | Generator Loss: 15.547350 | Discriminator Loss: 0.098594\n","| Global Epoch: 25 | Local Epoch: 2 | Generator Loss: 15.575056 | Discriminator Loss: 0.075200\n","| Global Epoch: 25 | Local Epoch: 3 | Generator Loss: 15.747631 | Discriminator Loss: 0.058097\n","| Global Epoch: 25 | Local Epoch: 4 | Generator Loss: 15.904419 | Discriminator Loss: 0.047819\n","| Global Epoch: 25 | Local Epoch: 5 | Generator Loss: 15.897114 | Discriminator Loss: 0.041934\n","| Global Epoch: 25 | Local Epoch: 6 | Generator Loss: 15.907130 | Discriminator Loss: 0.036354\n","| Global Epoch: 25 | Local Epoch: 7 | Generator Loss: 15.961804 | Discriminator Loss: 0.035292\n","| Global Epoch: 25 | Local Epoch: 8 | Generator Loss: 15.742120 | Discriminator Loss: 0.051411\n","| Global Epoch: 25 | Local Epoch: 9 | Generator Loss: 15.665873 | Discriminator Loss: 0.048803\n","| Global Epoch: 25 | Local Epoch: 0 | Generator Loss: 14.588181 | Discriminator Loss: 0.536480\n","| Global Epoch: 25 | Local Epoch: 1 | Generator Loss: 14.404671 | Discriminator Loss: 0.314665\n","| Global Epoch: 25 | Local Epoch: 2 | Generator Loss: 14.715395 | Discriminator Loss: 0.218232\n","| Global Epoch: 25 | Local Epoch: 3 | Generator Loss: 14.583172 | Discriminator Loss: 0.169704\n","| Global Epoch: 25 | Local Epoch: 4 | Generator Loss: 14.527978 | Discriminator Loss: 0.137566\n","| Global Epoch: 25 | Local Epoch: 5 | Generator Loss: 14.759437 | Discriminator Loss: 0.115579\n","| Global Epoch: 25 | Local Epoch: 6 | Generator Loss: 15.025602 | Discriminator Loss: 0.100361\n","| Global Epoch: 25 | Local Epoch: 7 | Generator Loss: 15.048363 | Discriminator Loss: 0.089188\n","| Global Epoch: 25 | Local Epoch: 8 | Generator Loss: 15.068443 | Discriminator Loss: 0.080638\n","| Global Epoch: 25 | Local Epoch: 9 | Generator Loss: 15.191318 | Discriminator Loss: 0.073121\n","| Global Epoch: 25 | Local Epoch: 0 | Generator Loss: 15.779749 | Discriminator Loss: 0.183336\n","| Global Epoch: 25 | Local Epoch: 1 | Generator Loss: 16.050535 | Discriminator Loss: 0.110377\n","| Global Epoch: 25 | Local Epoch: 2 | Generator Loss: 16.011638 | Discriminator Loss: 0.080774\n","| Global Epoch: 25 | Local Epoch: 3 | Generator Loss: 16.103397 | Discriminator Loss: 0.062307\n","| Global Epoch: 25 | Local Epoch: 4 | Generator Loss: 16.176831 | Discriminator Loss: 0.050386\n","| Global Epoch: 25 | Local Epoch: 5 | Generator Loss: 16.260957 | Discriminator Loss: 0.042653\n","| Global Epoch: 25 | Local Epoch: 6 | Generator Loss: 16.274719 | Discriminator Loss: 0.036894\n","| Global Epoch: 25 | Local Epoch: 7 | Generator Loss: 16.428786 | Discriminator Loss: 0.032563\n","| Global Epoch: 25 | Local Epoch: 8 | Generator Loss: 16.431952 | Discriminator Loss: 0.029110\n","| Global Epoch: 25 | Local Epoch: 9 | Generator Loss: 16.411753 | Discriminator Loss: 0.026303\n","| Global Epoch: 25 | Local Epoch: 0 | Generator Loss: 16.304543 | Discriminator Loss: 0.089774\n","| Global Epoch: 25 | Local Epoch: 1 | Generator Loss: 16.029865 | Discriminator Loss: 0.054399\n","| Global Epoch: 25 | Local Epoch: 2 | Generator Loss: 15.889460 | Discriminator Loss: 0.039061\n","| Global Epoch: 25 | Local Epoch: 3 | Generator Loss: 15.696063 | Discriminator Loss: 0.042652\n","| Global Epoch: 25 | Local Epoch: 4 | Generator Loss: 15.593199 | Discriminator Loss: 0.042788\n","| Global Epoch: 25 | Local Epoch: 5 | Generator Loss: 15.518217 | Discriminator Loss: 0.036551\n","| Global Epoch: 25 | Local Epoch: 6 | Generator Loss: 15.664416 | Discriminator Loss: 0.031599\n","| Global Epoch: 25 | Local Epoch: 7 | Generator Loss: 15.737227 | Discriminator Loss: 0.027785\n","| Global Epoch: 25 | Local Epoch: 8 | Generator Loss: 15.777824 | Discriminator Loss: 0.024812\n","| Global Epoch: 25 | Local Epoch: 9 | Generator Loss: 15.840482 | Discriminator Loss: 0.022697\n","After 26 epoch global training, averged local generator loss is: 16.5762, averged local discrimitor loss is: 0.1248\n","Averged validation loss is: 7.5396\n","| Global Epoch: 26 | Local Epoch: 0 | Generator Loss: 15.060482 | Discriminator Loss: 0.077199\n","| Global Epoch: 26 | Local Epoch: 1 | Generator Loss: 15.340771 | Discriminator Loss: 0.052228\n","| Global Epoch: 26 | Local Epoch: 2 | Generator Loss: 15.436859 | Discriminator Loss: 0.061093\n","| Global Epoch: 26 | Local Epoch: 3 | Generator Loss: 15.740786 | Discriminator Loss: 0.050563\n","| Global Epoch: 26 | Local Epoch: 4 | Generator Loss: 15.664650 | Discriminator Loss: 0.061751\n","| Global Epoch: 26 | Local Epoch: 5 | Generator Loss: 15.763423 | Discriminator Loss: 0.061858\n","| Global Epoch: 26 | Local Epoch: 6 | Generator Loss: 15.872429 | Discriminator Loss: 0.054286\n","| Global Epoch: 26 | Local Epoch: 7 | Generator Loss: 15.993464 | Discriminator Loss: 0.048095\n","| Global Epoch: 26 | Local Epoch: 8 | Generator Loss: 15.940124 | Discriminator Loss: 0.043020\n","| Global Epoch: 26 | Local Epoch: 9 | Generator Loss: 16.021780 | Discriminator Loss: 0.039859\n","| Global Epoch: 26 | Local Epoch: 0 | Generator Loss: 16.232012 | Discriminator Loss: 0.219211\n","| Global Epoch: 26 | Local Epoch: 1 | Generator Loss: 16.491876 | Discriminator Loss: 0.122827\n","| Global Epoch: 26 | Local Epoch: 2 | Generator Loss: 16.756167 | Discriminator Loss: 0.084096\n","| Global Epoch: 26 | Local Epoch: 3 | Generator Loss: 17.004473 | Discriminator Loss: 0.063929\n","| Global Epoch: 26 | Local Epoch: 4 | Generator Loss: 17.180121 | Discriminator Loss: 0.051749\n","| Global Epoch: 26 | Local Epoch: 5 | Generator Loss: 17.196250 | Discriminator Loss: 0.043449\n","| Global Epoch: 26 | Local Epoch: 6 | Generator Loss: 17.160945 | Discriminator Loss: 0.037449\n","| Global Epoch: 26 | Local Epoch: 7 | Generator Loss: 17.118071 | Discriminator Loss: 0.032928\n","| Global Epoch: 26 | Local Epoch: 8 | Generator Loss: 17.145683 | Discriminator Loss: 0.029613\n","| Global Epoch: 26 | Local Epoch: 9 | Generator Loss: 17.092968 | Discriminator Loss: 0.027246\n","| Global Epoch: 26 | Local Epoch: 0 | Generator Loss: 16.671020 | Discriminator Loss: 0.052083\n","| Global Epoch: 26 | Local Epoch: 1 | Generator Loss: 17.116185 | Discriminator Loss: 0.033676\n","| Global Epoch: 26 | Local Epoch: 2 | Generator Loss: 16.632374 | Discriminator Loss: 0.062203\n","| Global Epoch: 26 | Local Epoch: 3 | Generator Loss: 16.845451 | Discriminator Loss: 0.050723\n","| Global Epoch: 26 | Local Epoch: 4 | Generator Loss: 17.050746 | Discriminator Loss: 0.041423\n","| Global Epoch: 26 | Local Epoch: 5 | Generator Loss: 17.160298 | Discriminator Loss: 0.035008\n","| Global Epoch: 26 | Local Epoch: 6 | Generator Loss: 17.282881 | Discriminator Loss: 0.030985\n","| Global Epoch: 26 | Local Epoch: 7 | Generator Loss: 17.228657 | Discriminator Loss: 0.032794\n","| Global Epoch: 26 | Local Epoch: 8 | Generator Loss: 17.282180 | Discriminator Loss: 0.029834\n","| Global Epoch: 26 | Local Epoch: 9 | Generator Loss: 17.387842 | Discriminator Loss: 0.027074\n","| Global Epoch: 26 | Local Epoch: 0 | Generator Loss: 17.441636 | Discriminator Loss: 0.424555\n","| Global Epoch: 26 | Local Epoch: 1 | Generator Loss: 16.748839 | Discriminator Loss: 0.238678\n","| Global Epoch: 26 | Local Epoch: 2 | Generator Loss: 16.686498 | Discriminator Loss: 0.164087\n","| Global Epoch: 26 | Local Epoch: 3 | Generator Loss: 16.749396 | Discriminator Loss: 0.124774\n","| Global Epoch: 26 | Local Epoch: 4 | Generator Loss: 16.996747 | Discriminator Loss: 0.100911\n","| Global Epoch: 26 | Local Epoch: 5 | Generator Loss: 16.992158 | Discriminator Loss: 0.084655\n","| Global Epoch: 26 | Local Epoch: 6 | Generator Loss: 17.059892 | Discriminator Loss: 0.072950\n","| Global Epoch: 26 | Local Epoch: 7 | Generator Loss: 17.046126 | Discriminator Loss: 0.064120\n","| Global Epoch: 26 | Local Epoch: 8 | Generator Loss: 16.999151 | Discriminator Loss: 0.057210\n","| Global Epoch: 26 | Local Epoch: 9 | Generator Loss: 16.991091 | Discriminator Loss: 0.051670\n","| Global Epoch: 26 | Local Epoch: 0 | Generator Loss: 17.585929 | Discriminator Loss: 0.379147\n","| Global Epoch: 26 | Local Epoch: 1 | Generator Loss: 17.087291 | Discriminator Loss: 0.211534\n","| Global Epoch: 26 | Local Epoch: 2 | Generator Loss: 16.893112 | Discriminator Loss: 0.144837\n","| Global Epoch: 26 | Local Epoch: 3 | Generator Loss: 16.854502 | Discriminator Loss: 0.109812\n","| Global Epoch: 26 | Local Epoch: 4 | Generator Loss: 16.883159 | Discriminator Loss: 0.088565\n","| Global Epoch: 26 | Local Epoch: 5 | Generator Loss: 16.852690 | Discriminator Loss: 0.074243\n","| Global Epoch: 26 | Local Epoch: 6 | Generator Loss: 16.927181 | Discriminator Loss: 0.064009\n","| Global Epoch: 26 | Local Epoch: 7 | Generator Loss: 16.993902 | Discriminator Loss: 0.056266\n","| Global Epoch: 26 | Local Epoch: 8 | Generator Loss: 17.000392 | Discriminator Loss: 0.050212\n","| Global Epoch: 26 | Local Epoch: 9 | Generator Loss: 16.970209 | Discriminator Loss: 0.045343\n","After 27 epoch global training, averged local generator loss is: 16.5908, averged local discrimitor loss is: 0.1219\n","Averged validation loss is: 6.9079\n","| Global Epoch: 27 | Local Epoch: 0 | Generator Loss: 17.164702 | Discriminator Loss: 0.025821\n","| Global Epoch: 27 | Local Epoch: 1 | Generator Loss: 16.441690 | Discriminator Loss: 0.023576\n","| Global Epoch: 27 | Local Epoch: 2 | Generator Loss: 16.530585 | Discriminator Loss: 0.020617\n","| Global Epoch: 27 | Local Epoch: 3 | Generator Loss: 16.524226 | Discriminator Loss: 0.016995\n","| Global Epoch: 27 | Local Epoch: 4 | Generator Loss: 16.645894 | Discriminator Loss: 0.014730\n","| Global Epoch: 27 | Local Epoch: 5 | Generator Loss: 16.627121 | Discriminator Loss: 0.013206\n","| Global Epoch: 27 | Local Epoch: 6 | Generator Loss: 16.554715 | Discriminator Loss: 0.012540\n","| Global Epoch: 27 | Local Epoch: 7 | Generator Loss: 16.351913 | Discriminator Loss: 0.017866\n","| Global Epoch: 27 | Local Epoch: 8 | Generator Loss: 16.293202 | Discriminator Loss: 0.024273\n","| Global Epoch: 27 | Local Epoch: 9 | Generator Loss: 16.291573 | Discriminator Loss: 0.031598\n","| Global Epoch: 27 | Local Epoch: 0 | Generator Loss: 15.609820 | Discriminator Loss: 0.739872\n","| Global Epoch: 27 | Local Epoch: 1 | Generator Loss: 14.879499 | Discriminator Loss: 0.463314\n","| Global Epoch: 27 | Local Epoch: 2 | Generator Loss: 14.265689 | Discriminator Loss: 0.342211\n","| Global Epoch: 27 | Local Epoch: 3 | Generator Loss: 14.431972 | Discriminator Loss: 0.262063\n","| Global Epoch: 27 | Local Epoch: 4 | Generator Loss: 14.456258 | Discriminator Loss: 0.211846\n","| Global Epoch: 27 | Local Epoch: 5 | Generator Loss: 14.491189 | Discriminator Loss: 0.179472\n","| Global Epoch: 27 | Local Epoch: 6 | Generator Loss: 14.584126 | Discriminator Loss: 0.168768\n","| Global Epoch: 27 | Local Epoch: 7 | Generator Loss: 14.543026 | Discriminator Loss: 0.159414\n","| Global Epoch: 27 | Local Epoch: 8 | Generator Loss: 14.683342 | Discriminator Loss: 0.148751\n","| Global Epoch: 27 | Local Epoch: 9 | Generator Loss: 14.713421 | Discriminator Loss: 0.139359\n","| Global Epoch: 27 | Local Epoch: 0 | Generator Loss: 14.624245 | Discriminator Loss: 0.788786\n","| Global Epoch: 27 | Local Epoch: 1 | Generator Loss: 14.311745 | Discriminator Loss: 0.454534\n","| Global Epoch: 27 | Local Epoch: 2 | Generator Loss: 14.426631 | Discriminator Loss: 0.313648\n","| Global Epoch: 27 | Local Epoch: 3 | Generator Loss: 14.527660 | Discriminator Loss: 0.238937\n","| Global Epoch: 27 | Local Epoch: 4 | Generator Loss: 14.521049 | Discriminator Loss: 0.192890\n","| Global Epoch: 27 | Local Epoch: 5 | Generator Loss: 14.508812 | Discriminator Loss: 0.161845\n","| Global Epoch: 27 | Local Epoch: 6 | Generator Loss: 14.568391 | Discriminator Loss: 0.139467\n","| Global Epoch: 27 | Local Epoch: 7 | Generator Loss: 14.576488 | Discriminator Loss: 0.122563\n","| Global Epoch: 27 | Local Epoch: 8 | Generator Loss: 14.576582 | Discriminator Loss: 0.109348\n","| Global Epoch: 27 | Local Epoch: 9 | Generator Loss: 14.615457 | Discriminator Loss: 0.098740\n","| Global Epoch: 27 | Local Epoch: 0 | Generator Loss: 16.110491 | Discriminator Loss: 0.304489\n","| Global Epoch: 27 | Local Epoch: 1 | Generator Loss: 15.302100 | Discriminator Loss: 0.171054\n","| Global Epoch: 27 | Local Epoch: 2 | Generator Loss: 15.141651 | Discriminator Loss: 0.116804\n","| Global Epoch: 27 | Local Epoch: 3 | Generator Loss: 15.023446 | Discriminator Loss: 0.088745\n","| Global Epoch: 27 | Local Epoch: 4 | Generator Loss: 15.235427 | Discriminator Loss: 0.071771\n","| Global Epoch: 27 | Local Epoch: 5 | Generator Loss: 15.575665 | Discriminator Loss: 0.060647\n","| Global Epoch: 27 | Local Epoch: 6 | Generator Loss: 15.717447 | Discriminator Loss: 0.052443\n","| Global Epoch: 27 | Local Epoch: 7 | Generator Loss: 15.880659 | Discriminator Loss: 0.046197\n","| Global Epoch: 27 | Local Epoch: 8 | Generator Loss: 15.903639 | Discriminator Loss: 0.041267\n","| Global Epoch: 27 | Local Epoch: 9 | Generator Loss: 15.920697 | Discriminator Loss: 0.037301\n","| Global Epoch: 27 | Local Epoch: 0 | Generator Loss: 16.536696 | Discriminator Loss: 0.116431\n","| Global Epoch: 27 | Local Epoch: 1 | Generator Loss: 16.182635 | Discriminator Loss: 0.066146\n","| Global Epoch: 27 | Local Epoch: 2 | Generator Loss: 16.100651 | Discriminator Loss: 0.045918\n","| Global Epoch: 27 | Local Epoch: 3 | Generator Loss: 15.896745 | Discriminator Loss: 0.042389\n","| Global Epoch: 27 | Local Epoch: 4 | Generator Loss: 15.937516 | Discriminator Loss: 0.044125\n","| Global Epoch: 27 | Local Epoch: 5 | Generator Loss: 15.931849 | Discriminator Loss: 0.038261\n","| Global Epoch: 27 | Local Epoch: 6 | Generator Loss: 15.776954 | Discriminator Loss: 0.038281\n","| Global Epoch: 27 | Local Epoch: 7 | Generator Loss: 15.712363 | Discriminator Loss: 0.038133\n","| Global Epoch: 27 | Local Epoch: 8 | Generator Loss: 15.809441 | Discriminator Loss: 0.039430\n","| Global Epoch: 27 | Local Epoch: 9 | Generator Loss: 15.947157 | Discriminator Loss: 0.038969\n","After 28 epoch global training, averged local generator loss is: 16.5678, averged local discrimitor loss is: 0.1189\n","Averged validation loss is: 7.7222\n","| Global Epoch: 28 | Local Epoch: 0 | Generator Loss: 16.830251 | Discriminator Loss: 0.300686\n","| Global Epoch: 28 | Local Epoch: 1 | Generator Loss: 16.552328 | Discriminator Loss: 0.166325\n","| Global Epoch: 28 | Local Epoch: 2 | Generator Loss: 16.870414 | Discriminator Loss: 0.113853\n","| Global Epoch: 28 | Local Epoch: 3 | Generator Loss: 16.850132 | Discriminator Loss: 0.086534\n","| Global Epoch: 28 | Local Epoch: 4 | Generator Loss: 16.943934 | Discriminator Loss: 0.069985\n","| Global Epoch: 28 | Local Epoch: 5 | Generator Loss: 17.103230 | Discriminator Loss: 0.058776\n","| Global Epoch: 28 | Local Epoch: 6 | Generator Loss: 17.128169 | Discriminator Loss: 0.050703\n","| Global Epoch: 28 | Local Epoch: 7 | Generator Loss: 17.269817 | Discriminator Loss: 0.044675\n","| Global Epoch: 28 | Local Epoch: 8 | Generator Loss: 17.333598 | Discriminator Loss: 0.039913\n","| Global Epoch: 28 | Local Epoch: 9 | Generator Loss: 17.446365 | Discriminator Loss: 0.036081\n","| Global Epoch: 28 | Local Epoch: 0 | Generator Loss: 17.768631 | Discriminator Loss: 0.282315\n","| Global Epoch: 28 | Local Epoch: 1 | Generator Loss: 16.875829 | Discriminator Loss: 0.159605\n","| Global Epoch: 28 | Local Epoch: 2 | Generator Loss: 17.027490 | Discriminator Loss: 0.110030\n","| Global Epoch: 28 | Local Epoch: 3 | Generator Loss: 16.959327 | Discriminator Loss: 0.083669\n","| Global Epoch: 28 | Local Epoch: 4 | Generator Loss: 16.842174 | Discriminator Loss: 0.067590\n","| Global Epoch: 28 | Local Epoch: 5 | Generator Loss: 16.842733 | Discriminator Loss: 0.056740\n","| Global Epoch: 28 | Local Epoch: 6 | Generator Loss: 16.823708 | Discriminator Loss: 0.048960\n","| Global Epoch: 28 | Local Epoch: 7 | Generator Loss: 16.850539 | Discriminator Loss: 0.043078\n","| Global Epoch: 28 | Local Epoch: 8 | Generator Loss: 16.859218 | Discriminator Loss: 0.038474\n","| Global Epoch: 28 | Local Epoch: 9 | Generator Loss: 16.936571 | Discriminator Loss: 0.034836\n","| Global Epoch: 28 | Local Epoch: 0 | Generator Loss: 17.794652 | Discriminator Loss: 0.232512\n","| Global Epoch: 28 | Local Epoch: 1 | Generator Loss: 18.155678 | Discriminator Loss: 0.125313\n","| Global Epoch: 28 | Local Epoch: 2 | Generator Loss: 17.768406 | Discriminator Loss: 0.086218\n","| Global Epoch: 28 | Local Epoch: 3 | Generator Loss: 17.505234 | Discriminator Loss: 0.065332\n","| Global Epoch: 28 | Local Epoch: 4 | Generator Loss: 17.432491 | Discriminator Loss: 0.052572\n","| Global Epoch: 28 | Local Epoch: 5 | Generator Loss: 17.250663 | Discriminator Loss: 0.044039\n","| Global Epoch: 28 | Local Epoch: 6 | Generator Loss: 17.100019 | Discriminator Loss: 0.037891\n","| Global Epoch: 28 | Local Epoch: 7 | Generator Loss: 16.928008 | Discriminator Loss: 0.033234\n","| Global Epoch: 28 | Local Epoch: 8 | Generator Loss: 16.873313 | Discriminator Loss: 0.029627\n","| Global Epoch: 28 | Local Epoch: 9 | Generator Loss: 16.785722 | Discriminator Loss: 0.026707\n","| Global Epoch: 28 | Local Epoch: 0 | Generator Loss: 16.529106 | Discriminator Loss: 0.168157\n","| Global Epoch: 28 | Local Epoch: 1 | Generator Loss: 16.643101 | Discriminator Loss: 0.092821\n","| Global Epoch: 28 | Local Epoch: 2 | Generator Loss: 16.566098 | Discriminator Loss: 0.064991\n","| Global Epoch: 28 | Local Epoch: 3 | Generator Loss: 16.603496 | Discriminator Loss: 0.049752\n","| Global Epoch: 28 | Local Epoch: 4 | Generator Loss: 16.588269 | Discriminator Loss: 0.040214\n","| Global Epoch: 28 | Local Epoch: 5 | Generator Loss: 16.722310 | Discriminator Loss: 0.033867\n","| Global Epoch: 28 | Local Epoch: 6 | Generator Loss: 16.845032 | Discriminator Loss: 0.029237\n","| Global Epoch: 28 | Local Epoch: 7 | Generator Loss: 16.846048 | Discriminator Loss: 0.025744\n","| Global Epoch: 28 | Local Epoch: 8 | Generator Loss: 16.852315 | Discriminator Loss: 0.023013\n","| Global Epoch: 28 | Local Epoch: 9 | Generator Loss: 16.879618 | Discriminator Loss: 0.020821\n","| Global Epoch: 28 | Local Epoch: 0 | Generator Loss: 16.061252 | Discriminator Loss: 0.096086\n","| Global Epoch: 28 | Local Epoch: 1 | Generator Loss: 15.571788 | Discriminator Loss: 0.056462\n","| Global Epoch: 28 | Local Epoch: 2 | Generator Loss: 15.669048 | Discriminator Loss: 0.038973\n","| Global Epoch: 28 | Local Epoch: 3 | Generator Loss: 15.981301 | Discriminator Loss: 0.030459\n","| Global Epoch: 28 | Local Epoch: 4 | Generator Loss: 15.949726 | Discriminator Loss: 0.030877\n","| Global Epoch: 28 | Local Epoch: 5 | Generator Loss: 16.007605 | Discriminator Loss: 0.028839\n","| Global Epoch: 28 | Local Epoch: 6 | Generator Loss: 16.050958 | Discriminator Loss: 0.025328\n","| Global Epoch: 28 | Local Epoch: 7 | Generator Loss: 16.171523 | Discriminator Loss: 0.022352\n","| Global Epoch: 28 | Local Epoch: 8 | Generator Loss: 16.277719 | Discriminator Loss: 0.020007\n","| Global Epoch: 28 | Local Epoch: 9 | Generator Loss: 16.345007 | Discriminator Loss: 0.018403\n","After 29 epoch global training, averged local generator loss is: 16.5601, averged local discrimitor loss is: 0.1154\n","Averged validation loss is: 8.3077\n","| Global Epoch: 29 | Local Epoch: 0 | Generator Loss: 16.514338 | Discriminator Loss: 0.212352\n","| Global Epoch: 29 | Local Epoch: 1 | Generator Loss: 16.007024 | Discriminator Loss: 0.127356\n","| Global Epoch: 29 | Local Epoch: 2 | Generator Loss: 15.896623 | Discriminator Loss: 0.087537\n","| Global Epoch: 29 | Local Epoch: 3 | Generator Loss: 15.699673 | Discriminator Loss: 0.070218\n","| Global Epoch: 29 | Local Epoch: 4 | Generator Loss: 15.329185 | Discriminator Loss: 0.078124\n","| Global Epoch: 29 | Local Epoch: 5 | Generator Loss: 15.333324 | Discriminator Loss: 0.100840\n","| Global Epoch: 29 | Local Epoch: 6 | Generator Loss: 15.469218 | Discriminator Loss: 0.108994\n","| Global Epoch: 29 | Local Epoch: 7 | Generator Loss: 15.382258 | Discriminator Loss: 0.106584\n","| Global Epoch: 29 | Local Epoch: 8 | Generator Loss: 15.497980 | Discriminator Loss: 0.096350\n","| Global Epoch: 29 | Local Epoch: 9 | Generator Loss: 15.531029 | Discriminator Loss: 0.090317\n","| Global Epoch: 29 | Local Epoch: 0 | Generator Loss: 16.223333 | Discriminator Loss: 0.391859\n","| Global Epoch: 29 | Local Epoch: 1 | Generator Loss: 16.606882 | Discriminator Loss: 0.203415\n","| Global Epoch: 29 | Local Epoch: 2 | Generator Loss: 16.498377 | Discriminator Loss: 0.138476\n","| Global Epoch: 29 | Local Epoch: 3 | Generator Loss: 16.527426 | Discriminator Loss: 0.105309\n","| Global Epoch: 29 | Local Epoch: 4 | Generator Loss: 16.386010 | Discriminator Loss: 0.085015\n","| Global Epoch: 29 | Local Epoch: 5 | Generator Loss: 16.319027 | Discriminator Loss: 0.071060\n","| Global Epoch: 29 | Local Epoch: 6 | Generator Loss: 16.326405 | Discriminator Loss: 0.061087\n","| Global Epoch: 29 | Local Epoch: 7 | Generator Loss: 16.236637 | Discriminator Loss: 0.053541\n","| Global Epoch: 29 | Local Epoch: 8 | Generator Loss: 16.205417 | Discriminator Loss: 0.047661\n","| Global Epoch: 29 | Local Epoch: 9 | Generator Loss: 16.225806 | Discriminator Loss: 0.042965\n","| Global Epoch: 29 | Local Epoch: 0 | Generator Loss: 15.206971 | Discriminator Loss: 0.246230\n","| Global Epoch: 29 | Local Epoch: 1 | Generator Loss: 15.510062 | Discriminator Loss: 0.132108\n","| Global Epoch: 29 | Local Epoch: 2 | Generator Loss: 15.641050 | Discriminator Loss: 0.090421\n","| Global Epoch: 29 | Local Epoch: 3 | Generator Loss: 15.712997 | Discriminator Loss: 0.071339\n","| Global Epoch: 29 | Local Epoch: 4 | Generator Loss: 15.661772 | Discriminator Loss: 0.068235\n","| Global Epoch: 29 | Local Epoch: 5 | Generator Loss: 15.769140 | Discriminator Loss: 0.058842\n","| Global Epoch: 29 | Local Epoch: 6 | Generator Loss: 15.897856 | Discriminator Loss: 0.057682\n","| Global Epoch: 29 | Local Epoch: 7 | Generator Loss: 15.892165 | Discriminator Loss: 0.052961\n","| Global Epoch: 29 | Local Epoch: 8 | Generator Loss: 15.968436 | Discriminator Loss: 0.047479\n","| Global Epoch: 29 | Local Epoch: 9 | Generator Loss: 16.177551 | Discriminator Loss: 0.043400\n","| Global Epoch: 29 | Local Epoch: 0 | Generator Loss: 17.373030 | Discriminator Loss: 0.284290\n","| Global Epoch: 29 | Local Epoch: 1 | Generator Loss: 16.925078 | Discriminator Loss: 0.158683\n","| Global Epoch: 29 | Local Epoch: 2 | Generator Loss: 16.682383 | Discriminator Loss: 0.108632\n","| Global Epoch: 29 | Local Epoch: 3 | Generator Loss: 16.706908 | Discriminator Loss: 0.082602\n","| Global Epoch: 29 | Local Epoch: 4 | Generator Loss: 16.652077 | Discriminator Loss: 0.067719\n","| Global Epoch: 29 | Local Epoch: 5 | Generator Loss: 16.437191 | Discriminator Loss: 0.059623\n","| Global Epoch: 29 | Local Epoch: 6 | Generator Loss: 16.605383 | Discriminator Loss: 0.051762\n","| Global Epoch: 29 | Local Epoch: 7 | Generator Loss: 16.728234 | Discriminator Loss: 0.045646\n","| Global Epoch: 29 | Local Epoch: 8 | Generator Loss: 16.859366 | Discriminator Loss: 0.040780\n","| Global Epoch: 29 | Local Epoch: 9 | Generator Loss: 16.972584 | Discriminator Loss: 0.036869\n","| Global Epoch: 29 | Local Epoch: 0 | Generator Loss: 16.339701 | Discriminator Loss: 0.098720\n","| Global Epoch: 29 | Local Epoch: 1 | Generator Loss: 15.940325 | Discriminator Loss: 0.086420\n","| Global Epoch: 29 | Local Epoch: 2 | Generator Loss: 15.385175 | Discriminator Loss: 0.092761\n","| Global Epoch: 29 | Local Epoch: 3 | Generator Loss: 15.314392 | Discriminator Loss: 0.071099\n","| Global Epoch: 29 | Local Epoch: 4 | Generator Loss: 15.289387 | Discriminator Loss: 0.057505\n","| Global Epoch: 29 | Local Epoch: 5 | Generator Loss: 15.311480 | Discriminator Loss: 0.048326\n","| Global Epoch: 29 | Local Epoch: 6 | Generator Loss: 15.318515 | Discriminator Loss: 0.041795\n","| Global Epoch: 29 | Local Epoch: 7 | Generator Loss: 15.313665 | Discriminator Loss: 0.037023\n","| Global Epoch: 29 | Local Epoch: 8 | Generator Loss: 15.320357 | Discriminator Loss: 0.033091\n","| Global Epoch: 29 | Local Epoch: 9 | Generator Loss: 15.365172 | Discriminator Loss: 0.029923\n","After 30 epoch global training, averged local generator loss is: 16.5203, averged local discrimitor loss is: 0.1126\n","Averged validation loss is: 7.9697\n","| Global Epoch: 30 | Local Epoch: 0 | Generator Loss: 14.452480 | Discriminator Loss: 0.101584\n","| Global Epoch: 30 | Local Epoch: 1 | Generator Loss: 14.500866 | Discriminator Loss: 0.070027\n","| Global Epoch: 30 | Local Epoch: 2 | Generator Loss: 14.587727 | Discriminator Loss: 0.059605\n","| Global Epoch: 30 | Local Epoch: 3 | Generator Loss: 14.351152 | Discriminator Loss: 0.075724\n","| Global Epoch: 30 | Local Epoch: 4 | Generator Loss: 14.544892 | Discriminator Loss: 0.064813\n","| Global Epoch: 30 | Local Epoch: 5 | Generator Loss: 14.451959 | Discriminator Loss: 0.065713\n","| Global Epoch: 30 | Local Epoch: 6 | Generator Loss: 14.511562 | Discriminator Loss: 0.060644\n","| Global Epoch: 30 | Local Epoch: 7 | Generator Loss: 14.340809 | Discriminator Loss: 0.054390\n","| Global Epoch: 30 | Local Epoch: 8 | Generator Loss: 14.488489 | Discriminator Loss: 0.051609\n","| Global Epoch: 30 | Local Epoch: 9 | Generator Loss: 14.584919 | Discriminator Loss: 0.050863\n","| Global Epoch: 30 | Local Epoch: 0 | Generator Loss: 14.745965 | Discriminator Loss: 0.470327\n","| Global Epoch: 30 | Local Epoch: 1 | Generator Loss: 14.591428 | Discriminator Loss: 0.268325\n","| Global Epoch: 30 | Local Epoch: 2 | Generator Loss: 14.611837 | Discriminator Loss: 0.187826\n","| Global Epoch: 30 | Local Epoch: 3 | Generator Loss: 14.818764 | Discriminator Loss: 0.143324\n","| Global Epoch: 30 | Local Epoch: 4 | Generator Loss: 14.879361 | Discriminator Loss: 0.115827\n","| Global Epoch: 30 | Local Epoch: 5 | Generator Loss: 14.928395 | Discriminator Loss: 0.097231\n","| Global Epoch: 30 | Local Epoch: 6 | Generator Loss: 14.927478 | Discriminator Loss: 0.083829\n","| Global Epoch: 30 | Local Epoch: 7 | Generator Loss: 15.017348 | Discriminator Loss: 0.075544\n","| Global Epoch: 30 | Local Epoch: 8 | Generator Loss: 15.063633 | Discriminator Loss: 0.069432\n","| Global Epoch: 30 | Local Epoch: 9 | Generator Loss: 15.195205 | Discriminator Loss: 0.063086\n","| Global Epoch: 30 | Local Epoch: 0 | Generator Loss: 15.519978 | Discriminator Loss: 0.103002\n","| Global Epoch: 30 | Local Epoch: 1 | Generator Loss: 15.031552 | Discriminator Loss: 0.060665\n","| Global Epoch: 30 | Local Epoch: 2 | Generator Loss: 15.246943 | Discriminator Loss: 0.041921\n","| Global Epoch: 30 | Local Epoch: 3 | Generator Loss: 15.341946 | Discriminator Loss: 0.032759\n","| Global Epoch: 30 | Local Epoch: 4 | Generator Loss: 15.518913 | Discriminator Loss: 0.026794\n","| Global Epoch: 30 | Local Epoch: 5 | Generator Loss: 15.604750 | Discriminator Loss: 0.037402\n","| Global Epoch: 30 | Local Epoch: 6 | Generator Loss: 15.635038 | Discriminator Loss: 0.042330\n","| Global Epoch: 30 | Local Epoch: 7 | Generator Loss: 15.715967 | Discriminator Loss: 0.040700\n","| Global Epoch: 30 | Local Epoch: 8 | Generator Loss: 15.798387 | Discriminator Loss: 0.036896\n","| Global Epoch: 30 | Local Epoch: 9 | Generator Loss: 15.874792 | Discriminator Loss: 0.033433\n","| Global Epoch: 30 | Local Epoch: 0 | Generator Loss: 17.099897 | Discriminator Loss: 0.321958\n","| Global Epoch: 30 | Local Epoch: 1 | Generator Loss: 16.349344 | Discriminator Loss: 0.177812\n","| Global Epoch: 30 | Local Epoch: 2 | Generator Loss: 16.131532 | Discriminator Loss: 0.121037\n","| Global Epoch: 30 | Local Epoch: 3 | Generator Loss: 16.016630 | Discriminator Loss: 0.091730\n","| Global Epoch: 30 | Local Epoch: 4 | Generator Loss: 15.976294 | Discriminator Loss: 0.073972\n","| Global Epoch: 30 | Local Epoch: 5 | Generator Loss: 15.952526 | Discriminator Loss: 0.062039\n","| Global Epoch: 30 | Local Epoch: 6 | Generator Loss: 15.940865 | Discriminator Loss: 0.053454\n","| Global Epoch: 30 | Local Epoch: 7 | Generator Loss: 15.957724 | Discriminator Loss: 0.046987\n","| Global Epoch: 30 | Local Epoch: 8 | Generator Loss: 16.029099 | Discriminator Loss: 0.041949\n","| Global Epoch: 30 | Local Epoch: 9 | Generator Loss: 16.110662 | Discriminator Loss: 0.037887\n","| Global Epoch: 30 | Local Epoch: 0 | Generator Loss: 16.614667 | Discriminator Loss: 0.090009\n","| Global Epoch: 30 | Local Epoch: 1 | Generator Loss: 16.391711 | Discriminator Loss: 0.050817\n","| Global Epoch: 30 | Local Epoch: 2 | Generator Loss: 16.122735 | Discriminator Loss: 0.034917\n","| Global Epoch: 30 | Local Epoch: 3 | Generator Loss: 16.322399 | Discriminator Loss: 0.026665\n","| Global Epoch: 30 | Local Epoch: 4 | Generator Loss: 16.379630 | Discriminator Loss: 0.022311\n","| Global Epoch: 30 | Local Epoch: 5 | Generator Loss: 16.268422 | Discriminator Loss: 0.021850\n","| Global Epoch: 30 | Local Epoch: 6 | Generator Loss: 16.356525 | Discriminator Loss: 0.024742\n","| Global Epoch: 30 | Local Epoch: 7 | Generator Loss: 16.255444 | Discriminator Loss: 0.045758\n","| Global Epoch: 30 | Local Epoch: 8 | Generator Loss: 16.112445 | Discriminator Loss: 0.049350\n","| Global Epoch: 30 | Local Epoch: 9 | Generator Loss: 16.005542 | Discriminator Loss: 0.045280\n","After 31 epoch global training, averged local generator loss is: 16.5037, averged local discrimitor loss is: 0.1104\n","Averged validation loss is: 6.2836\n","| Global Epoch: 31 | Local Epoch: 0 | Generator Loss: 15.411434 | Discriminator Loss: 0.030671\n","| Global Epoch: 31 | Local Epoch: 1 | Generator Loss: 15.486473 | Discriminator Loss: 0.027677\n","| Global Epoch: 31 | Local Epoch: 2 | Generator Loss: 15.722574 | Discriminator Loss: 0.019841\n","| Global Epoch: 31 | Local Epoch: 3 | Generator Loss: 16.001280 | Discriminator Loss: 0.020914\n","| Global Epoch: 31 | Local Epoch: 4 | Generator Loss: 15.671610 | Discriminator Loss: 0.034241\n","| Global Epoch: 31 | Local Epoch: 5 | Generator Loss: 15.645553 | Discriminator Loss: 0.030067\n","| Global Epoch: 31 | Local Epoch: 6 | Generator Loss: 15.783489 | Discriminator Loss: 0.026485\n","| Global Epoch: 31 | Local Epoch: 7 | Generator Loss: 15.797609 | Discriminator Loss: 0.025631\n","| Global Epoch: 31 | Local Epoch: 8 | Generator Loss: 15.809933 | Discriminator Loss: 0.025750\n","| Global Epoch: 31 | Local Epoch: 9 | Generator Loss: 15.810006 | Discriminator Loss: 0.023561\n","| Global Epoch: 31 | Local Epoch: 0 | Generator Loss: 16.483130 | Discriminator Loss: 0.486414\n","| Global Epoch: 31 | Local Epoch: 1 | Generator Loss: 17.016654 | Discriminator Loss: 0.250413\n","| Global Epoch: 31 | Local Epoch: 2 | Generator Loss: 17.228293 | Discriminator Loss: 0.169795\n","| Global Epoch: 31 | Local Epoch: 3 | Generator Loss: 17.190097 | Discriminator Loss: 0.129304\n","| Global Epoch: 31 | Local Epoch: 4 | Generator Loss: 17.237198 | Discriminator Loss: 0.104218\n","| Global Epoch: 31 | Local Epoch: 5 | Generator Loss: 17.118811 | Discriminator Loss: 0.087142\n","| Global Epoch: 31 | Local Epoch: 6 | Generator Loss: 16.963546 | Discriminator Loss: 0.074902\n","| Global Epoch: 31 | Local Epoch: 7 | Generator Loss: 16.735410 | Discriminator Loss: 0.065675\n","| Global Epoch: 31 | Local Epoch: 8 | Generator Loss: 16.684959 | Discriminator Loss: 0.058456\n","| Global Epoch: 31 | Local Epoch: 9 | Generator Loss: 16.525753 | Discriminator Loss: 0.052678\n","| Global Epoch: 31 | Local Epoch: 0 | Generator Loss: 15.551981 | Discriminator Loss: 0.456030\n","| Global Epoch: 31 | Local Epoch: 1 | Generator Loss: 15.518025 | Discriminator Loss: 0.251169\n","| Global Epoch: 31 | Local Epoch: 2 | Generator Loss: 15.631815 | Discriminator Loss: 0.171878\n","| Global Epoch: 31 | Local Epoch: 3 | Generator Loss: 15.685270 | Discriminator Loss: 0.130770\n","| Global Epoch: 31 | Local Epoch: 4 | Generator Loss: 15.793264 | Discriminator Loss: 0.106177\n","| Global Epoch: 31 | Local Epoch: 5 | Generator Loss: 15.739445 | Discriminator Loss: 0.097187\n","| Global Epoch: 31 | Local Epoch: 6 | Generator Loss: 15.624061 | Discriminator Loss: 0.089963\n","| Global Epoch: 31 | Local Epoch: 7 | Generator Loss: 15.810574 | Discriminator Loss: 0.082960\n","| Global Epoch: 31 | Local Epoch: 8 | Generator Loss: 16.079922 | Discriminator Loss: 0.074962\n","| Global Epoch: 31 | Local Epoch: 9 | Generator Loss: 16.218635 | Discriminator Loss: 0.067887\n","| Global Epoch: 31 | Local Epoch: 0 | Generator Loss: 17.073467 | Discriminator Loss: 0.683262\n","| Global Epoch: 31 | Local Epoch: 1 | Generator Loss: 15.678555 | Discriminator Loss: 0.401617\n","| Global Epoch: 31 | Local Epoch: 2 | Generator Loss: 15.576389 | Discriminator Loss: 0.279660\n","| Global Epoch: 31 | Local Epoch: 3 | Generator Loss: 15.392491 | Discriminator Loss: 0.213473\n","| Global Epoch: 31 | Local Epoch: 4 | Generator Loss: 15.348832 | Discriminator Loss: 0.172689\n","| Global Epoch: 31 | Local Epoch: 5 | Generator Loss: 15.420424 | Discriminator Loss: 0.145109\n","| Global Epoch: 31 | Local Epoch: 6 | Generator Loss: 15.399082 | Discriminator Loss: 0.125119\n","| Global Epoch: 31 | Local Epoch: 7 | Generator Loss: 15.429262 | Discriminator Loss: 0.110004\n","| Global Epoch: 31 | Local Epoch: 8 | Generator Loss: 15.536671 | Discriminator Loss: 0.098355\n","| Global Epoch: 31 | Local Epoch: 9 | Generator Loss: 15.572819 | Discriminator Loss: 0.088825\n","| Global Epoch: 31 | Local Epoch: 0 | Generator Loss: 16.576632 | Discriminator Loss: 0.218394\n","| Global Epoch: 31 | Local Epoch: 1 | Generator Loss: 16.290136 | Discriminator Loss: 0.142283\n","| Global Epoch: 31 | Local Epoch: 2 | Generator Loss: 15.924802 | Discriminator Loss: 0.117168\n","| Global Epoch: 31 | Local Epoch: 3 | Generator Loss: 15.963399 | Discriminator Loss: 0.100543\n","| Global Epoch: 31 | Local Epoch: 4 | Generator Loss: 15.908017 | Discriminator Loss: 0.087606\n","| Global Epoch: 31 | Local Epoch: 5 | Generator Loss: 15.831199 | Discriminator Loss: 0.074098\n","| Global Epoch: 31 | Local Epoch: 6 | Generator Loss: 15.810254 | Discriminator Loss: 0.064165\n","| Global Epoch: 31 | Local Epoch: 7 | Generator Loss: 15.702035 | Discriminator Loss: 0.057009\n","| Global Epoch: 31 | Local Epoch: 8 | Generator Loss: 15.669404 | Discriminator Loss: 0.051060\n","| Global Epoch: 31 | Local Epoch: 9 | Generator Loss: 15.700641 | Discriminator Loss: 0.046320\n","After 32 epoch global training, averged local generator loss is: 16.4786, averged local discrimitor loss is: 0.1084\n","Averged validation loss is: 7.6656\n","| Global Epoch: 32 | Local Epoch: 0 | Generator Loss: 16.419019 | Discriminator Loss: 0.412354\n","| Global Epoch: 32 | Local Epoch: 1 | Generator Loss: 15.857750 | Discriminator Loss: 0.227366\n","| Global Epoch: 32 | Local Epoch: 2 | Generator Loss: 15.631203 | Discriminator Loss: 0.155282\n","| Global Epoch: 32 | Local Epoch: 3 | Generator Loss: 15.531235 | Discriminator Loss: 0.117956\n","| Global Epoch: 32 | Local Epoch: 4 | Generator Loss: 15.355361 | Discriminator Loss: 0.095225\n","| Global Epoch: 32 | Local Epoch: 5 | Generator Loss: 15.426051 | Discriminator Loss: 0.080600\n","| Global Epoch: 32 | Local Epoch: 6 | Generator Loss: 15.368062 | Discriminator Loss: 0.069660\n","| Global Epoch: 32 | Local Epoch: 7 | Generator Loss: 15.563199 | Discriminator Loss: 0.061375\n","| Global Epoch: 32 | Local Epoch: 8 | Generator Loss: 15.658105 | Discriminator Loss: 0.054815\n","| Global Epoch: 32 | Local Epoch: 9 | Generator Loss: 15.703975 | Discriminator Loss: 0.049621\n","| Global Epoch: 32 | Local Epoch: 0 | Generator Loss: 16.290401 | Discriminator Loss: 0.127617\n","| Global Epoch: 32 | Local Epoch: 1 | Generator Loss: 16.211394 | Discriminator Loss: 0.075184\n","| Global Epoch: 32 | Local Epoch: 2 | Generator Loss: 15.834950 | Discriminator Loss: 0.052518\n","| Global Epoch: 32 | Local Epoch: 3 | Generator Loss: 15.970348 | Discriminator Loss: 0.040216\n","| Global Epoch: 32 | Local Epoch: 4 | Generator Loss: 16.155114 | Discriminator Loss: 0.032613\n","| Global Epoch: 32 | Local Epoch: 5 | Generator Loss: 16.274123 | Discriminator Loss: 0.029523\n","| Global Epoch: 32 | Local Epoch: 6 | Generator Loss: 16.267508 | Discriminator Loss: 0.026584\n","| Global Epoch: 32 | Local Epoch: 7 | Generator Loss: 16.385274 | Discriminator Loss: 0.023716\n","| Global Epoch: 32 | Local Epoch: 8 | Generator Loss: 16.444251 | Discriminator Loss: 0.025010\n","| Global Epoch: 32 | Local Epoch: 9 | Generator Loss: 16.306264 | Discriminator Loss: 0.039637\n","| Global Epoch: 32 | Local Epoch: 0 | Generator Loss: 14.817947 | Discriminator Loss: 0.286042\n","| Global Epoch: 32 | Local Epoch: 1 | Generator Loss: 14.541491 | Discriminator Loss: 0.155225\n","| Global Epoch: 32 | Local Epoch: 2 | Generator Loss: 14.827648 | Discriminator Loss: 0.107245\n","| Global Epoch: 32 | Local Epoch: 3 | Generator Loss: 14.737305 | Discriminator Loss: 0.081466\n","| Global Epoch: 32 | Local Epoch: 4 | Generator Loss: 14.727458 | Discriminator Loss: 0.065761\n","| Global Epoch: 32 | Local Epoch: 5 | Generator Loss: 14.809800 | Discriminator Loss: 0.055229\n","| Global Epoch: 32 | Local Epoch: 6 | Generator Loss: 14.940439 | Discriminator Loss: 0.047618\n","| Global Epoch: 32 | Local Epoch: 7 | Generator Loss: 15.104527 | Discriminator Loss: 0.041901\n","| Global Epoch: 32 | Local Epoch: 8 | Generator Loss: 15.220968 | Discriminator Loss: 0.037424\n","| Global Epoch: 32 | Local Epoch: 9 | Generator Loss: 15.310421 | Discriminator Loss: 0.034010\n","| Global Epoch: 32 | Local Epoch: 0 | Generator Loss: 17.249031 | Discriminator Loss: 0.366356\n","| Global Epoch: 32 | Local Epoch: 1 | Generator Loss: 16.453730 | Discriminator Loss: 0.203860\n","| Global Epoch: 32 | Local Epoch: 2 | Generator Loss: 16.174273 | Discriminator Loss: 0.139112\n","| Global Epoch: 32 | Local Epoch: 3 | Generator Loss: 16.036657 | Discriminator Loss: 0.105460\n","| Global Epoch: 32 | Local Epoch: 4 | Generator Loss: 15.856842 | Discriminator Loss: 0.085018\n","| Global Epoch: 32 | Local Epoch: 5 | Generator Loss: 15.776997 | Discriminator Loss: 0.071307\n","| Global Epoch: 32 | Local Epoch: 6 | Generator Loss: 15.790388 | Discriminator Loss: 0.063754\n","| Global Epoch: 32 | Local Epoch: 7 | Generator Loss: 15.894988 | Discriminator Loss: 0.058861\n","| Global Epoch: 32 | Local Epoch: 8 | Generator Loss: 16.045033 | Discriminator Loss: 0.052748\n","| Global Epoch: 32 | Local Epoch: 9 | Generator Loss: 16.171035 | Discriminator Loss: 0.047687\n","| Global Epoch: 32 | Local Epoch: 0 | Generator Loss: 16.476518 | Discriminator Loss: 0.159401\n","| Global Epoch: 32 | Local Epoch: 1 | Generator Loss: 15.608710 | Discriminator Loss: 0.092415\n","| Global Epoch: 32 | Local Epoch: 2 | Generator Loss: 15.692381 | Discriminator Loss: 0.063686\n","| Global Epoch: 32 | Local Epoch: 3 | Generator Loss: 15.812838 | Discriminator Loss: 0.048616\n","| Global Epoch: 32 | Local Epoch: 4 | Generator Loss: 16.025725 | Discriminator Loss: 0.039391\n","| Global Epoch: 32 | Local Epoch: 5 | Generator Loss: 16.142233 | Discriminator Loss: 0.035695\n","| Global Epoch: 32 | Local Epoch: 6 | Generator Loss: 15.751126 | Discriminator Loss: 0.069744\n","| Global Epoch: 32 | Local Epoch: 7 | Generator Loss: 15.689727 | Discriminator Loss: 0.063621\n","| Global Epoch: 32 | Local Epoch: 8 | Generator Loss: 15.601191 | Discriminator Loss: 0.058971\n","| Global Epoch: 32 | Local Epoch: 9 | Generator Loss: 15.583517 | Discriminator Loss: 0.053655\n","After 33 epoch global training, averged local generator loss is: 16.4515, averged local discrimitor loss is: 0.1068\n","Averged validation loss is: 8.1004\n","| Global Epoch: 33 | Local Epoch: 0 | Generator Loss: 15.109146 | Discriminator Loss: 0.033948\n","| Global Epoch: 33 | Local Epoch: 1 | Generator Loss: 15.298148 | Discriminator Loss: 0.035494\n","| Global Epoch: 33 | Local Epoch: 2 | Generator Loss: 15.198364 | Discriminator Loss: 0.032184\n","| Global Epoch: 33 | Local Epoch: 3 | Generator Loss: 15.154603 | Discriminator Loss: 0.024909\n","| Global Epoch: 33 | Local Epoch: 4 | Generator Loss: 15.328801 | Discriminator Loss: 0.020273\n","| Global Epoch: 33 | Local Epoch: 5 | Generator Loss: 15.504407 | Discriminator Loss: 0.017311\n","| Global Epoch: 33 | Local Epoch: 6 | Generator Loss: 15.616948 | Discriminator Loss: 0.015331\n","| Global Epoch: 33 | Local Epoch: 7 | Generator Loss: 15.695932 | Discriminator Loss: 0.013540\n","| Global Epoch: 33 | Local Epoch: 8 | Generator Loss: 15.863270 | Discriminator Loss: 0.012794\n","| Global Epoch: 33 | Local Epoch: 9 | Generator Loss: 15.831418 | Discriminator Loss: 0.029169\n","| Global Epoch: 33 | Local Epoch: 0 | Generator Loss: 14.358186 | Discriminator Loss: 0.361014\n","| Global Epoch: 33 | Local Epoch: 1 | Generator Loss: 15.191203 | Discriminator Loss: 0.223394\n","| Global Epoch: 33 | Local Epoch: 2 | Generator Loss: 14.920433 | Discriminator Loss: 0.171278\n","| Global Epoch: 33 | Local Epoch: 3 | Generator Loss: 15.285290 | Discriminator Loss: 0.139088\n","| Global Epoch: 33 | Local Epoch: 4 | Generator Loss: 15.519512 | Discriminator Loss: 0.116105\n","| Global Epoch: 33 | Local Epoch: 5 | Generator Loss: 15.068978 | Discriminator Loss: 0.109097\n","| Global Epoch: 33 | Local Epoch: 6 | Generator Loss: 15.143472 | Discriminator Loss: 0.096216\n","| Global Epoch: 33 | Local Epoch: 7 | Generator Loss: 15.329605 | Discriminator Loss: 0.085294\n","| Global Epoch: 33 | Local Epoch: 8 | Generator Loss: 15.432223 | Discriminator Loss: 0.076286\n","| Global Epoch: 33 | Local Epoch: 9 | Generator Loss: 15.516119 | Discriminator Loss: 0.069310\n","| Global Epoch: 33 | Local Epoch: 0 | Generator Loss: 16.414587 | Discriminator Loss: 0.414346\n","| Global Epoch: 33 | Local Epoch: 1 | Generator Loss: 15.965049 | Discriminator Loss: 0.231245\n","| Global Epoch: 33 | Local Epoch: 2 | Generator Loss: 15.291302 | Discriminator Loss: 0.171919\n","| Global Epoch: 33 | Local Epoch: 3 | Generator Loss: 15.161264 | Discriminator Loss: 0.132318\n","| Global Epoch: 33 | Local Epoch: 4 | Generator Loss: 15.053769 | Discriminator Loss: 0.107080\n","| Global Epoch: 33 | Local Epoch: 5 | Generator Loss: 14.973712 | Discriminator Loss: 0.089953\n","| Global Epoch: 33 | Local Epoch: 6 | Generator Loss: 15.119412 | Discriminator Loss: 0.078174\n","| Global Epoch: 33 | Local Epoch: 7 | Generator Loss: 15.209968 | Discriminator Loss: 0.068883\n","| Global Epoch: 33 | Local Epoch: 8 | Generator Loss: 15.236417 | Discriminator Loss: 0.061526\n","| Global Epoch: 33 | Local Epoch: 9 | Generator Loss: 15.295117 | Discriminator Loss: 0.055618\n","| Global Epoch: 33 | Local Epoch: 0 | Generator Loss: 15.245536 | Discriminator Loss: 0.241199\n","| Global Epoch: 33 | Local Epoch: 1 | Generator Loss: 15.105808 | Discriminator Loss: 0.135008\n","| Global Epoch: 33 | Local Epoch: 2 | Generator Loss: 15.139754 | Discriminator Loss: 0.092521\n","| Global Epoch: 33 | Local Epoch: 3 | Generator Loss: 15.382638 | Discriminator Loss: 0.070565\n","| Global Epoch: 33 | Local Epoch: 4 | Generator Loss: 15.460143 | Discriminator Loss: 0.057075\n","| Global Epoch: 33 | Local Epoch: 5 | Generator Loss: 15.620810 | Discriminator Loss: 0.049243\n","| Global Epoch: 33 | Local Epoch: 6 | Generator Loss: 15.457656 | Discriminator Loss: 0.055706\n","| Global Epoch: 33 | Local Epoch: 7 | Generator Loss: 15.361989 | Discriminator Loss: 0.052548\n","| Global Epoch: 33 | Local Epoch: 8 | Generator Loss: 15.369520 | Discriminator Loss: 0.047521\n","| Global Epoch: 33 | Local Epoch: 9 | Generator Loss: 15.429655 | Discriminator Loss: 0.043047\n","| Global Epoch: 33 | Local Epoch: 0 | Generator Loss: 15.741779 | Discriminator Loss: 0.162593\n","| Global Epoch: 33 | Local Epoch: 1 | Generator Loss: 15.380072 | Discriminator Loss: 0.088729\n","| Global Epoch: 33 | Local Epoch: 2 | Generator Loss: 15.385819 | Discriminator Loss: 0.060605\n","| Global Epoch: 33 | Local Epoch: 3 | Generator Loss: 15.649214 | Discriminator Loss: 0.046739\n","| Global Epoch: 33 | Local Epoch: 4 | Generator Loss: 15.565753 | Discriminator Loss: 0.043064\n","| Global Epoch: 33 | Local Epoch: 5 | Generator Loss: 15.560568 | Discriminator Loss: 0.036833\n","| Global Epoch: 33 | Local Epoch: 6 | Generator Loss: 15.567599 | Discriminator Loss: 0.031909\n","| Global Epoch: 33 | Local Epoch: 7 | Generator Loss: 15.687758 | Discriminator Loss: 0.028126\n","| Global Epoch: 33 | Local Epoch: 8 | Generator Loss: 15.779032 | Discriminator Loss: 0.025186\n","| Global Epoch: 33 | Local Epoch: 9 | Generator Loss: 15.833210 | Discriminator Loss: 0.025210\n","After 34 epoch global training, averged local generator loss is: 16.4333, averged local discrimitor loss is: 0.1044\n","Averged validation loss is: 9.1261\n","| Global Epoch: 34 | Local Epoch: 0 | Generator Loss: 15.819536 | Discriminator Loss: 0.124700\n","| Global Epoch: 34 | Local Epoch: 1 | Generator Loss: 15.674327 | Discriminator Loss: 0.110547\n","| Global Epoch: 34 | Local Epoch: 2 | Generator Loss: 15.471533 | Discriminator Loss: 0.126342\n","| Global Epoch: 34 | Local Epoch: 3 | Generator Loss: 15.337593 | Discriminator Loss: 0.102142\n","| Global Epoch: 34 | Local Epoch: 4 | Generator Loss: 15.244287 | Discriminator Loss: 0.094856\n","| Global Epoch: 34 | Local Epoch: 5 | Generator Loss: 15.243489 | Discriminator Loss: 0.086879\n","| Global Epoch: 34 | Local Epoch: 6 | Generator Loss: 15.096579 | Discriminator Loss: 0.113523\n","| Global Epoch: 34 | Local Epoch: 7 | Generator Loss: 14.987736 | Discriminator Loss: 0.109304\n","| Global Epoch: 34 | Local Epoch: 8 | Generator Loss: 14.946879 | Discriminator Loss: 0.098871\n","| Global Epoch: 34 | Local Epoch: 9 | Generator Loss: 14.999819 | Discriminator Loss: 0.090007\n","| Global Epoch: 34 | Local Epoch: 0 | Generator Loss: 14.205037 | Discriminator Loss: 0.127755\n","| Global Epoch: 34 | Local Epoch: 1 | Generator Loss: 14.443931 | Discriminator Loss: 0.072019\n","| Global Epoch: 34 | Local Epoch: 2 | Generator Loss: 14.773591 | Discriminator Loss: 0.050148\n","| Global Epoch: 34 | Local Epoch: 3 | Generator Loss: 14.995444 | Discriminator Loss: 0.038350\n","| Global Epoch: 34 | Local Epoch: 4 | Generator Loss: 15.135101 | Discriminator Loss: 0.031052\n","| Global Epoch: 34 | Local Epoch: 5 | Generator Loss: 15.224225 | Discriminator Loss: 0.026117\n","| Global Epoch: 34 | Local Epoch: 6 | Generator Loss: 15.353912 | Discriminator Loss: 0.022568\n","| Global Epoch: 34 | Local Epoch: 7 | Generator Loss: 15.444044 | Discriminator Loss: 0.019879\n","| Global Epoch: 34 | Local Epoch: 8 | Generator Loss: 15.559603 | Discriminator Loss: 0.017779\n","| Global Epoch: 34 | Local Epoch: 9 | Generator Loss: 15.642812 | Discriminator Loss: 0.016089\n","| Global Epoch: 34 | Local Epoch: 0 | Generator Loss: 15.701640 | Discriminator Loss: 0.080534\n","| Global Epoch: 34 | Local Epoch: 1 | Generator Loss: 15.435166 | Discriminator Loss: 0.046539\n","| Global Epoch: 34 | Local Epoch: 2 | Generator Loss: 15.529550 | Discriminator Loss: 0.032561\n","| Global Epoch: 34 | Local Epoch: 3 | Generator Loss: 15.727913 | Discriminator Loss: 0.025024\n","| Global Epoch: 34 | Local Epoch: 4 | Generator Loss: 15.833028 | Discriminator Loss: 0.020259\n","| Global Epoch: 34 | Local Epoch: 5 | Generator Loss: 15.993405 | Discriminator Loss: 0.017054\n","| Global Epoch: 34 | Local Epoch: 6 | Generator Loss: 16.153507 | Discriminator Loss: 0.014764\n","| Global Epoch: 34 | Local Epoch: 7 | Generator Loss: 15.971860 | Discriminator Loss: 0.024389\n","| Global Epoch: 34 | Local Epoch: 8 | Generator Loss: 15.906508 | Discriminator Loss: 0.030211\n","| Global Epoch: 34 | Local Epoch: 9 | Generator Loss: 15.878689 | Discriminator Loss: 0.038316\n","| Global Epoch: 34 | Local Epoch: 0 | Generator Loss: 15.325308 | Discriminator Loss: 0.619242\n","| Global Epoch: 34 | Local Epoch: 1 | Generator Loss: 15.555910 | Discriminator Loss: 0.325482\n","| Global Epoch: 34 | Local Epoch: 2 | Generator Loss: 15.501653 | Discriminator Loss: 0.221118\n","| Global Epoch: 34 | Local Epoch: 3 | Generator Loss: 15.651783 | Discriminator Loss: 0.167621\n","| Global Epoch: 34 | Local Epoch: 4 | Generator Loss: 15.977331 | Discriminator Loss: 0.143878\n","| Global Epoch: 34 | Local Epoch: 5 | Generator Loss: 16.347786 | Discriminator Loss: 0.121609\n","| Global Epoch: 34 | Local Epoch: 6 | Generator Loss: 16.694968 | Discriminator Loss: 0.106649\n","| Global Epoch: 34 | Local Epoch: 7 | Generator Loss: 16.765913 | Discriminator Loss: 0.094066\n","| Global Epoch: 34 | Local Epoch: 8 | Generator Loss: 16.734510 | Discriminator Loss: 0.083815\n","| Global Epoch: 34 | Local Epoch: 9 | Generator Loss: 16.676481 | Discriminator Loss: 0.075579\n","| Global Epoch: 34 | Local Epoch: 0 | Generator Loss: 16.129281 | Discriminator Loss: 0.787957\n","| Global Epoch: 34 | Local Epoch: 1 | Generator Loss: 15.476893 | Discriminator Loss: 0.458326\n","| Global Epoch: 34 | Local Epoch: 2 | Generator Loss: 15.176941 | Discriminator Loss: 0.317166\n","| Global Epoch: 34 | Local Epoch: 3 | Generator Loss: 15.176611 | Discriminator Loss: 0.241549\n","| Global Epoch: 34 | Local Epoch: 4 | Generator Loss: 15.115283 | Discriminator Loss: 0.195180\n","| Global Epoch: 34 | Local Epoch: 5 | Generator Loss: 15.117110 | Discriminator Loss: 0.163921\n","| Global Epoch: 34 | Local Epoch: 6 | Generator Loss: 15.092074 | Discriminator Loss: 0.141341\n","| Global Epoch: 34 | Local Epoch: 7 | Generator Loss: 15.292632 | Discriminator Loss: 0.124537\n","| Global Epoch: 34 | Local Epoch: 8 | Generator Loss: 15.448486 | Discriminator Loss: 0.111300\n","| Global Epoch: 34 | Local Epoch: 9 | Generator Loss: 15.531444 | Discriminator Loss: 0.100557\n","After 35 epoch global training, averged local generator loss is: 16.4075, averged local discrimitor loss is: 0.1042\n","Averged validation loss is: 6.6612\n","| Global Epoch: 35 | Local Epoch: 0 | Generator Loss: 16.197679 | Discriminator Loss: 0.370888\n","| Global Epoch: 35 | Local Epoch: 1 | Generator Loss: 15.576780 | Discriminator Loss: 0.207704\n","| Global Epoch: 35 | Local Epoch: 2 | Generator Loss: 15.269931 | Discriminator Loss: 0.142210\n","| Global Epoch: 35 | Local Epoch: 3 | Generator Loss: 15.031401 | Discriminator Loss: 0.108162\n","| Global Epoch: 35 | Local Epoch: 4 | Generator Loss: 15.112676 | Discriminator Loss: 0.088699\n","| Global Epoch: 35 | Local Epoch: 5 | Generator Loss: 15.150133 | Discriminator Loss: 0.074708\n","| Global Epoch: 35 | Local Epoch: 6 | Generator Loss: 15.176538 | Discriminator Loss: 0.064510\n","| Global Epoch: 35 | Local Epoch: 7 | Generator Loss: 15.301945 | Discriminator Loss: 0.056811\n","| Global Epoch: 35 | Local Epoch: 8 | Generator Loss: 15.474449 | Discriminator Loss: 0.050776\n","| Global Epoch: 35 | Local Epoch: 9 | Generator Loss: 15.549736 | Discriminator Loss: 0.045885\n","| Global Epoch: 35 | Local Epoch: 0 | Generator Loss: 16.119949 | Discriminator Loss: 0.487904\n","| Global Epoch: 35 | Local Epoch: 1 | Generator Loss: 14.909323 | Discriminator Loss: 0.273112\n","| Global Epoch: 35 | Local Epoch: 2 | Generator Loss: 14.732376 | Discriminator Loss: 0.186803\n","| Global Epoch: 35 | Local Epoch: 3 | Generator Loss: 14.543059 | Discriminator Loss: 0.141982\n","| Global Epoch: 35 | Local Epoch: 4 | Generator Loss: 14.480653 | Discriminator Loss: 0.114827\n","| Global Epoch: 35 | Local Epoch: 5 | Generator Loss: 14.601646 | Discriminator Loss: 0.096605\n","| Global Epoch: 35 | Local Epoch: 6 | Generator Loss: 14.614642 | Discriminator Loss: 0.083987\n","| Global Epoch: 35 | Local Epoch: 7 | Generator Loss: 14.766470 | Discriminator Loss: 0.076385\n","| Global Epoch: 35 | Local Epoch: 8 | Generator Loss: 14.715519 | Discriminator Loss: 0.079177\n","| Global Epoch: 35 | Local Epoch: 9 | Generator Loss: 14.821769 | Discriminator Loss: 0.073449\n","| Global Epoch: 35 | Local Epoch: 0 | Generator Loss: 14.622527 | Discriminator Loss: 0.342191\n","| Global Epoch: 35 | Local Epoch: 1 | Generator Loss: 14.057619 | Discriminator Loss: 0.196036\n","| Global Epoch: 35 | Local Epoch: 2 | Generator Loss: 14.011538 | Discriminator Loss: 0.135045\n","| Global Epoch: 35 | Local Epoch: 3 | Generator Loss: 13.964430 | Discriminator Loss: 0.102873\n","| Global Epoch: 35 | Local Epoch: 4 | Generator Loss: 14.067510 | Discriminator Loss: 0.084300\n","| Global Epoch: 35 | Local Epoch: 5 | Generator Loss: 14.078369 | Discriminator Loss: 0.070947\n","| Global Epoch: 35 | Local Epoch: 6 | Generator Loss: 14.258660 | Discriminator Loss: 0.061610\n","| Global Epoch: 35 | Local Epoch: 7 | Generator Loss: 14.449714 | Discriminator Loss: 0.054407\n","| Global Epoch: 35 | Local Epoch: 8 | Generator Loss: 14.609613 | Discriminator Loss: 0.048720\n","| Global Epoch: 35 | Local Epoch: 9 | Generator Loss: 14.770376 | Discriminator Loss: 0.044115\n","| Global Epoch: 35 | Local Epoch: 0 | Generator Loss: 14.877641 | Discriminator Loss: 0.105024\n","| Global Epoch: 35 | Local Epoch: 1 | Generator Loss: 15.039373 | Discriminator Loss: 0.060635\n","| Global Epoch: 35 | Local Epoch: 2 | Generator Loss: 15.079721 | Discriminator Loss: 0.046728\n","| Global Epoch: 35 | Local Epoch: 3 | Generator Loss: 15.191131 | Discriminator Loss: 0.035990\n","| Global Epoch: 35 | Local Epoch: 4 | Generator Loss: 15.386841 | Discriminator Loss: 0.029305\n","| Global Epoch: 35 | Local Epoch: 5 | Generator Loss: 15.629239 | Discriminator Loss: 0.024678\n","| Global Epoch: 35 | Local Epoch: 6 | Generator Loss: 15.811299 | Discriminator Loss: 0.021339\n","| Global Epoch: 35 | Local Epoch: 7 | Generator Loss: 15.789863 | Discriminator Loss: 0.021777\n","| Global Epoch: 35 | Local Epoch: 8 | Generator Loss: 15.638991 | Discriminator Loss: 0.034568\n","| Global Epoch: 35 | Local Epoch: 9 | Generator Loss: 15.664193 | Discriminator Loss: 0.036785\n","| Global Epoch: 35 | Local Epoch: 0 | Generator Loss: 15.293037 | Discriminator Loss: 0.455440\n","| Global Epoch: 35 | Local Epoch: 1 | Generator Loss: 14.980062 | Discriminator Loss: 0.252903\n","| Global Epoch: 35 | Local Epoch: 2 | Generator Loss: 14.944229 | Discriminator Loss: 0.173686\n","| Global Epoch: 35 | Local Epoch: 3 | Generator Loss: 14.866301 | Discriminator Loss: 0.132208\n","| Global Epoch: 35 | Local Epoch: 4 | Generator Loss: 14.796600 | Discriminator Loss: 0.106808\n","| Global Epoch: 35 | Local Epoch: 5 | Generator Loss: 14.983003 | Discriminator Loss: 0.089858\n","| Global Epoch: 35 | Local Epoch: 6 | Generator Loss: 15.091771 | Discriminator Loss: 0.077523\n","| Global Epoch: 35 | Local Epoch: 7 | Generator Loss: 15.126201 | Discriminator Loss: 0.068196\n","| Global Epoch: 35 | Local Epoch: 8 | Generator Loss: 15.163178 | Discriminator Loss: 0.060899\n","| Global Epoch: 35 | Local Epoch: 9 | Generator Loss: 15.210315 | Discriminator Loss: 0.055036\n","After 36 epoch global training, averged local generator loss is: 16.3743, averged local discrimitor loss is: 0.1029\n","Averged validation loss is: 5.2509\n","| Global Epoch: 36 | Local Epoch: 0 | Generator Loss: 16.224273 | Discriminator Loss: 0.136538\n","| Global Epoch: 36 | Local Epoch: 1 | Generator Loss: 15.849009 | Discriminator Loss: 0.073390\n","| Global Epoch: 36 | Local Epoch: 2 | Generator Loss: 15.811004 | Discriminator Loss: 0.050379\n","| Global Epoch: 36 | Local Epoch: 3 | Generator Loss: 15.906180 | Discriminator Loss: 0.038511\n","| Global Epoch: 36 | Local Epoch: 4 | Generator Loss: 15.970584 | Discriminator Loss: 0.031118\n","| Global Epoch: 36 | Local Epoch: 5 | Generator Loss: 15.944162 | Discriminator Loss: 0.026140\n","| Global Epoch: 36 | Local Epoch: 6 | Generator Loss: 15.902309 | Discriminator Loss: 0.022508\n","| Global Epoch: 36 | Local Epoch: 7 | Generator Loss: 15.875047 | Discriminator Loss: 0.019774\n","| Global Epoch: 36 | Local Epoch: 8 | Generator Loss: 15.786049 | Discriminator Loss: 0.017639\n","| Global Epoch: 36 | Local Epoch: 9 | Generator Loss: 15.799718 | Discriminator Loss: 0.015921\n","| Global Epoch: 36 | Local Epoch: 0 | Generator Loss: 15.012277 | Discriminator Loss: 0.193346\n","| Global Epoch: 36 | Local Epoch: 1 | Generator Loss: 14.885570 | Discriminator Loss: 0.108869\n","| Global Epoch: 36 | Local Epoch: 2 | Generator Loss: 14.813833 | Discriminator Loss: 0.082088\n","| Global Epoch: 36 | Local Epoch: 3 | Generator Loss: 14.919238 | Discriminator Loss: 0.082136\n","| Global Epoch: 36 | Local Epoch: 4 | Generator Loss: 15.017419 | Discriminator Loss: 0.070342\n","| Global Epoch: 36 | Local Epoch: 5 | Generator Loss: 15.035442 | Discriminator Loss: 0.059838\n","| Global Epoch: 36 | Local Epoch: 6 | Generator Loss: 15.195928 | Discriminator Loss: 0.053265\n","| Global Epoch: 36 | Local Epoch: 7 | Generator Loss: 15.377655 | Discriminator Loss: 0.047094\n","| Global Epoch: 36 | Local Epoch: 8 | Generator Loss: 15.343694 | Discriminator Loss: 0.042756\n","| Global Epoch: 36 | Local Epoch: 9 | Generator Loss: 15.375801 | Discriminator Loss: 0.048480\n","| Global Epoch: 36 | Local Epoch: 0 | Generator Loss: 15.956416 | Discriminator Loss: 0.234000\n","| Global Epoch: 36 | Local Epoch: 1 | Generator Loss: 16.118993 | Discriminator Loss: 0.132507\n","| Global Epoch: 36 | Local Epoch: 2 | Generator Loss: 16.126325 | Discriminator Loss: 0.092286\n","| Global Epoch: 36 | Local Epoch: 3 | Generator Loss: 15.858499 | Discriminator Loss: 0.074132\n","| Global Epoch: 36 | Local Epoch: 4 | Generator Loss: 15.849621 | Discriminator Loss: 0.060300\n","| Global Epoch: 36 | Local Epoch: 5 | Generator Loss: 15.865920 | Discriminator Loss: 0.050712\n","| Global Epoch: 36 | Local Epoch: 6 | Generator Loss: 15.880872 | Discriminator Loss: 0.043807\n","| Global Epoch: 36 | Local Epoch: 7 | Generator Loss: 15.859295 | Discriminator Loss: 0.038634\n","| Global Epoch: 36 | Local Epoch: 8 | Generator Loss: 15.862373 | Discriminator Loss: 0.034516\n","| Global Epoch: 36 | Local Epoch: 9 | Generator Loss: 15.881284 | Discriminator Loss: 0.031207\n","| Global Epoch: 36 | Local Epoch: 0 | Generator Loss: 16.232695 | Discriminator Loss: 0.078016\n","| Global Epoch: 36 | Local Epoch: 1 | Generator Loss: 16.065224 | Discriminator Loss: 0.044862\n","| Global Epoch: 36 | Local Epoch: 2 | Generator Loss: 16.138266 | Discriminator Loss: 0.030897\n","| Global Epoch: 36 | Local Epoch: 3 | Generator Loss: 16.378741 | Discriminator Loss: 0.023532\n","| Global Epoch: 36 | Local Epoch: 4 | Generator Loss: 16.504920 | Discriminator Loss: 0.019035\n","| Global Epoch: 36 | Local Epoch: 5 | Generator Loss: 16.792201 | Discriminator Loss: 0.016041\n","| Global Epoch: 36 | Local Epoch: 6 | Generator Loss: 16.868231 | Discriminator Loss: 0.013855\n","| Global Epoch: 36 | Local Epoch: 7 | Generator Loss: 16.985854 | Discriminator Loss: 0.012210\n","| Global Epoch: 36 | Local Epoch: 8 | Generator Loss: 17.071474 | Discriminator Loss: 0.011063\n","| Global Epoch: 36 | Local Epoch: 9 | Generator Loss: 16.829098 | Discriminator Loss: 0.029803\n","| Global Epoch: 36 | Local Epoch: 0 | Generator Loss: 13.960649 | Discriminator Loss: 0.392262\n","| Global Epoch: 36 | Local Epoch: 1 | Generator Loss: 14.080978 | Discriminator Loss: 0.219834\n","| Global Epoch: 36 | Local Epoch: 2 | Generator Loss: 14.299305 | Discriminator Loss: 0.153852\n","| Global Epoch: 36 | Local Epoch: 3 | Generator Loss: 14.370482 | Discriminator Loss: 0.117739\n","| Global Epoch: 36 | Local Epoch: 4 | Generator Loss: 14.642669 | Discriminator Loss: 0.095542\n","| Global Epoch: 36 | Local Epoch: 5 | Generator Loss: 14.708225 | Discriminator Loss: 0.082563\n","| Global Epoch: 36 | Local Epoch: 6 | Generator Loss: 14.873232 | Discriminator Loss: 0.071514\n","| Global Epoch: 36 | Local Epoch: 7 | Generator Loss: 15.053549 | Discriminator Loss: 0.063005\n","| Global Epoch: 36 | Local Epoch: 8 | Generator Loss: 15.175939 | Discriminator Loss: 0.056287\n","| Global Epoch: 36 | Local Epoch: 9 | Generator Loss: 15.259771 | Discriminator Loss: 0.050867\n","After 37 epoch global training, averged local generator loss is: 16.3441, averged local discrimitor loss is: 0.1015\n","Averged validation loss is: 5.0937\n","| Global Epoch: 37 | Local Epoch: 0 | Generator Loss: 16.045749 | Discriminator Loss: 0.090358\n","| Global Epoch: 37 | Local Epoch: 1 | Generator Loss: 16.212691 | Discriminator Loss: 0.049090\n","| Global Epoch: 37 | Local Epoch: 2 | Generator Loss: 16.306704 | Discriminator Loss: 0.034591\n","| Global Epoch: 37 | Local Epoch: 3 | Generator Loss: 16.100741 | Discriminator Loss: 0.026458\n","| Global Epoch: 37 | Local Epoch: 4 | Generator Loss: 15.914945 | Discriminator Loss: 0.021422\n","| Global Epoch: 37 | Local Epoch: 5 | Generator Loss: 15.744150 | Discriminator Loss: 0.018012\n","| Global Epoch: 37 | Local Epoch: 6 | Generator Loss: 15.492383 | Discriminator Loss: 0.015534\n","| Global Epoch: 37 | Local Epoch: 7 | Generator Loss: 15.472598 | Discriminator Loss: 0.013662\n","| Global Epoch: 37 | Local Epoch: 8 | Generator Loss: 15.496158 | Discriminator Loss: 0.012185\n","| Global Epoch: 37 | Local Epoch: 9 | Generator Loss: 15.478272 | Discriminator Loss: 0.011021\n","| Global Epoch: 37 | Local Epoch: 0 | Generator Loss: 16.611351 | Discriminator Loss: 0.442761\n","| Global Epoch: 37 | Local Epoch: 1 | Generator Loss: 16.568658 | Discriminator Loss: 0.251048\n","| Global Epoch: 37 | Local Epoch: 2 | Generator Loss: 16.254030 | Discriminator Loss: 0.172384\n","| Global Epoch: 37 | Local Epoch: 3 | Generator Loss: 16.075080 | Discriminator Loss: 0.131415\n","| Global Epoch: 37 | Local Epoch: 4 | Generator Loss: 15.953648 | Discriminator Loss: 0.106247\n","| Global Epoch: 37 | Local Epoch: 5 | Generator Loss: 15.999841 | Discriminator Loss: 0.090649\n","| Global Epoch: 37 | Local Epoch: 6 | Generator Loss: 15.949630 | Discriminator Loss: 0.078336\n","| Global Epoch: 37 | Local Epoch: 7 | Generator Loss: 15.910648 | Discriminator Loss: 0.068963\n","| Global Epoch: 37 | Local Epoch: 8 | Generator Loss: 15.917323 | Discriminator Loss: 0.061619\n","| Global Epoch: 37 | Local Epoch: 9 | Generator Loss: 15.956135 | Discriminator Loss: 0.055732\n","| Global Epoch: 37 | Local Epoch: 0 | Generator Loss: 16.536274 | Discriminator Loss: 0.272018\n","| Global Epoch: 37 | Local Epoch: 1 | Generator Loss: 16.409688 | Discriminator Loss: 0.152852\n","| Global Epoch: 37 | Local Epoch: 2 | Generator Loss: 16.256271 | Discriminator Loss: 0.104438\n","| Global Epoch: 37 | Local Epoch: 3 | Generator Loss: 16.114314 | Discriminator Loss: 0.079358\n","| Global Epoch: 37 | Local Epoch: 4 | Generator Loss: 15.949597 | Discriminator Loss: 0.064083\n","| Global Epoch: 37 | Local Epoch: 5 | Generator Loss: 16.042205 | Discriminator Loss: 0.053917\n","| Global Epoch: 37 | Local Epoch: 6 | Generator Loss: 16.177619 | Discriminator Loss: 0.046553\n","| Global Epoch: 37 | Local Epoch: 7 | Generator Loss: 16.246567 | Discriminator Loss: 0.040967\n","| Global Epoch: 37 | Local Epoch: 8 | Generator Loss: 16.235430 | Discriminator Loss: 0.036592\n","| Global Epoch: 37 | Local Epoch: 9 | Generator Loss: 16.229831 | Discriminator Loss: 0.033087\n","| Global Epoch: 37 | Local Epoch: 0 | Generator Loss: 15.556027 | Discriminator Loss: 0.044149\n","| Global Epoch: 37 | Local Epoch: 1 | Generator Loss: 15.953768 | Discriminator Loss: 0.026613\n","| Global Epoch: 37 | Local Epoch: 2 | Generator Loss: 15.865462 | Discriminator Loss: 0.018685\n","| Global Epoch: 37 | Local Epoch: 3 | Generator Loss: 15.696866 | Discriminator Loss: 0.019872\n","| Global Epoch: 37 | Local Epoch: 4 | Generator Loss: 15.598878 | Discriminator Loss: 0.020095\n","| Global Epoch: 37 | Local Epoch: 5 | Generator Loss: 15.597091 | Discriminator Loss: 0.018677\n","| Global Epoch: 37 | Local Epoch: 6 | Generator Loss: 15.588800 | Discriminator Loss: 0.021110\n","| Global Epoch: 37 | Local Epoch: 7 | Generator Loss: 15.518342 | Discriminator Loss: 0.036595\n","| Global Epoch: 37 | Local Epoch: 8 | Generator Loss: 15.502588 | Discriminator Loss: 0.034076\n","| Global Epoch: 37 | Local Epoch: 9 | Generator Loss: 15.582654 | Discriminator Loss: 0.031151\n","| Global Epoch: 37 | Local Epoch: 0 | Generator Loss: 15.353997 | Discriminator Loss: 0.146356\n","| Global Epoch: 37 | Local Epoch: 1 | Generator Loss: 15.373375 | Discriminator Loss: 0.081536\n","| Global Epoch: 37 | Local Epoch: 2 | Generator Loss: 15.862586 | Discriminator Loss: 0.055925\n","| Global Epoch: 37 | Local Epoch: 3 | Generator Loss: 16.125955 | Discriminator Loss: 0.042670\n","| Global Epoch: 37 | Local Epoch: 4 | Generator Loss: 15.973088 | Discriminator Loss: 0.048452\n","| Global Epoch: 37 | Local Epoch: 5 | Generator Loss: 15.913935 | Discriminator Loss: 0.045677\n","| Global Epoch: 37 | Local Epoch: 6 | Generator Loss: 16.007062 | Discriminator Loss: 0.039956\n","| Global Epoch: 37 | Local Epoch: 7 | Generator Loss: 16.041181 | Discriminator Loss: 0.035210\n","| Global Epoch: 37 | Local Epoch: 8 | Generator Loss: 16.080590 | Discriminator Loss: 0.031696\n","| Global Epoch: 37 | Local Epoch: 9 | Generator Loss: 16.071922 | Discriminator Loss: 0.028921\n","After 38 epoch global training, averged local generator loss is: 16.3370, averged local discrimitor loss is: 0.0996\n","Averged validation loss is: 6.0704\n","| Global Epoch: 38 | Local Epoch: 0 | Generator Loss: 16.314259 | Discriminator Loss: 0.213323\n","| Global Epoch: 38 | Local Epoch: 1 | Generator Loss: 16.263817 | Discriminator Loss: 0.119254\n","| Global Epoch: 38 | Local Epoch: 2 | Generator Loss: 16.397025 | Discriminator Loss: 0.081874\n","| Global Epoch: 38 | Local Epoch: 3 | Generator Loss: 16.561979 | Discriminator Loss: 0.065297\n","| Global Epoch: 38 | Local Epoch: 4 | Generator Loss: 16.456907 | Discriminator Loss: 0.053828\n","| Global Epoch: 38 | Local Epoch: 5 | Generator Loss: 16.520749 | Discriminator Loss: 0.045413\n","| Global Epoch: 38 | Local Epoch: 6 | Generator Loss: 16.549497 | Discriminator Loss: 0.039265\n","| Global Epoch: 38 | Local Epoch: 7 | Generator Loss: 16.512273 | Discriminator Loss: 0.034586\n","| Global Epoch: 38 | Local Epoch: 8 | Generator Loss: 16.626788 | Discriminator Loss: 0.031010\n","| Global Epoch: 38 | Local Epoch: 9 | Generator Loss: 16.640863 | Discriminator Loss: 0.028065\n","| Global Epoch: 38 | Local Epoch: 0 | Generator Loss: 15.953118 | Discriminator Loss: 0.037200\n","| Global Epoch: 38 | Local Epoch: 1 | Generator Loss: 15.938603 | Discriminator Loss: 0.026046\n","| Global Epoch: 38 | Local Epoch: 2 | Generator Loss: 16.351692 | Discriminator Loss: 0.020318\n","| Global Epoch: 38 | Local Epoch: 3 | Generator Loss: 16.425838 | Discriminator Loss: 0.016087\n","| Global Epoch: 38 | Local Epoch: 4 | Generator Loss: 16.364994 | Discriminator Loss: 0.037178\n","| Global Epoch: 38 | Local Epoch: 5 | Generator Loss: 16.300871 | Discriminator Loss: 0.035605\n","| Global Epoch: 38 | Local Epoch: 6 | Generator Loss: 16.285778 | Discriminator Loss: 0.031160\n","| Global Epoch: 38 | Local Epoch: 7 | Generator Loss: 16.266990 | Discriminator Loss: 0.027528\n","| Global Epoch: 38 | Local Epoch: 8 | Generator Loss: 16.313291 | Discriminator Loss: 0.024641\n","| Global Epoch: 38 | Local Epoch: 9 | Generator Loss: 16.352536 | Discriminator Loss: 0.022292\n","| Global Epoch: 38 | Local Epoch: 0 | Generator Loss: 15.805640 | Discriminator Loss: 0.136455\n","| Global Epoch: 38 | Local Epoch: 1 | Generator Loss: 15.829008 | Discriminator Loss: 0.076166\n","| Global Epoch: 38 | Local Epoch: 2 | Generator Loss: 15.533836 | Discriminator Loss: 0.063209\n","| Global Epoch: 38 | Local Epoch: 3 | Generator Loss: 15.541901 | Discriminator Loss: 0.049673\n","| Global Epoch: 38 | Local Epoch: 4 | Generator Loss: 15.751504 | Discriminator Loss: 0.042952\n","| Global Epoch: 38 | Local Epoch: 5 | Generator Loss: 15.809927 | Discriminator Loss: 0.036240\n","| Global Epoch: 38 | Local Epoch: 6 | Generator Loss: 15.858681 | Discriminator Loss: 0.035000\n","| Global Epoch: 38 | Local Epoch: 7 | Generator Loss: 15.666332 | Discriminator Loss: 0.050525\n","| Global Epoch: 38 | Local Epoch: 8 | Generator Loss: 15.778646 | Discriminator Loss: 0.047329\n","| Global Epoch: 38 | Local Epoch: 9 | Generator Loss: 15.863505 | Discriminator Loss: 0.043084\n","| Global Epoch: 38 | Local Epoch: 0 | Generator Loss: 16.292766 | Discriminator Loss: 0.095022\n","| Global Epoch: 38 | Local Epoch: 1 | Generator Loss: 16.458744 | Discriminator Loss: 0.054090\n","| Global Epoch: 38 | Local Epoch: 2 | Generator Loss: 16.341659 | Discriminator Loss: 0.038760\n","| Global Epoch: 38 | Local Epoch: 3 | Generator Loss: 16.362470 | Discriminator Loss: 0.029626\n","| Global Epoch: 38 | Local Epoch: 4 | Generator Loss: 16.445406 | Discriminator Loss: 0.024018\n","| Global Epoch: 38 | Local Epoch: 5 | Generator Loss: 16.592839 | Discriminator Loss: 0.020314\n","| Global Epoch: 38 | Local Epoch: 6 | Generator Loss: 16.717254 | Discriminator Loss: 0.017596\n","| Global Epoch: 38 | Local Epoch: 7 | Generator Loss: 16.810606 | Discriminator Loss: 0.015810\n","| Global Epoch: 38 | Local Epoch: 8 | Generator Loss: 16.696294 | Discriminator Loss: 0.026308\n","| Global Epoch: 38 | Local Epoch: 9 | Generator Loss: 16.645548 | Discriminator Loss: 0.029877\n","| Global Epoch: 38 | Local Epoch: 0 | Generator Loss: 16.217880 | Discriminator Loss: 0.383730\n","| Global Epoch: 38 | Local Epoch: 1 | Generator Loss: 16.205446 | Discriminator Loss: 0.210970\n","| Global Epoch: 38 | Local Epoch: 2 | Generator Loss: 16.070246 | Discriminator Loss: 0.144073\n","| Global Epoch: 38 | Local Epoch: 3 | Generator Loss: 16.234981 | Discriminator Loss: 0.111002\n","| Global Epoch: 38 | Local Epoch: 4 | Generator Loss: 16.180337 | Discriminator Loss: 0.089758\n","| Global Epoch: 38 | Local Epoch: 5 | Generator Loss: 16.146817 | Discriminator Loss: 0.075376\n","| Global Epoch: 38 | Local Epoch: 6 | Generator Loss: 16.158770 | Discriminator Loss: 0.065069\n","| Global Epoch: 38 | Local Epoch: 7 | Generator Loss: 16.263929 | Discriminator Loss: 0.057566\n","| Global Epoch: 38 | Local Epoch: 8 | Generator Loss: 16.157471 | Discriminator Loss: 0.051437\n","| Global Epoch: 38 | Local Epoch: 9 | Generator Loss: 16.137612 | Discriminator Loss: 0.046496\n","After 39 epoch global training, averged local generator loss is: 16.3319, averged local discrimitor loss is: 0.0982\n","Averged validation loss is: 6.6006\n","| Global Epoch: 39 | Local Epoch: 0 | Generator Loss: 15.594089 | Discriminator Loss: 0.455385\n","| Global Epoch: 39 | Local Epoch: 1 | Generator Loss: 15.009729 | Discriminator Loss: 0.258196\n","| Global Epoch: 39 | Local Epoch: 2 | Generator Loss: 14.887510 | Discriminator Loss: 0.178296\n","| Global Epoch: 39 | Local Epoch: 3 | Generator Loss: 14.806924 | Discriminator Loss: 0.136739\n","| Global Epoch: 39 | Local Epoch: 4 | Generator Loss: 14.775258 | Discriminator Loss: 0.112224\n","| Global Epoch: 39 | Local Epoch: 5 | Generator Loss: 14.815034 | Discriminator Loss: 0.094323\n","| Global Epoch: 39 | Local Epoch: 6 | Generator Loss: 14.829796 | Discriminator Loss: 0.081345\n","| Global Epoch: 39 | Local Epoch: 7 | Generator Loss: 14.836117 | Discriminator Loss: 0.071538\n","| Global Epoch: 39 | Local Epoch: 8 | Generator Loss: 14.891399 | Discriminator Loss: 0.063872\n","| Global Epoch: 39 | Local Epoch: 9 | Generator Loss: 15.022890 | Discriminator Loss: 0.057738\n","| Global Epoch: 39 | Local Epoch: 0 | Generator Loss: 14.893688 | Discriminator Loss: 0.232375\n","| Global Epoch: 39 | Local Epoch: 1 | Generator Loss: 15.001878 | Discriminator Loss: 0.227584\n","| Global Epoch: 39 | Local Epoch: 2 | Generator Loss: 14.537081 | Discriminator Loss: 0.179157\n","| Global Epoch: 39 | Local Epoch: 3 | Generator Loss: 14.334119 | Discriminator Loss: 0.147679\n","| Global Epoch: 39 | Local Epoch: 4 | Generator Loss: 14.153723 | Discriminator Loss: 0.130237\n","| Global Epoch: 39 | Local Epoch: 5 | Generator Loss: 14.246774 | Discriminator Loss: 0.121305\n","| Global Epoch: 39 | Local Epoch: 6 | Generator Loss: 14.029743 | Discriminator Loss: 0.115718\n","| Global Epoch: 39 | Local Epoch: 7 | Generator Loss: 13.996842 | Discriminator Loss: 0.106563\n","| Global Epoch: 39 | Local Epoch: 8 | Generator Loss: 14.050320 | Discriminator Loss: 0.095676\n","| Global Epoch: 39 | Local Epoch: 9 | Generator Loss: 14.095442 | Discriminator Loss: 0.086593\n","| Global Epoch: 39 | Local Epoch: 0 | Generator Loss: 14.270320 | Discriminator Loss: 0.151337\n","| Global Epoch: 39 | Local Epoch: 1 | Generator Loss: 14.230540 | Discriminator Loss: 0.086025\n","| Global Epoch: 39 | Local Epoch: 2 | Generator Loss: 14.558521 | Discriminator Loss: 0.059817\n","| Global Epoch: 39 | Local Epoch: 3 | Generator Loss: 14.839218 | Discriminator Loss: 0.045670\n","| Global Epoch: 39 | Local Epoch: 4 | Generator Loss: 14.998642 | Discriminator Loss: 0.036981\n","| Global Epoch: 39 | Local Epoch: 5 | Generator Loss: 15.061870 | Discriminator Loss: 0.031111\n","| Global Epoch: 39 | Local Epoch: 6 | Generator Loss: 14.962376 | Discriminator Loss: 0.043550\n","| Global Epoch: 39 | Local Epoch: 7 | Generator Loss: 14.884001 | Discriminator Loss: 0.047598\n","| Global Epoch: 39 | Local Epoch: 8 | Generator Loss: 15.066686 | Discriminator Loss: 0.044234\n","| Global Epoch: 39 | Local Epoch: 9 | Generator Loss: 15.157367 | Discriminator Loss: 0.040241\n","| Global Epoch: 39 | Local Epoch: 0 | Generator Loss: 15.012879 | Discriminator Loss: 0.371585\n","| Global Epoch: 39 | Local Epoch: 1 | Generator Loss: 14.793601 | Discriminator Loss: 0.206310\n","| Global Epoch: 39 | Local Epoch: 2 | Generator Loss: 15.044367 | Discriminator Loss: 0.141964\n","| Global Epoch: 39 | Local Epoch: 3 | Generator Loss: 14.972053 | Discriminator Loss: 0.107996\n","| Global Epoch: 39 | Local Epoch: 4 | Generator Loss: 15.083999 | Discriminator Loss: 0.087345\n","| Global Epoch: 39 | Local Epoch: 5 | Generator Loss: 15.185051 | Discriminator Loss: 0.074820\n","| Global Epoch: 39 | Local Epoch: 6 | Generator Loss: 15.121419 | Discriminator Loss: 0.067388\n","| Global Epoch: 39 | Local Epoch: 7 | Generator Loss: 15.034494 | Discriminator Loss: 0.059611\n","| Global Epoch: 39 | Local Epoch: 8 | Generator Loss: 15.049831 | Discriminator Loss: 0.053354\n","| Global Epoch: 39 | Local Epoch: 9 | Generator Loss: 15.055294 | Discriminator Loss: 0.048251\n","| Global Epoch: 39 | Local Epoch: 0 | Generator Loss: 14.833528 | Discriminator Loss: 0.080062\n","| Global Epoch: 39 | Local Epoch: 1 | Generator Loss: 14.733070 | Discriminator Loss: 0.045693\n","| Global Epoch: 39 | Local Epoch: 2 | Generator Loss: 14.915396 | Discriminator Loss: 0.032432\n","| Global Epoch: 39 | Local Epoch: 3 | Generator Loss: 14.754914 | Discriminator Loss: 0.031349\n","| Global Epoch: 39 | Local Epoch: 4 | Generator Loss: 14.758514 | Discriminator Loss: 0.025920\n","| Global Epoch: 39 | Local Epoch: 5 | Generator Loss: 15.018657 | Discriminator Loss: 0.022127\n","| Global Epoch: 39 | Local Epoch: 6 | Generator Loss: 15.129122 | Discriminator Loss: 0.019146\n","| Global Epoch: 39 | Local Epoch: 7 | Generator Loss: 15.185030 | Discriminator Loss: 0.016868\n","| Global Epoch: 39 | Local Epoch: 8 | Generator Loss: 15.306049 | Discriminator Loss: 0.015100\n","| Global Epoch: 39 | Local Epoch: 9 | Generator Loss: 15.402178 | Discriminator Loss: 0.013678\n","After 40 epoch global training, averged local generator loss is: 16.3086, averged local discrimitor loss is: 0.0961\n","Averged validation loss is: 6.8183\n","| Global Epoch: 40 | Local Epoch: 0 | Generator Loss: 15.858533 | Discriminator Loss: 0.443588\n","| Global Epoch: 40 | Local Epoch: 1 | Generator Loss: 15.029574 | Discriminator Loss: 0.248491\n","| Global Epoch: 40 | Local Epoch: 2 | Generator Loss: 14.581261 | Discriminator Loss: 0.170558\n","| Global Epoch: 40 | Local Epoch: 3 | Generator Loss: 14.690970 | Discriminator Loss: 0.131527\n","| Global Epoch: 40 | Local Epoch: 4 | Generator Loss: 14.840269 | Discriminator Loss: 0.108527\n","| Global Epoch: 40 | Local Epoch: 5 | Generator Loss: 14.823120 | Discriminator Loss: 0.091515\n","| Global Epoch: 40 | Local Epoch: 6 | Generator Loss: 14.935442 | Discriminator Loss: 0.078999\n","| Global Epoch: 40 | Local Epoch: 7 | Generator Loss: 14.977045 | Discriminator Loss: 0.069499\n","| Global Epoch: 40 | Local Epoch: 8 | Generator Loss: 15.052039 | Discriminator Loss: 0.062080\n","| Global Epoch: 40 | Local Epoch: 9 | Generator Loss: 15.089426 | Discriminator Loss: 0.056082\n","| Global Epoch: 40 | Local Epoch: 0 | Generator Loss: 15.255130 | Discriminator Loss: 0.062635\n","| Global Epoch: 40 | Local Epoch: 1 | Generator Loss: 14.852226 | Discriminator Loss: 0.044193\n","| Global Epoch: 40 | Local Epoch: 2 | Generator Loss: 15.179297 | Discriminator Loss: 0.031347\n","| Global Epoch: 40 | Local Epoch: 3 | Generator Loss: 15.223054 | Discriminator Loss: 0.023936\n","| Global Epoch: 40 | Local Epoch: 4 | Generator Loss: 15.427302 | Discriminator Loss: 0.020107\n","| Global Epoch: 40 | Local Epoch: 5 | Generator Loss: 15.546795 | Discriminator Loss: 0.016995\n","| Global Epoch: 40 | Local Epoch: 6 | Generator Loss: 15.721351 | Discriminator Loss: 0.014782\n","| Global Epoch: 40 | Local Epoch: 7 | Generator Loss: 15.868504 | Discriminator Loss: 0.013100\n","| Global Epoch: 40 | Local Epoch: 8 | Generator Loss: 15.998735 | Discriminator Loss: 0.011752\n","| Global Epoch: 40 | Local Epoch: 9 | Generator Loss: 16.060302 | Discriminator Loss: 0.010636\n","| Global Epoch: 40 | Local Epoch: 0 | Generator Loss: 16.578287 | Discriminator Loss: 0.170390\n","| Global Epoch: 40 | Local Epoch: 1 | Generator Loss: 16.244475 | Discriminator Loss: 0.096593\n","| Global Epoch: 40 | Local Epoch: 2 | Generator Loss: 15.926278 | Discriminator Loss: 0.066337\n","| Global Epoch: 40 | Local Epoch: 3 | Generator Loss: 15.785211 | Discriminator Loss: 0.050585\n","| Global Epoch: 40 | Local Epoch: 4 | Generator Loss: 15.678205 | Discriminator Loss: 0.040941\n","| Global Epoch: 40 | Local Epoch: 5 | Generator Loss: 15.596161 | Discriminator Loss: 0.034424\n","| Global Epoch: 40 | Local Epoch: 6 | Generator Loss: 15.646407 | Discriminator Loss: 0.029754\n","| Global Epoch: 40 | Local Epoch: 7 | Generator Loss: 15.647264 | Discriminator Loss: 0.026222\n","| Global Epoch: 40 | Local Epoch: 8 | Generator Loss: 15.705834 | Discriminator Loss: 0.023456\n","| Global Epoch: 40 | Local Epoch: 9 | Generator Loss: 15.702450 | Discriminator Loss: 0.021213\n","| Global Epoch: 40 | Local Epoch: 0 | Generator Loss: 16.097451 | Discriminator Loss: 0.056313\n","| Global Epoch: 40 | Local Epoch: 1 | Generator Loss: 15.713878 | Discriminator Loss: 0.031960\n","| Global Epoch: 40 | Local Epoch: 2 | Generator Loss: 15.646792 | Discriminator Loss: 0.022100\n","| Global Epoch: 40 | Local Epoch: 3 | Generator Loss: 15.636488 | Discriminator Loss: 0.016877\n","| Global Epoch: 40 | Local Epoch: 4 | Generator Loss: 15.599139 | Discriminator Loss: 0.013693\n","| Global Epoch: 40 | Local Epoch: 5 | Generator Loss: 15.612618 | Discriminator Loss: 0.011549\n","| Global Epoch: 40 | Local Epoch: 6 | Generator Loss: 15.665618 | Discriminator Loss: 0.010672\n","| Global Epoch: 40 | Local Epoch: 7 | Generator Loss: 15.415465 | Discriminator Loss: 0.010966\n","| Global Epoch: 40 | Local Epoch: 8 | Generator Loss: 15.423890 | Discriminator Loss: 0.012377\n","| Global Epoch: 40 | Local Epoch: 9 | Generator Loss: 15.425619 | Discriminator Loss: 0.013310\n","| Global Epoch: 40 | Local Epoch: 0 | Generator Loss: 14.552413 | Discriminator Loss: 0.169343\n","| Global Epoch: 40 | Local Epoch: 1 | Generator Loss: 14.495135 | Discriminator Loss: 0.108421\n","| Global Epoch: 40 | Local Epoch: 2 | Generator Loss: 14.549916 | Discriminator Loss: 0.106285\n","| Global Epoch: 40 | Local Epoch: 3 | Generator Loss: 14.318127 | Discriminator Loss: 0.106447\n","| Global Epoch: 40 | Local Epoch: 4 | Generator Loss: 14.007493 | Discriminator Loss: 0.089104\n","| Global Epoch: 40 | Local Epoch: 5 | Generator Loss: 14.092014 | Discriminator Loss: 0.075291\n","| Global Epoch: 40 | Local Epoch: 6 | Generator Loss: 14.202484 | Discriminator Loss: 0.065028\n","| Global Epoch: 40 | Local Epoch: 7 | Generator Loss: 14.284147 | Discriminator Loss: 0.074987\n","| Global Epoch: 40 | Local Epoch: 8 | Generator Loss: 14.084884 | Discriminator Loss: 0.077135\n","| Global Epoch: 40 | Local Epoch: 9 | Generator Loss: 14.089061 | Discriminator Loss: 0.072043\n","After 41 epoch global training, averged local generator loss is: 16.2545, averged local discrimitor loss is: 0.0955\n","Averged validation loss is: 7.9791\n","| Global Epoch: 41 | Local Epoch: 0 | Generator Loss: 14.206772 | Discriminator Loss: 0.084310\n","| Global Epoch: 41 | Local Epoch: 1 | Generator Loss: 14.322027 | Discriminator Loss: 0.050168\n","| Global Epoch: 41 | Local Epoch: 2 | Generator Loss: 14.466619 | Discriminator Loss: 0.034834\n","| Global Epoch: 41 | Local Epoch: 3 | Generator Loss: 14.562394 | Discriminator Loss: 0.026881\n","| Global Epoch: 41 | Local Epoch: 4 | Generator Loss: 14.643403 | Discriminator Loss: 0.022972\n","| Global Epoch: 41 | Local Epoch: 5 | Generator Loss: 14.719995 | Discriminator Loss: 0.023871\n","| Global Epoch: 41 | Local Epoch: 6 | Generator Loss: 14.779293 | Discriminator Loss: 0.021305\n","| Global Epoch: 41 | Local Epoch: 7 | Generator Loss: 14.746799 | Discriminator Loss: 0.028688\n","| Global Epoch: 41 | Local Epoch: 8 | Generator Loss: 14.802439 | Discriminator Loss: 0.026998\n","| Global Epoch: 41 | Local Epoch: 9 | Generator Loss: 14.966796 | Discriminator Loss: 0.024659\n","| Global Epoch: 41 | Local Epoch: 0 | Generator Loss: 15.703738 | Discriminator Loss: 0.287511\n","| Global Epoch: 41 | Local Epoch: 1 | Generator Loss: 15.359172 | Discriminator Loss: 0.159752\n","| Global Epoch: 41 | Local Epoch: 2 | Generator Loss: 15.224309 | Discriminator Loss: 0.109527\n","| Global Epoch: 41 | Local Epoch: 3 | Generator Loss: 15.305968 | Discriminator Loss: 0.083787\n","| Global Epoch: 41 | Local Epoch: 4 | Generator Loss: 15.184475 | Discriminator Loss: 0.067843\n","| Global Epoch: 41 | Local Epoch: 5 | Generator Loss: 15.272828 | Discriminator Loss: 0.057059\n","| Global Epoch: 41 | Local Epoch: 6 | Generator Loss: 15.246831 | Discriminator Loss: 0.068642\n","| Global Epoch: 41 | Local Epoch: 7 | Generator Loss: 15.284405 | Discriminator Loss: 0.068051\n","| Global Epoch: 41 | Local Epoch: 8 | Generator Loss: 15.323651 | Discriminator Loss: 0.061643\n","| Global Epoch: 41 | Local Epoch: 9 | Generator Loss: 15.393009 | Discriminator Loss: 0.055890\n","| Global Epoch: 41 | Local Epoch: 0 | Generator Loss: 15.251051 | Discriminator Loss: 0.211019\n","| Global Epoch: 41 | Local Epoch: 1 | Generator Loss: 15.093400 | Discriminator Loss: 0.118014\n","| Global Epoch: 41 | Local Epoch: 2 | Generator Loss: 15.039611 | Discriminator Loss: 0.080873\n","| Global Epoch: 41 | Local Epoch: 3 | Generator Loss: 15.197040 | Discriminator Loss: 0.061718\n","| Global Epoch: 41 | Local Epoch: 4 | Generator Loss: 15.296524 | Discriminator Loss: 0.049909\n","| Global Epoch: 41 | Local Epoch: 5 | Generator Loss: 15.382252 | Discriminator Loss: 0.042124\n","| Global Epoch: 41 | Local Epoch: 6 | Generator Loss: 15.570271 | Discriminator Loss: 0.036533\n","| Global Epoch: 41 | Local Epoch: 7 | Generator Loss: 15.683071 | Discriminator Loss: 0.032182\n","| Global Epoch: 41 | Local Epoch: 8 | Generator Loss: 15.760999 | Discriminator Loss: 0.028759\n","| Global Epoch: 41 | Local Epoch: 9 | Generator Loss: 15.839600 | Discriminator Loss: 0.026070\n","| Global Epoch: 41 | Local Epoch: 0 | Generator Loss: 16.110130 | Discriminator Loss: 0.080013\n","| Global Epoch: 41 | Local Epoch: 1 | Generator Loss: 15.872403 | Discriminator Loss: 0.058561\n","| Global Epoch: 41 | Local Epoch: 2 | Generator Loss: 16.081729 | Discriminator Loss: 0.046347\n","| Global Epoch: 41 | Local Epoch: 3 | Generator Loss: 15.908321 | Discriminator Loss: 0.048726\n","| Global Epoch: 41 | Local Epoch: 4 | Generator Loss: 15.654955 | Discriminator Loss: 0.064027\n","| Global Epoch: 41 | Local Epoch: 5 | Generator Loss: 15.588767 | Discriminator Loss: 0.068290\n","| Global Epoch: 41 | Local Epoch: 6 | Generator Loss: 15.555920 | Discriminator Loss: 0.082097\n","| Global Epoch: 41 | Local Epoch: 7 | Generator Loss: 15.138948 | Discriminator Loss: 0.093670\n","| Global Epoch: 41 | Local Epoch: 8 | Generator Loss: 14.972314 | Discriminator Loss: 0.084776\n","| Global Epoch: 41 | Local Epoch: 9 | Generator Loss: 14.831159 | Discriminator Loss: 0.076707\n","| Global Epoch: 41 | Local Epoch: 0 | Generator Loss: 14.481965 | Discriminator Loss: 0.212763\n","| Global Epoch: 41 | Local Epoch: 1 | Generator Loss: 14.627848 | Discriminator Loss: 0.118409\n","| Global Epoch: 41 | Local Epoch: 2 | Generator Loss: 14.778147 | Discriminator Loss: 0.081246\n","| Global Epoch: 41 | Local Epoch: 3 | Generator Loss: 14.874312 | Discriminator Loss: 0.061764\n","| Global Epoch: 41 | Local Epoch: 4 | Generator Loss: 14.824071 | Discriminator Loss: 0.049902\n","| Global Epoch: 41 | Local Epoch: 5 | Generator Loss: 14.856145 | Discriminator Loss: 0.041892\n","| Global Epoch: 41 | Local Epoch: 6 | Generator Loss: 14.906777 | Discriminator Loss: 0.038915\n","| Global Epoch: 41 | Local Epoch: 7 | Generator Loss: 14.767212 | Discriminator Loss: 0.037916\n","| Global Epoch: 41 | Local Epoch: 8 | Generator Loss: 14.803678 | Discriminator Loss: 0.040921\n","| Global Epoch: 41 | Local Epoch: 9 | Generator Loss: 14.941427 | Discriminator Loss: 0.040161\n","After 42 epoch global training, averged local generator loss is: 16.2232, averged local discrimitor loss is: 0.0942\n","Averged validation loss is: 7.3983\n","| Global Epoch: 42 | Local Epoch: 0 | Generator Loss: 15.673000 | Discriminator Loss: 0.141661\n","| Global Epoch: 42 | Local Epoch: 1 | Generator Loss: 15.333295 | Discriminator Loss: 0.079914\n","| Global Epoch: 42 | Local Epoch: 2 | Generator Loss: 15.372466 | Discriminator Loss: 0.055048\n","| Global Epoch: 42 | Local Epoch: 3 | Generator Loss: 15.374399 | Discriminator Loss: 0.047752\n","| Global Epoch: 42 | Local Epoch: 4 | Generator Loss: 15.339342 | Discriminator Loss: 0.039137\n","| Global Epoch: 42 | Local Epoch: 5 | Generator Loss: 15.409878 | Discriminator Loss: 0.032998\n","| Global Epoch: 42 | Local Epoch: 6 | Generator Loss: 15.513505 | Discriminator Loss: 0.028545\n","| Global Epoch: 42 | Local Epoch: 7 | Generator Loss: 15.557401 | Discriminator Loss: 0.025146\n","| Global Epoch: 42 | Local Epoch: 8 | Generator Loss: 15.587344 | Discriminator Loss: 0.022476\n","| Global Epoch: 42 | Local Epoch: 9 | Generator Loss: 15.611237 | Discriminator Loss: 0.020328\n","| Global Epoch: 42 | Local Epoch: 0 | Generator Loss: 16.334800 | Discriminator Loss: 0.301333\n","| Global Epoch: 42 | Local Epoch: 1 | Generator Loss: 15.651023 | Discriminator Loss: 0.170589\n","| Global Epoch: 42 | Local Epoch: 2 | Generator Loss: 15.663594 | Discriminator Loss: 0.117449\n","| Global Epoch: 42 | Local Epoch: 3 | Generator Loss: 15.743468 | Discriminator Loss: 0.089530\n","| Global Epoch: 42 | Local Epoch: 4 | Generator Loss: 15.701679 | Discriminator Loss: 0.072341\n","| Global Epoch: 42 | Local Epoch: 5 | Generator Loss: 15.635298 | Discriminator Loss: 0.060770\n","| Global Epoch: 42 | Local Epoch: 6 | Generator Loss: 15.572367 | Discriminator Loss: 0.052434\n","| Global Epoch: 42 | Local Epoch: 7 | Generator Loss: 15.519494 | Discriminator Loss: 0.046142\n","| Global Epoch: 42 | Local Epoch: 8 | Generator Loss: 15.491654 | Discriminator Loss: 0.041210\n","| Global Epoch: 42 | Local Epoch: 9 | Generator Loss: 15.498983 | Discriminator Loss: 0.037277\n","| Global Epoch: 42 | Local Epoch: 0 | Generator Loss: 15.694704 | Discriminator Loss: 0.143409\n","| Global Epoch: 42 | Local Epoch: 1 | Generator Loss: 15.154807 | Discriminator Loss: 0.081228\n","| Global Epoch: 42 | Local Epoch: 2 | Generator Loss: 14.953226 | Discriminator Loss: 0.056173\n","| Global Epoch: 42 | Local Epoch: 3 | Generator Loss: 14.881112 | Discriminator Loss: 0.042834\n","| Global Epoch: 42 | Local Epoch: 4 | Generator Loss: 14.864165 | Discriminator Loss: 0.034680\n","| Global Epoch: 42 | Local Epoch: 5 | Generator Loss: 14.931342 | Discriminator Loss: 0.029159\n","| Global Epoch: 42 | Local Epoch: 6 | Generator Loss: 14.978511 | Discriminator Loss: 0.025198\n","| Global Epoch: 42 | Local Epoch: 7 | Generator Loss: 15.059958 | Discriminator Loss: 0.022196\n","| Global Epoch: 42 | Local Epoch: 8 | Generator Loss: 15.123511 | Discriminator Loss: 0.019841\n","| Global Epoch: 42 | Local Epoch: 9 | Generator Loss: 15.176296 | Discriminator Loss: 0.017948\n","| Global Epoch: 42 | Local Epoch: 0 | Generator Loss: 16.340584 | Discriminator Loss: 0.480445\n","| Global Epoch: 42 | Local Epoch: 1 | Generator Loss: 16.052783 | Discriminator Loss: 0.271708\n","| Global Epoch: 42 | Local Epoch: 2 | Generator Loss: 15.680937 | Discriminator Loss: 0.194105\n","| Global Epoch: 42 | Local Epoch: 3 | Generator Loss: 15.107754 | Discriminator Loss: 0.169647\n","| Global Epoch: 42 | Local Epoch: 4 | Generator Loss: 14.804452 | Discriminator Loss: 0.139532\n","| Global Epoch: 42 | Local Epoch: 5 | Generator Loss: 14.668046 | Discriminator Loss: 0.117693\n","| Global Epoch: 42 | Local Epoch: 6 | Generator Loss: 14.524206 | Discriminator Loss: 0.101665\n","| Global Epoch: 42 | Local Epoch: 7 | Generator Loss: 14.444779 | Discriminator Loss: 0.089530\n","| Global Epoch: 42 | Local Epoch: 8 | Generator Loss: 14.446017 | Discriminator Loss: 0.080033\n","| Global Epoch: 42 | Local Epoch: 9 | Generator Loss: 14.421043 | Discriminator Loss: 0.072310\n","| Global Epoch: 42 | Local Epoch: 0 | Generator Loss: 14.082831 | Discriminator Loss: 0.064624\n","| Global Epoch: 42 | Local Epoch: 1 | Generator Loss: 14.411017 | Discriminator Loss: 0.037930\n","| Global Epoch: 42 | Local Epoch: 2 | Generator Loss: 14.469437 | Discriminator Loss: 0.026100\n","| Global Epoch: 42 | Local Epoch: 3 | Generator Loss: 14.512261 | Discriminator Loss: 0.021220\n","| Global Epoch: 42 | Local Epoch: 4 | Generator Loss: 14.463226 | Discriminator Loss: 0.032450\n","| Global Epoch: 42 | Local Epoch: 5 | Generator Loss: 14.448714 | Discriminator Loss: 0.028243\n","| Global Epoch: 42 | Local Epoch: 6 | Generator Loss: 14.536786 | Discriminator Loss: 0.024512\n","| Global Epoch: 42 | Local Epoch: 7 | Generator Loss: 14.558408 | Discriminator Loss: 0.021723\n","| Global Epoch: 42 | Local Epoch: 8 | Generator Loss: 14.612263 | Discriminator Loss: 0.020896\n","| Global Epoch: 42 | Local Epoch: 9 | Generator Loss: 14.727775 | Discriminator Loss: 0.019072\n","After 43 epoch global training, averged local generator loss is: 16.1885, averged local discrimitor loss is: 0.0924\n","Averged validation loss is: 5.8219\n","| Global Epoch: 43 | Local Epoch: 0 | Generator Loss: 14.784678 | Discriminator Loss: 0.100589\n","| Global Epoch: 43 | Local Epoch: 1 | Generator Loss: 14.822789 | Discriminator Loss: 0.058174\n","| Global Epoch: 43 | Local Epoch: 2 | Generator Loss: 14.957654 | Discriminator Loss: 0.040028\n","| Global Epoch: 43 | Local Epoch: 3 | Generator Loss: 15.010414 | Discriminator Loss: 0.030505\n","| Global Epoch: 43 | Local Epoch: 4 | Generator Loss: 15.235265 | Discriminator Loss: 0.024705\n","| Global Epoch: 43 | Local Epoch: 5 | Generator Loss: 15.322037 | Discriminator Loss: 0.020794\n","| Global Epoch: 43 | Local Epoch: 6 | Generator Loss: 15.419644 | Discriminator Loss: 0.017961\n","| Global Epoch: 43 | Local Epoch: 7 | Generator Loss: 15.459597 | Discriminator Loss: 0.015825\n","| Global Epoch: 43 | Local Epoch: 8 | Generator Loss: 15.516386 | Discriminator Loss: 0.014155\n","| Global Epoch: 43 | Local Epoch: 9 | Generator Loss: 15.612551 | Discriminator Loss: 0.012811\n","| Global Epoch: 43 | Local Epoch: 0 | Generator Loss: 15.472882 | Discriminator Loss: 0.061807\n","| Global Epoch: 43 | Local Epoch: 1 | Generator Loss: 15.546180 | Discriminator Loss: 0.037201\n","| Global Epoch: 43 | Local Epoch: 2 | Generator Loss: 15.572615 | Discriminator Loss: 0.025891\n","| Global Epoch: 43 | Local Epoch: 3 | Generator Loss: 15.710828 | Discriminator Loss: 0.019818\n","| Global Epoch: 43 | Local Epoch: 4 | Generator Loss: 15.922534 | Discriminator Loss: 0.016071\n","| Global Epoch: 43 | Local Epoch: 5 | Generator Loss: 16.040247 | Discriminator Loss: 0.013527\n","| Global Epoch: 43 | Local Epoch: 6 | Generator Loss: 16.096841 | Discriminator Loss: 0.011694\n","| Global Epoch: 43 | Local Epoch: 7 | Generator Loss: 16.123520 | Discriminator Loss: 0.010308\n","| Global Epoch: 43 | Local Epoch: 8 | Generator Loss: 16.181254 | Discriminator Loss: 0.009285\n","| Global Epoch: 43 | Local Epoch: 9 | Generator Loss: 16.153093 | Discriminator Loss: 0.008413\n","| Global Epoch: 43 | Local Epoch: 0 | Generator Loss: 15.947849 | Discriminator Loss: 0.024383\n","| Global Epoch: 43 | Local Epoch: 1 | Generator Loss: 16.007280 | Discriminator Loss: 0.016882\n","| Global Epoch: 43 | Local Epoch: 2 | Generator Loss: 15.987121 | Discriminator Loss: 0.016336\n","| Global Epoch: 43 | Local Epoch: 3 | Generator Loss: 15.979502 | Discriminator Loss: 0.015085\n","| Global Epoch: 43 | Local Epoch: 4 | Generator Loss: 15.761928 | Discriminator Loss: 0.030871\n","| Global Epoch: 43 | Local Epoch: 5 | Generator Loss: 15.696303 | Discriminator Loss: 0.028790\n","| Global Epoch: 43 | Local Epoch: 6 | Generator Loss: 15.664354 | Discriminator Loss: 0.030758\n","| Global Epoch: 43 | Local Epoch: 7 | Generator Loss: 15.723353 | Discriminator Loss: 0.030952\n","| Global Epoch: 43 | Local Epoch: 8 | Generator Loss: 15.838572 | Discriminator Loss: 0.028312\n","| Global Epoch: 43 | Local Epoch: 9 | Generator Loss: 15.850126 | Discriminator Loss: 0.025664\n","| Global Epoch: 43 | Local Epoch: 0 | Generator Loss: 16.556697 | Discriminator Loss: 0.571813\n","| Global Epoch: 43 | Local Epoch: 1 | Generator Loss: 16.570263 | Discriminator Loss: 0.298002\n","| Global Epoch: 43 | Local Epoch: 2 | Generator Loss: 16.655961 | Discriminator Loss: 0.202560\n","| Global Epoch: 43 | Local Epoch: 3 | Generator Loss: 16.670065 | Discriminator Loss: 0.153070\n","| Global Epoch: 43 | Local Epoch: 4 | Generator Loss: 16.406585 | Discriminator Loss: 0.123008\n","| Global Epoch: 43 | Local Epoch: 5 | Generator Loss: 16.306077 | Discriminator Loss: 0.102925\n","| Global Epoch: 43 | Local Epoch: 6 | Generator Loss: 16.331249 | Discriminator Loss: 0.088412\n","| Global Epoch: 43 | Local Epoch: 7 | Generator Loss: 16.226417 | Discriminator Loss: 0.077501\n","| Global Epoch: 43 | Local Epoch: 8 | Generator Loss: 16.182502 | Discriminator Loss: 0.068999\n","| Global Epoch: 43 | Local Epoch: 9 | Generator Loss: 16.066950 | Discriminator Loss: 0.066716\n","| Global Epoch: 43 | Local Epoch: 0 | Generator Loss: 15.951946 | Discriminator Loss: 0.439185\n","| Global Epoch: 43 | Local Epoch: 1 | Generator Loss: 15.585116 | Discriminator Loss: 0.253684\n","| Global Epoch: 43 | Local Epoch: 2 | Generator Loss: 15.402942 | Discriminator Loss: 0.174117\n","| Global Epoch: 43 | Local Epoch: 3 | Generator Loss: 15.230715 | Discriminator Loss: 0.132541\n","| Global Epoch: 43 | Local Epoch: 4 | Generator Loss: 15.143264 | Discriminator Loss: 0.107187\n","| Global Epoch: 43 | Local Epoch: 5 | Generator Loss: 15.196428 | Discriminator Loss: 0.090212\n","| Global Epoch: 43 | Local Epoch: 6 | Generator Loss: 15.304132 | Discriminator Loss: 0.079290\n","| Global Epoch: 43 | Local Epoch: 7 | Generator Loss: 15.353211 | Discriminator Loss: 0.070459\n","| Global Epoch: 43 | Local Epoch: 8 | Generator Loss: 15.312833 | Discriminator Loss: 0.063045\n","| Global Epoch: 43 | Local Epoch: 9 | Generator Loss: 15.280912 | Discriminator Loss: 0.057054\n","After 44 epoch global training, averged local generator loss is: 16.1678, averged local discrimitor loss is: 0.0916\n","Averged validation loss is: 5.8661\n","| Global Epoch: 44 | Local Epoch: 0 | Generator Loss: 15.156684 | Discriminator Loss: 0.020956\n","| Global Epoch: 44 | Local Epoch: 1 | Generator Loss: 15.143148 | Discriminator Loss: 0.019352\n","| Global Epoch: 44 | Local Epoch: 2 | Generator Loss: 15.469577 | Discriminator Loss: 0.013773\n","| Global Epoch: 44 | Local Epoch: 3 | Generator Loss: 15.633606 | Discriminator Loss: 0.010583\n","| Global Epoch: 44 | Local Epoch: 4 | Generator Loss: 15.734218 | Discriminator Loss: 0.008606\n","| Global Epoch: 44 | Local Epoch: 5 | Generator Loss: 15.646511 | Discriminator Loss: 0.008187\n","| Global Epoch: 44 | Local Epoch: 6 | Generator Loss: 15.619859 | Discriminator Loss: 0.009849\n","| Global Epoch: 44 | Local Epoch: 7 | Generator Loss: 15.636178 | Discriminator Loss: 0.013638\n","| Global Epoch: 44 | Local Epoch: 8 | Generator Loss: 15.561792 | Discriminator Loss: 0.014838\n","| Global Epoch: 44 | Local Epoch: 9 | Generator Loss: 15.571824 | Discriminator Loss: 0.014720\n","| Global Epoch: 44 | Local Epoch: 0 | Generator Loss: 15.487146 | Discriminator Loss: 0.210699\n","| Global Epoch: 44 | Local Epoch: 1 | Generator Loss: 15.127183 | Discriminator Loss: 0.118485\n","| Global Epoch: 44 | Local Epoch: 2 | Generator Loss: 14.798318 | Discriminator Loss: 0.081388\n","| Global Epoch: 44 | Local Epoch: 3 | Generator Loss: 14.829970 | Discriminator Loss: 0.064140\n","| Global Epoch: 44 | Local Epoch: 4 | Generator Loss: 14.860932 | Discriminator Loss: 0.051984\n","| Global Epoch: 44 | Local Epoch: 5 | Generator Loss: 14.889398 | Discriminator Loss: 0.043774\n","| Global Epoch: 44 | Local Epoch: 6 | Generator Loss: 14.913140 | Discriminator Loss: 0.037787\n","| Global Epoch: 44 | Local Epoch: 7 | Generator Loss: 14.972638 | Discriminator Loss: 0.039838\n","| Global Epoch: 44 | Local Epoch: 8 | Generator Loss: 14.838759 | Discriminator Loss: 0.037337\n","| Global Epoch: 44 | Local Epoch: 9 | Generator Loss: 14.847325 | Discriminator Loss: 0.034068\n","| Global Epoch: 44 | Local Epoch: 0 | Generator Loss: 14.886239 | Discriminator Loss: 0.450838\n","| Global Epoch: 44 | Local Epoch: 1 | Generator Loss: 14.131048 | Discriminator Loss: 0.253409\n","| Global Epoch: 44 | Local Epoch: 2 | Generator Loss: 13.860806 | Discriminator Loss: 0.174023\n","| Global Epoch: 44 | Local Epoch: 3 | Generator Loss: 13.775845 | Discriminator Loss: 0.132482\n","| Global Epoch: 44 | Local Epoch: 4 | Generator Loss: 14.070091 | Discriminator Loss: 0.107293\n","| Global Epoch: 44 | Local Epoch: 5 | Generator Loss: 14.224752 | Discriminator Loss: 0.090106\n","| Global Epoch: 44 | Local Epoch: 6 | Generator Loss: 14.422245 | Discriminator Loss: 0.077756\n","| Global Epoch: 44 | Local Epoch: 7 | Generator Loss: 14.542656 | Discriminator Loss: 0.068409\n","| Global Epoch: 44 | Local Epoch: 8 | Generator Loss: 14.700624 | Discriminator Loss: 0.061082\n","| Global Epoch: 44 | Local Epoch: 9 | Generator Loss: 14.837341 | Discriminator Loss: 0.055191\n","| Global Epoch: 44 | Local Epoch: 0 | Generator Loss: 15.815272 | Discriminator Loss: 0.172589\n","| Global Epoch: 44 | Local Epoch: 1 | Generator Loss: 15.477980 | Discriminator Loss: 0.097291\n","| Global Epoch: 44 | Local Epoch: 2 | Generator Loss: 15.399991 | Discriminator Loss: 0.066815\n","| Global Epoch: 44 | Local Epoch: 3 | Generator Loss: 15.354979 | Discriminator Loss: 0.051041\n","| Global Epoch: 44 | Local Epoch: 4 | Generator Loss: 15.086919 | Discriminator Loss: 0.043064\n","| Global Epoch: 44 | Local Epoch: 5 | Generator Loss: 15.065845 | Discriminator Loss: 0.036351\n","| Global Epoch: 44 | Local Epoch: 6 | Generator Loss: 15.091032 | Discriminator Loss: 0.031417\n","| Global Epoch: 44 | Local Epoch: 7 | Generator Loss: 15.187980 | Discriminator Loss: 0.027880\n","| Global Epoch: 44 | Local Epoch: 8 | Generator Loss: 15.153569 | Discriminator Loss: 0.025003\n","| Global Epoch: 44 | Local Epoch: 9 | Generator Loss: 15.224706 | Discriminator Loss: 0.022658\n","| Global Epoch: 44 | Local Epoch: 0 | Generator Loss: 15.623408 | Discriminator Loss: 0.193374\n","| Global Epoch: 44 | Local Epoch: 1 | Generator Loss: 15.324511 | Discriminator Loss: 0.110232\n","| Global Epoch: 44 | Local Epoch: 2 | Generator Loss: 14.974282 | Discriminator Loss: 0.076028\n","| Global Epoch: 44 | Local Epoch: 3 | Generator Loss: 14.840313 | Discriminator Loss: 0.057796\n","| Global Epoch: 44 | Local Epoch: 4 | Generator Loss: 14.894444 | Discriminator Loss: 0.046698\n","| Global Epoch: 44 | Local Epoch: 5 | Generator Loss: 14.874239 | Discriminator Loss: 0.045450\n","| Global Epoch: 44 | Local Epoch: 6 | Generator Loss: 15.010865 | Discriminator Loss: 0.041898\n","| Global Epoch: 44 | Local Epoch: 7 | Generator Loss: 14.982679 | Discriminator Loss: 0.037672\n","| Global Epoch: 44 | Local Epoch: 8 | Generator Loss: 15.149369 | Discriminator Loss: 0.033948\n","| Global Epoch: 44 | Local Epoch: 9 | Generator Loss: 15.281131 | Discriminator Loss: 0.030705\n","After 45 epoch global training, averged local generator loss is: 16.1481, averged local discrimitor loss is: 0.0903\n","Averged validation loss is: 5.7287\n","| Global Epoch: 45 | Local Epoch: 0 | Generator Loss: 16.018008 | Discriminator Loss: 0.048388\n","| Global Epoch: 45 | Local Epoch: 1 | Generator Loss: 15.750295 | Discriminator Loss: 0.029739\n","| Global Epoch: 45 | Local Epoch: 2 | Generator Loss: 15.579872 | Discriminator Loss: 0.021635\n","| Global Epoch: 45 | Local Epoch: 3 | Generator Loss: 15.659927 | Discriminator Loss: 0.018076\n","| Global Epoch: 45 | Local Epoch: 4 | Generator Loss: 15.815116 | Discriminator Loss: 0.016977\n","| Global Epoch: 45 | Local Epoch: 5 | Generator Loss: 15.865865 | Discriminator Loss: 0.018444\n","| Global Epoch: 45 | Local Epoch: 6 | Generator Loss: 15.848003 | Discriminator Loss: 0.032150\n","| Global Epoch: 45 | Local Epoch: 7 | Generator Loss: 15.801638 | Discriminator Loss: 0.031243\n","| Global Epoch: 45 | Local Epoch: 8 | Generator Loss: 15.874890 | Discriminator Loss: 0.028423\n","| Global Epoch: 45 | Local Epoch: 9 | Generator Loss: 15.911669 | Discriminator Loss: 0.025736\n","| Global Epoch: 45 | Local Epoch: 0 | Generator Loss: 15.818829 | Discriminator Loss: 0.056441\n","| Global Epoch: 45 | Local Epoch: 1 | Generator Loss: 15.828279 | Discriminator Loss: 0.034010\n","| Global Epoch: 45 | Local Epoch: 2 | Generator Loss: 15.904500 | Discriminator Loss: 0.023425\n","| Global Epoch: 45 | Local Epoch: 3 | Generator Loss: 16.069990 | Discriminator Loss: 0.021381\n","| Global Epoch: 45 | Local Epoch: 4 | Generator Loss: 16.229250 | Discriminator Loss: 0.017744\n","| Global Epoch: 45 | Local Epoch: 5 | Generator Loss: 16.290910 | Discriminator Loss: 0.014955\n","| Global Epoch: 45 | Local Epoch: 6 | Generator Loss: 16.357547 | Discriminator Loss: 0.012921\n","| Global Epoch: 45 | Local Epoch: 7 | Generator Loss: 16.360238 | Discriminator Loss: 0.011372\n","| Global Epoch: 45 | Local Epoch: 8 | Generator Loss: 16.468183 | Discriminator Loss: 0.010224\n","| Global Epoch: 45 | Local Epoch: 9 | Generator Loss: 16.493808 | Discriminator Loss: 0.009261\n","| Global Epoch: 45 | Local Epoch: 0 | Generator Loss: 16.919861 | Discriminator Loss: 0.156073\n","| Global Epoch: 45 | Local Epoch: 1 | Generator Loss: 16.429031 | Discriminator Loss: 0.087427\n","| Global Epoch: 45 | Local Epoch: 2 | Generator Loss: 16.072801 | Discriminator Loss: 0.059823\n","| Global Epoch: 45 | Local Epoch: 3 | Generator Loss: 15.931452 | Discriminator Loss: 0.045522\n","| Global Epoch: 45 | Local Epoch: 4 | Generator Loss: 15.890805 | Discriminator Loss: 0.036878\n","| Global Epoch: 45 | Local Epoch: 5 | Generator Loss: 15.927833 | Discriminator Loss: 0.031118\n","| Global Epoch: 45 | Local Epoch: 6 | Generator Loss: 15.827901 | Discriminator Loss: 0.032161\n","| Global Epoch: 45 | Local Epoch: 7 | Generator Loss: 15.709734 | Discriminator Loss: 0.030391\n","| Global Epoch: 45 | Local Epoch: 8 | Generator Loss: 15.726990 | Discriminator Loss: 0.027493\n","| Global Epoch: 45 | Local Epoch: 9 | Generator Loss: 15.804452 | Discriminator Loss: 0.024964\n","| Global Epoch: 45 | Local Epoch: 0 | Generator Loss: 16.796225 | Discriminator Loss: 0.398596\n","| Global Epoch: 45 | Local Epoch: 1 | Generator Loss: 16.161044 | Discriminator Loss: 0.218056\n","| Global Epoch: 45 | Local Epoch: 2 | Generator Loss: 15.974706 | Discriminator Loss: 0.148901\n","| Global Epoch: 45 | Local Epoch: 3 | Generator Loss: 15.941776 | Discriminator Loss: 0.113031\n","| Global Epoch: 45 | Local Epoch: 4 | Generator Loss: 15.832293 | Discriminator Loss: 0.091175\n","| Global Epoch: 45 | Local Epoch: 5 | Generator Loss: 15.745673 | Discriminator Loss: 0.076477\n","| Global Epoch: 45 | Local Epoch: 6 | Generator Loss: 15.720951 | Discriminator Loss: 0.065916\n","| Global Epoch: 45 | Local Epoch: 7 | Generator Loss: 15.724934 | Discriminator Loss: 0.057952\n","| Global Epoch: 45 | Local Epoch: 8 | Generator Loss: 15.753257 | Discriminator Loss: 0.051751\n","| Global Epoch: 45 | Local Epoch: 9 | Generator Loss: 15.790491 | Discriminator Loss: 0.046748\n","| Global Epoch: 45 | Local Epoch: 0 | Generator Loss: 16.455087 | Discriminator Loss: 0.329592\n","| Global Epoch: 45 | Local Epoch: 1 | Generator Loss: 16.088606 | Discriminator Loss: 0.185839\n","| Global Epoch: 45 | Local Epoch: 2 | Generator Loss: 15.739596 | Discriminator Loss: 0.127150\n","| Global Epoch: 45 | Local Epoch: 3 | Generator Loss: 15.506833 | Discriminator Loss: 0.096578\n","| Global Epoch: 45 | Local Epoch: 4 | Generator Loss: 15.356298 | Discriminator Loss: 0.077982\n","| Global Epoch: 45 | Local Epoch: 5 | Generator Loss: 15.365400 | Discriminator Loss: 0.065470\n","| Global Epoch: 45 | Local Epoch: 6 | Generator Loss: 15.406460 | Discriminator Loss: 0.056479\n","| Global Epoch: 45 | Local Epoch: 7 | Generator Loss: 15.317827 | Discriminator Loss: 0.049870\n","| Global Epoch: 45 | Local Epoch: 8 | Generator Loss: 15.347713 | Discriminator Loss: 0.044570\n","| Global Epoch: 45 | Local Epoch: 9 | Generator Loss: 15.468379 | Discriminator Loss: 0.040420\n","After 46 epoch global training, averged local generator loss is: 16.1333, averged local discrimitor loss is: 0.0892\n","Averged validation loss is: 9.1398\n","| Global Epoch: 46 | Local Epoch: 0 | Generator Loss: 17.307855 | Discriminator Loss: 0.270948\n","| Global Epoch: 46 | Local Epoch: 1 | Generator Loss: 17.319823 | Discriminator Loss: 0.139262\n","| Global Epoch: 46 | Local Epoch: 2 | Generator Loss: 17.233447 | Discriminator Loss: 0.095821\n","| Global Epoch: 46 | Local Epoch: 3 | Generator Loss: 16.983857 | Discriminator Loss: 0.072479\n","| Global Epoch: 46 | Local Epoch: 4 | Generator Loss: 16.843780 | Discriminator Loss: 0.058349\n","| Global Epoch: 46 | Local Epoch: 5 | Generator Loss: 16.735608 | Discriminator Loss: 0.048811\n","| Global Epoch: 46 | Local Epoch: 6 | Generator Loss: 16.554543 | Discriminator Loss: 0.041945\n","| Global Epoch: 46 | Local Epoch: 7 | Generator Loss: 16.469621 | Discriminator Loss: 0.036784\n","| Global Epoch: 46 | Local Epoch: 8 | Generator Loss: 16.424034 | Discriminator Loss: 0.032755\n","| Global Epoch: 46 | Local Epoch: 9 | Generator Loss: 16.407239 | Discriminator Loss: 0.029522\n","| Global Epoch: 46 | Local Epoch: 0 | Generator Loss: 15.866622 | Discriminator Loss: 0.229108\n","| Global Epoch: 46 | Local Epoch: 1 | Generator Loss: 15.899112 | Discriminator Loss: 0.125159\n","| Global Epoch: 46 | Local Epoch: 2 | Generator Loss: 15.821942 | Discriminator Loss: 0.085261\n","| Global Epoch: 46 | Local Epoch: 3 | Generator Loss: 15.891611 | Discriminator Loss: 0.064735\n","| Global Epoch: 46 | Local Epoch: 4 | Generator Loss: 15.744295 | Discriminator Loss: 0.052176\n","| Global Epoch: 46 | Local Epoch: 5 | Generator Loss: 15.617690 | Discriminator Loss: 0.043770\n","| Global Epoch: 46 | Local Epoch: 6 | Generator Loss: 15.491684 | Discriminator Loss: 0.037725\n","| Global Epoch: 46 | Local Epoch: 7 | Generator Loss: 15.462133 | Discriminator Loss: 0.033164\n","| Global Epoch: 46 | Local Epoch: 8 | Generator Loss: 15.453808 | Discriminator Loss: 0.029614\n","| Global Epoch: 46 | Local Epoch: 9 | Generator Loss: 15.511295 | Discriminator Loss: 0.026760\n","| Global Epoch: 46 | Local Epoch: 0 | Generator Loss: 15.375835 | Discriminator Loss: 0.023935\n","| Global Epoch: 46 | Local Epoch: 1 | Generator Loss: 15.349973 | Discriminator Loss: 0.019808\n","| Global Epoch: 46 | Local Epoch: 2 | Generator Loss: 15.214110 | Discriminator Loss: 0.020951\n","| Global Epoch: 46 | Local Epoch: 3 | Generator Loss: 15.160934 | Discriminator Loss: 0.022453\n","| Global Epoch: 46 | Local Epoch: 4 | Generator Loss: 15.412933 | Discriminator Loss: 0.018788\n","| Global Epoch: 46 | Local Epoch: 5 | Generator Loss: 15.553485 | Discriminator Loss: 0.016396\n","| Global Epoch: 46 | Local Epoch: 6 | Generator Loss: 15.622783 | Discriminator Loss: 0.014192\n","| Global Epoch: 46 | Local Epoch: 7 | Generator Loss: 15.686241 | Discriminator Loss: 0.012967\n","| Global Epoch: 46 | Local Epoch: 8 | Generator Loss: 15.441608 | Discriminator Loss: 0.030222\n","| Global Epoch: 46 | Local Epoch: 9 | Generator Loss: 15.302615 | Discriminator Loss: 0.028373\n","| Global Epoch: 46 | Local Epoch: 0 | Generator Loss: 13.353795 | Discriminator Loss: 0.131464\n","| Global Epoch: 46 | Local Epoch: 1 | Generator Loss: 13.759732 | Discriminator Loss: 0.081870\n","| Global Epoch: 46 | Local Epoch: 2 | Generator Loss: 13.871856 | Discriminator Loss: 0.056873\n","| Global Epoch: 46 | Local Epoch: 3 | Generator Loss: 14.045983 | Discriminator Loss: 0.043454\n","| Global Epoch: 46 | Local Epoch: 4 | Generator Loss: 14.200080 | Discriminator Loss: 0.035243\n","| Global Epoch: 46 | Local Epoch: 5 | Generator Loss: 14.411063 | Discriminator Loss: 0.029824\n","| Global Epoch: 46 | Local Epoch: 6 | Generator Loss: 14.543563 | Discriminator Loss: 0.025761\n","| Global Epoch: 46 | Local Epoch: 7 | Generator Loss: 14.649094 | Discriminator Loss: 0.022674\n","| Global Epoch: 46 | Local Epoch: 8 | Generator Loss: 14.702180 | Discriminator Loss: 0.020253\n","| Global Epoch: 46 | Local Epoch: 9 | Generator Loss: 14.798038 | Discriminator Loss: 0.018358\n","| Global Epoch: 46 | Local Epoch: 0 | Generator Loss: 15.130137 | Discriminator Loss: 0.187652\n","| Global Epoch: 46 | Local Epoch: 1 | Generator Loss: 14.802324 | Discriminator Loss: 0.106654\n","| Global Epoch: 46 | Local Epoch: 2 | Generator Loss: 14.817297 | Discriminator Loss: 0.073264\n","| Global Epoch: 46 | Local Epoch: 3 | Generator Loss: 14.941771 | Discriminator Loss: 0.055909\n","| Global Epoch: 46 | Local Epoch: 4 | Generator Loss: 14.951309 | Discriminator Loss: 0.045276\n","| Global Epoch: 46 | Local Epoch: 5 | Generator Loss: 15.091483 | Discriminator Loss: 0.038089\n","| Global Epoch: 46 | Local Epoch: 6 | Generator Loss: 15.224461 | Discriminator Loss: 0.032949\n","| Global Epoch: 46 | Local Epoch: 7 | Generator Loss: 15.320922 | Discriminator Loss: 0.029016\n","| Global Epoch: 46 | Local Epoch: 8 | Generator Loss: 15.373173 | Discriminator Loss: 0.025943\n","| Global Epoch: 46 | Local Epoch: 9 | Generator Loss: 15.424641 | Discriminator Loss: 0.023465\n","After 47 epoch global training, averged local generator loss is: 16.1183, averged local discrimitor loss is: 0.0878\n","Averged validation loss is: 5.0555\n","| Global Epoch: 47 | Local Epoch: 0 | Generator Loss: 15.966379 | Discriminator Loss: 0.018536\n","| Global Epoch: 47 | Local Epoch: 1 | Generator Loss: 15.468006 | Discriminator Loss: 0.024428\n","| Global Epoch: 47 | Local Epoch: 2 | Generator Loss: 15.663497 | Discriminator Loss: 0.017965\n","| Global Epoch: 47 | Local Epoch: 3 | Generator Loss: 15.789459 | Discriminator Loss: 0.013871\n","| Global Epoch: 47 | Local Epoch: 4 | Generator Loss: 15.921452 | Discriminator Loss: 0.011287\n","| Global Epoch: 47 | Local Epoch: 5 | Generator Loss: 16.067375 | Discriminator Loss: 0.009677\n","| Global Epoch: 47 | Local Epoch: 6 | Generator Loss: 16.210133 | Discriminator Loss: 0.008523\n","| Global Epoch: 47 | Local Epoch: 7 | Generator Loss: 16.269037 | Discriminator Loss: 0.007616\n","| Global Epoch: 47 | Local Epoch: 8 | Generator Loss: 16.406384 | Discriminator Loss: 0.006848\n","| Global Epoch: 47 | Local Epoch: 9 | Generator Loss: 16.484407 | Discriminator Loss: 0.006434\n","| Global Epoch: 47 | Local Epoch: 0 | Generator Loss: 15.721552 | Discriminator Loss: 0.328574\n","| Global Epoch: 47 | Local Epoch: 1 | Generator Loss: 14.912741 | Discriminator Loss: 0.198893\n","| Global Epoch: 47 | Local Epoch: 2 | Generator Loss: 15.027215 | Discriminator Loss: 0.138500\n","| Global Epoch: 47 | Local Epoch: 3 | Generator Loss: 14.954167 | Discriminator Loss: 0.142533\n","| Global Epoch: 47 | Local Epoch: 4 | Generator Loss: 14.636751 | Discriminator Loss: 0.152077\n","| Global Epoch: 47 | Local Epoch: 5 | Generator Loss: 14.528397 | Discriminator Loss: 0.145248\n","| Global Epoch: 47 | Local Epoch: 6 | Generator Loss: 14.452473 | Discriminator Loss: 0.133114\n","| Global Epoch: 47 | Local Epoch: 7 | Generator Loss: 14.387930 | Discriminator Loss: 0.120273\n","| Global Epoch: 47 | Local Epoch: 8 | Generator Loss: 14.369822 | Discriminator Loss: 0.113144\n","| Global Epoch: 47 | Local Epoch: 9 | Generator Loss: 14.322028 | Discriminator Loss: 0.102911\n","| Global Epoch: 47 | Local Epoch: 0 | Generator Loss: 14.038170 | Discriminator Loss: 0.297979\n","| Global Epoch: 47 | Local Epoch: 1 | Generator Loss: 13.596467 | Discriminator Loss: 0.171694\n","| Global Epoch: 47 | Local Epoch: 2 | Generator Loss: 13.591555 | Discriminator Loss: 0.118189\n","| Global Epoch: 47 | Local Epoch: 3 | Generator Loss: 13.645639 | Discriminator Loss: 0.089833\n","| Global Epoch: 47 | Local Epoch: 4 | Generator Loss: 13.739844 | Discriminator Loss: 0.072540\n","| Global Epoch: 47 | Local Epoch: 5 | Generator Loss: 13.855337 | Discriminator Loss: 0.060954\n","| Global Epoch: 47 | Local Epoch: 6 | Generator Loss: 13.967891 | Discriminator Loss: 0.052695\n","| Global Epoch: 47 | Local Epoch: 7 | Generator Loss: 14.152485 | Discriminator Loss: 0.046627\n","| Global Epoch: 47 | Local Epoch: 8 | Generator Loss: 14.218671 | Discriminator Loss: 0.044868\n","| Global Epoch: 47 | Local Epoch: 9 | Generator Loss: 14.255276 | Discriminator Loss: 0.042473\n","| Global Epoch: 47 | Local Epoch: 0 | Generator Loss: 15.499783 | Discriminator Loss: 0.052284\n","| Global Epoch: 47 | Local Epoch: 1 | Generator Loss: 15.642422 | Discriminator Loss: 0.035181\n","| Global Epoch: 47 | Local Epoch: 2 | Generator Loss: 15.641882 | Discriminator Loss: 0.024370\n","| Global Epoch: 47 | Local Epoch: 3 | Generator Loss: 15.708990 | Discriminator Loss: 0.018581\n","| Global Epoch: 47 | Local Epoch: 4 | Generator Loss: 15.831958 | Discriminator Loss: 0.015036\n","| Global Epoch: 47 | Local Epoch: 5 | Generator Loss: 15.819023 | Discriminator Loss: 0.012654\n","| Global Epoch: 47 | Local Epoch: 6 | Generator Loss: 15.891817 | Discriminator Loss: 0.010946\n","| Global Epoch: 47 | Local Epoch: 7 | Generator Loss: 15.784919 | Discriminator Loss: 0.012853\n","| Global Epoch: 47 | Local Epoch: 8 | Generator Loss: 15.728005 | Discriminator Loss: 0.022103\n","| Global Epoch: 47 | Local Epoch: 9 | Generator Loss: 15.736855 | Discriminator Loss: 0.022607\n","| Global Epoch: 47 | Local Epoch: 0 | Generator Loss: 14.958205 | Discriminator Loss: 0.473387\n","| Global Epoch: 47 | Local Epoch: 1 | Generator Loss: 14.652104 | Discriminator Loss: 0.264859\n","| Global Epoch: 47 | Local Epoch: 2 | Generator Loss: 14.663866 | Discriminator Loss: 0.182123\n","| Global Epoch: 47 | Local Epoch: 3 | Generator Loss: 14.658835 | Discriminator Loss: 0.138808\n","| Global Epoch: 47 | Local Epoch: 4 | Generator Loss: 14.854883 | Discriminator Loss: 0.112484\n","| Global Epoch: 47 | Local Epoch: 5 | Generator Loss: 15.076433 | Discriminator Loss: 0.094541\n","| Global Epoch: 47 | Local Epoch: 6 | Generator Loss: 15.124163 | Discriminator Loss: 0.081553\n","| Global Epoch: 47 | Local Epoch: 7 | Generator Loss: 15.160059 | Discriminator Loss: 0.071745\n","| Global Epoch: 47 | Local Epoch: 8 | Generator Loss: 15.231155 | Discriminator Loss: 0.064329\n","| Global Epoch: 47 | Local Epoch: 9 | Generator Loss: 15.236040 | Discriminator Loss: 0.058771\n","After 48 epoch global training, averged local generator loss is: 16.0999, averged local discrimitor loss is: 0.0872\n","Averged validation loss is: 3.5010\n","| Global Epoch: 48 | Local Epoch: 0 | Generator Loss: 15.933285 | Discriminator Loss: 0.094300\n","| Global Epoch: 48 | Local Epoch: 1 | Generator Loss: 15.858661 | Discriminator Loss: 0.054215\n","| Global Epoch: 48 | Local Epoch: 2 | Generator Loss: 15.846678 | Discriminator Loss: 0.037320\n","| Global Epoch: 48 | Local Epoch: 3 | Generator Loss: 15.938726 | Discriminator Loss: 0.028507\n","| Global Epoch: 48 | Local Epoch: 4 | Generator Loss: 15.963262 | Discriminator Loss: 0.023069\n","| Global Epoch: 48 | Local Epoch: 5 | Generator Loss: 16.046533 | Discriminator Loss: 0.019416\n","| Global Epoch: 48 | Local Epoch: 6 | Generator Loss: 16.083193 | Discriminator Loss: 0.016769\n","| Global Epoch: 48 | Local Epoch: 7 | Generator Loss: 16.104893 | Discriminator Loss: 0.014771\n","| Global Epoch: 48 | Local Epoch: 8 | Generator Loss: 16.141572 | Discriminator Loss: 0.013208\n","| Global Epoch: 48 | Local Epoch: 9 | Generator Loss: 16.215043 | Discriminator Loss: 0.011959\n","| Global Epoch: 48 | Local Epoch: 0 | Generator Loss: 16.970325 | Discriminator Loss: 0.183967\n","| Global Epoch: 48 | Local Epoch: 1 | Generator Loss: 17.164566 | Discriminator Loss: 0.096027\n","| Global Epoch: 48 | Local Epoch: 2 | Generator Loss: 17.012138 | Discriminator Loss: 0.065445\n","| Global Epoch: 48 | Local Epoch: 3 | Generator Loss: 16.877548 | Discriminator Loss: 0.049586\n","| Global Epoch: 48 | Local Epoch: 4 | Generator Loss: 16.735462 | Discriminator Loss: 0.039971\n","| Global Epoch: 48 | Local Epoch: 5 | Generator Loss: 16.606569 | Discriminator Loss: 0.033457\n","| Global Epoch: 48 | Local Epoch: 6 | Generator Loss: 16.361806 | Discriminator Loss: 0.028782\n","| Global Epoch: 48 | Local Epoch: 7 | Generator Loss: 16.304866 | Discriminator Loss: 0.025264\n","| Global Epoch: 48 | Local Epoch: 8 | Generator Loss: 16.218328 | Discriminator Loss: 0.022575\n","| Global Epoch: 48 | Local Epoch: 9 | Generator Loss: 16.161595 | Discriminator Loss: 0.020360\n","| Global Epoch: 48 | Local Epoch: 0 | Generator Loss: 15.740111 | Discriminator Loss: 0.021081\n","| Global Epoch: 48 | Local Epoch: 1 | Generator Loss: 15.920883 | Discriminator Loss: 0.012311\n","| Global Epoch: 48 | Local Epoch: 2 | Generator Loss: 16.008407 | Discriminator Loss: 0.008526\n","| Global Epoch: 48 | Local Epoch: 3 | Generator Loss: 16.193044 | Discriminator Loss: 0.006522\n","| Global Epoch: 48 | Local Epoch: 4 | Generator Loss: 16.205776 | Discriminator Loss: 0.005311\n","| Global Epoch: 48 | Local Epoch: 5 | Generator Loss: 16.237279 | Discriminator Loss: 0.004671\n","| Global Epoch: 48 | Local Epoch: 6 | Generator Loss: 16.191211 | Discriminator Loss: 0.004201\n","| Global Epoch: 48 | Local Epoch: 7 | Generator Loss: 16.144983 | Discriminator Loss: 0.013891\n","| Global Epoch: 48 | Local Epoch: 8 | Generator Loss: 15.869182 | Discriminator Loss: 0.028105\n","| Global Epoch: 48 | Local Epoch: 9 | Generator Loss: 15.698879 | Discriminator Loss: 0.026821\n","| Global Epoch: 48 | Local Epoch: 0 | Generator Loss: 15.051738 | Discriminator Loss: 0.539917\n","| Global Epoch: 48 | Local Epoch: 1 | Generator Loss: 14.284997 | Discriminator Loss: 0.302686\n","| Global Epoch: 48 | Local Epoch: 2 | Generator Loss: 14.287045 | Discriminator Loss: 0.213006\n","| Global Epoch: 48 | Local Epoch: 3 | Generator Loss: 14.469939 | Discriminator Loss: 0.163334\n","| Global Epoch: 48 | Local Epoch: 4 | Generator Loss: 14.493653 | Discriminator Loss: 0.132275\n","| Global Epoch: 48 | Local Epoch: 5 | Generator Loss: 14.595232 | Discriminator Loss: 0.112007\n","| Global Epoch: 48 | Local Epoch: 6 | Generator Loss: 14.591792 | Discriminator Loss: 0.097573\n","| Global Epoch: 48 | Local Epoch: 7 | Generator Loss: 14.704474 | Discriminator Loss: 0.086041\n","| Global Epoch: 48 | Local Epoch: 8 | Generator Loss: 14.776044 | Discriminator Loss: 0.076866\n","| Global Epoch: 48 | Local Epoch: 9 | Generator Loss: 14.796076 | Discriminator Loss: 0.069480\n","| Global Epoch: 48 | Local Epoch: 0 | Generator Loss: 14.960620 | Discriminator Loss: 0.184502\n","| Global Epoch: 48 | Local Epoch: 1 | Generator Loss: 14.679587 | Discriminator Loss: 0.108371\n","| Global Epoch: 48 | Local Epoch: 2 | Generator Loss: 14.917660 | Discriminator Loss: 0.079437\n","| Global Epoch: 48 | Local Epoch: 3 | Generator Loss: 15.091481 | Discriminator Loss: 0.060802\n","| Global Epoch: 48 | Local Epoch: 4 | Generator Loss: 15.200867 | Discriminator Loss: 0.049270\n","| Global Epoch: 48 | Local Epoch: 5 | Generator Loss: 15.273017 | Discriminator Loss: 0.042540\n","| Global Epoch: 48 | Local Epoch: 6 | Generator Loss: 15.314619 | Discriminator Loss: 0.036867\n","| Global Epoch: 48 | Local Epoch: 7 | Generator Loss: 15.355606 | Discriminator Loss: 0.032604\n","| Global Epoch: 48 | Local Epoch: 8 | Generator Loss: 15.420063 | Discriminator Loss: 0.029195\n","| Global Epoch: 48 | Local Epoch: 9 | Generator Loss: 15.509077 | Discriminator Loss: 0.026697\n","After 49 epoch global training, averged local generator loss is: 16.0878, averged local discrimitor loss is: 0.0860\n","Averged validation loss is: 4.2787\n","| Global Epoch: 49 | Local Epoch: 0 | Generator Loss: 16.488600 | Discriminator Loss: 0.021727\n","| Global Epoch: 49 | Local Epoch: 1 | Generator Loss: 16.166531 | Discriminator Loss: 0.013164\n","| Global Epoch: 49 | Local Epoch: 2 | Generator Loss: 16.222667 | Discriminator Loss: 0.009205\n","| Global Epoch: 49 | Local Epoch: 3 | Generator Loss: 16.286884 | Discriminator Loss: 0.009314\n","| Global Epoch: 49 | Local Epoch: 4 | Generator Loss: 15.938208 | Discriminator Loss: 0.018771\n","| Global Epoch: 49 | Local Epoch: 5 | Generator Loss: 15.848778 | Discriminator Loss: 0.017147\n","| Global Epoch: 49 | Local Epoch: 6 | Generator Loss: 16.032348 | Discriminator Loss: 0.014982\n","| Global Epoch: 49 | Local Epoch: 7 | Generator Loss: 16.081624 | Discriminator Loss: 0.013214\n","| Global Epoch: 49 | Local Epoch: 8 | Generator Loss: 16.169870 | Discriminator Loss: 0.011834\n","| Global Epoch: 49 | Local Epoch: 9 | Generator Loss: 16.285604 | Discriminator Loss: 0.010711\n","| Global Epoch: 49 | Local Epoch: 0 | Generator Loss: 15.909864 | Discriminator Loss: 0.307153\n","| Global Epoch: 49 | Local Epoch: 1 | Generator Loss: 15.496279 | Discriminator Loss: 0.214245\n","| Global Epoch: 49 | Local Epoch: 2 | Generator Loss: 15.102107 | Discriminator Loss: 0.194342\n","| Global Epoch: 49 | Local Epoch: 3 | Generator Loss: 15.094146 | Discriminator Loss: 0.148897\n","| Global Epoch: 49 | Local Epoch: 4 | Generator Loss: 15.082881 | Discriminator Loss: 0.120561\n","| Global Epoch: 49 | Local Epoch: 5 | Generator Loss: 15.111122 | Discriminator Loss: 0.101576\n","| Global Epoch: 49 | Local Epoch: 6 | Generator Loss: 15.262433 | Discriminator Loss: 0.087976\n","| Global Epoch: 49 | Local Epoch: 7 | Generator Loss: 15.241623 | Discriminator Loss: 0.077490\n","| Global Epoch: 49 | Local Epoch: 8 | Generator Loss: 15.245002 | Discriminator Loss: 0.073240\n","| Global Epoch: 49 | Local Epoch: 9 | Generator Loss: 15.068408 | Discriminator Loss: 0.083404\n","| Global Epoch: 49 | Local Epoch: 0 | Generator Loss: 14.758935 | Discriminator Loss: 0.435353\n","| Global Epoch: 49 | Local Epoch: 1 | Generator Loss: 14.901459 | Discriminator Loss: 0.253311\n","| Global Epoch: 49 | Local Epoch: 2 | Generator Loss: 14.330241 | Discriminator Loss: 0.174436\n","| Global Epoch: 49 | Local Epoch: 3 | Generator Loss: 14.259656 | Discriminator Loss: 0.133020\n","| Global Epoch: 49 | Local Epoch: 4 | Generator Loss: 14.294109 | Discriminator Loss: 0.107718\n","| Global Epoch: 49 | Local Epoch: 5 | Generator Loss: 14.489583 | Discriminator Loss: 0.091364\n","| Global Epoch: 49 | Local Epoch: 6 | Generator Loss: 14.545522 | Discriminator Loss: 0.080750\n","| Global Epoch: 49 | Local Epoch: 7 | Generator Loss: 14.572590 | Discriminator Loss: 0.071318\n","| Global Epoch: 49 | Local Epoch: 8 | Generator Loss: 14.618492 | Discriminator Loss: 0.063758\n","| Global Epoch: 49 | Local Epoch: 9 | Generator Loss: 14.717507 | Discriminator Loss: 0.057670\n","| Global Epoch: 49 | Local Epoch: 0 | Generator Loss: 15.983916 | Discriminator Loss: 0.377222\n","| Global Epoch: 49 | Local Epoch: 1 | Generator Loss: 15.575887 | Discriminator Loss: 0.219624\n","| Global Epoch: 49 | Local Epoch: 2 | Generator Loss: 15.399817 | Discriminator Loss: 0.157423\n","| Global Epoch: 49 | Local Epoch: 3 | Generator Loss: 15.167233 | Discriminator Loss: 0.120305\n","| Global Epoch: 49 | Local Epoch: 4 | Generator Loss: 15.254822 | Discriminator Loss: 0.099287\n","| Global Epoch: 49 | Local Epoch: 5 | Generator Loss: 15.255490 | Discriminator Loss: 0.083709\n","| Global Epoch: 49 | Local Epoch: 6 | Generator Loss: 15.250437 | Discriminator Loss: 0.072314\n","| Global Epoch: 49 | Local Epoch: 7 | Generator Loss: 15.160036 | Discriminator Loss: 0.063708\n","| Global Epoch: 49 | Local Epoch: 8 | Generator Loss: 15.083973 | Discriminator Loss: 0.057068\n","| Global Epoch: 49 | Local Epoch: 9 | Generator Loss: 15.112134 | Discriminator Loss: 0.051660\n","| Global Epoch: 49 | Local Epoch: 0 | Generator Loss: 15.428855 | Discriminator Loss: 0.197902\n","| Global Epoch: 49 | Local Epoch: 1 | Generator Loss: 15.040123 | Discriminator Loss: 0.111222\n","| Global Epoch: 49 | Local Epoch: 2 | Generator Loss: 15.021585 | Discriminator Loss: 0.076690\n","| Global Epoch: 49 | Local Epoch: 3 | Generator Loss: 15.055465 | Discriminator Loss: 0.058604\n","| Global Epoch: 49 | Local Epoch: 4 | Generator Loss: 15.144298 | Discriminator Loss: 0.047537\n","| Global Epoch: 49 | Local Epoch: 5 | Generator Loss: 15.198949 | Discriminator Loss: 0.040041\n","| Global Epoch: 49 | Local Epoch: 6 | Generator Loss: 15.304206 | Discriminator Loss: 0.034914\n","| Global Epoch: 49 | Local Epoch: 7 | Generator Loss: 15.212206 | Discriminator Loss: 0.031112\n","| Global Epoch: 49 | Local Epoch: 8 | Generator Loss: 15.222310 | Discriminator Loss: 0.027863\n","| Global Epoch: 49 | Local Epoch: 9 | Generator Loss: 15.251586 | Discriminator Loss: 0.025224\n","After 50 epoch global training, averged local generator loss is: 16.0711, averged local discrimitor loss is: 0.0847\n","Averged validation loss is: 5.9921\n","| Global Epoch: 50 | Local Epoch: 0 | Generator Loss: 15.831359 | Discriminator Loss: 0.313500\n","| Global Epoch: 50 | Local Epoch: 1 | Generator Loss: 15.124498 | Discriminator Loss: 0.173678\n","| Global Epoch: 50 | Local Epoch: 2 | Generator Loss: 14.966189 | Discriminator Loss: 0.118903\n","| Global Epoch: 50 | Local Epoch: 3 | Generator Loss: 14.930468 | Discriminator Loss: 0.090224\n","| Global Epoch: 50 | Local Epoch: 4 | Generator Loss: 15.022652 | Discriminator Loss: 0.072889\n","| Global Epoch: 50 | Local Epoch: 5 | Generator Loss: 15.096296 | Discriminator Loss: 0.061199\n","| Global Epoch: 50 | Local Epoch: 6 | Generator Loss: 15.109453 | Discriminator Loss: 0.052784\n","| Global Epoch: 50 | Local Epoch: 7 | Generator Loss: 15.133028 | Discriminator Loss: 0.046436\n","| Global Epoch: 50 | Local Epoch: 8 | Generator Loss: 15.132448 | Discriminator Loss: 0.041457\n","| Global Epoch: 50 | Local Epoch: 9 | Generator Loss: 15.156179 | Discriminator Loss: 0.037466\n","| Global Epoch: 50 | Local Epoch: 0 | Generator Loss: 15.212858 | Discriminator Loss: 0.147812\n","| Global Epoch: 50 | Local Epoch: 1 | Generator Loss: 15.522548 | Discriminator Loss: 0.078229\n","| Global Epoch: 50 | Local Epoch: 2 | Generator Loss: 15.433176 | Discriminator Loss: 0.053485\n","| Global Epoch: 50 | Local Epoch: 3 | Generator Loss: 15.447427 | Discriminator Loss: 0.040868\n","| Global Epoch: 50 | Local Epoch: 4 | Generator Loss: 15.389246 | Discriminator Loss: 0.033043\n","| Global Epoch: 50 | Local Epoch: 5 | Generator Loss: 15.519901 | Discriminator Loss: 0.027712\n","| Global Epoch: 50 | Local Epoch: 6 | Generator Loss: 15.401153 | Discriminator Loss: 0.023867\n","| Global Epoch: 50 | Local Epoch: 7 | Generator Loss: 15.379212 | Discriminator Loss: 0.021027\n","| Global Epoch: 50 | Local Epoch: 8 | Generator Loss: 15.371987 | Discriminator Loss: 0.018754\n","| Global Epoch: 50 | Local Epoch: 9 | Generator Loss: 15.457920 | Discriminator Loss: 0.016925\n","| Global Epoch: 50 | Local Epoch: 0 | Generator Loss: 14.437688 | Discriminator Loss: 0.103498\n","| Global Epoch: 50 | Local Epoch: 1 | Generator Loss: 14.884651 | Discriminator Loss: 0.086970\n","| Global Epoch: 50 | Local Epoch: 2 | Generator Loss: 14.562220 | Discriminator Loss: 0.092643\n","| Global Epoch: 50 | Local Epoch: 3 | Generator Loss: 14.570761 | Discriminator Loss: 0.071778\n","| Global Epoch: 50 | Local Epoch: 4 | Generator Loss: 14.503595 | Discriminator Loss: 0.069484\n","| Global Epoch: 50 | Local Epoch: 5 | Generator Loss: 14.185223 | Discriminator Loss: 0.069616\n","| Global Epoch: 50 | Local Epoch: 6 | Generator Loss: 14.062033 | Discriminator Loss: 0.060777\n","| Global Epoch: 50 | Local Epoch: 7 | Generator Loss: 14.085812 | Discriminator Loss: 0.053693\n","| Global Epoch: 50 | Local Epoch: 8 | Generator Loss: 14.123030 | Discriminator Loss: 0.048532\n","| Global Epoch: 50 | Local Epoch: 9 | Generator Loss: 14.156263 | Discriminator Loss: 0.044040\n","| Global Epoch: 50 | Local Epoch: 0 | Generator Loss: 14.352968 | Discriminator Loss: 0.031267\n","| Global Epoch: 50 | Local Epoch: 1 | Generator Loss: 14.744687 | Discriminator Loss: 0.021443\n","| Global Epoch: 50 | Local Epoch: 2 | Generator Loss: 14.744201 | Discriminator Loss: 0.015550\n","| Global Epoch: 50 | Local Epoch: 3 | Generator Loss: 14.941181 | Discriminator Loss: 0.011912\n","| Global Epoch: 50 | Local Epoch: 4 | Generator Loss: 15.111905 | Discriminator Loss: 0.009678\n","| Global Epoch: 50 | Local Epoch: 5 | Generator Loss: 15.281455 | Discriminator Loss: 0.008258\n","| Global Epoch: 50 | Local Epoch: 6 | Generator Loss: 15.340538 | Discriminator Loss: 0.007160\n","| Global Epoch: 50 | Local Epoch: 7 | Generator Loss: 15.388633 | Discriminator Loss: 0.006313\n","| Global Epoch: 50 | Local Epoch: 8 | Generator Loss: 15.445532 | Discriminator Loss: 0.005651\n","| Global Epoch: 50 | Local Epoch: 9 | Generator Loss: 15.512879 | Discriminator Loss: 0.005137\n","| Global Epoch: 50 | Local Epoch: 0 | Generator Loss: 14.902224 | Discriminator Loss: 0.097248\n","| Global Epoch: 50 | Local Epoch: 1 | Generator Loss: 14.940409 | Discriminator Loss: 0.059753\n","| Global Epoch: 50 | Local Epoch: 2 | Generator Loss: 15.249078 | Discriminator Loss: 0.041833\n","| Global Epoch: 50 | Local Epoch: 3 | Generator Loss: 15.413248 | Discriminator Loss: 0.032015\n","| Global Epoch: 50 | Local Epoch: 4 | Generator Loss: 15.458805 | Discriminator Loss: 0.025970\n","| Global Epoch: 50 | Local Epoch: 5 | Generator Loss: 15.482842 | Discriminator Loss: 0.021892\n","| Global Epoch: 50 | Local Epoch: 6 | Generator Loss: 15.536660 | Discriminator Loss: 0.018949\n","| Global Epoch: 50 | Local Epoch: 7 | Generator Loss: 15.527557 | Discriminator Loss: 0.016706\n","| Global Epoch: 50 | Local Epoch: 8 | Generator Loss: 15.611454 | Discriminator Loss: 0.014954\n","| Global Epoch: 50 | Local Epoch: 9 | Generator Loss: 15.679943 | Discriminator Loss: 0.013537\n","After 51 epoch global training, averged local generator loss is: 16.0634, averged local discrimitor loss is: 0.0833\n","Averged validation loss is: 6.1875\n","| Global Epoch: 51 | Local Epoch: 0 | Generator Loss: 15.844521 | Discriminator Loss: 0.011618\n","| Global Epoch: 51 | Local Epoch: 1 | Generator Loss: 15.542863 | Discriminator Loss: 0.011406\n","| Global Epoch: 51 | Local Epoch: 2 | Generator Loss: 15.512301 | Discriminator Loss: 0.015173\n","| Global Epoch: 51 | Local Epoch: 3 | Generator Loss: 15.220088 | Discriminator Loss: 0.037032\n","| Global Epoch: 51 | Local Epoch: 4 | Generator Loss: 15.148829 | Discriminator Loss: 0.030558\n","| Global Epoch: 51 | Local Epoch: 5 | Generator Loss: 15.119721 | Discriminator Loss: 0.026654\n","| Global Epoch: 51 | Local Epoch: 6 | Generator Loss: 15.113833 | Discriminator Loss: 0.024095\n","| Global Epoch: 51 | Local Epoch: 7 | Generator Loss: 15.159652 | Discriminator Loss: 0.021256\n","| Global Epoch: 51 | Local Epoch: 8 | Generator Loss: 15.226885 | Discriminator Loss: 0.018999\n","| Global Epoch: 51 | Local Epoch: 9 | Generator Loss: 15.307070 | Discriminator Loss: 0.017169\n","| Global Epoch: 51 | Local Epoch: 0 | Generator Loss: 15.327805 | Discriminator Loss: 0.238738\n","| Global Epoch: 51 | Local Epoch: 1 | Generator Loss: 14.971177 | Discriminator Loss: 0.131808\n","| Global Epoch: 51 | Local Epoch: 2 | Generator Loss: 15.047744 | Discriminator Loss: 0.090169\n","| Global Epoch: 51 | Local Epoch: 3 | Generator Loss: 14.880695 | Discriminator Loss: 0.068527\n","| Global Epoch: 51 | Local Epoch: 4 | Generator Loss: 14.910445 | Discriminator Loss: 0.055411\n","| Global Epoch: 51 | Local Epoch: 5 | Generator Loss: 15.031668 | Discriminator Loss: 0.046640\n","| Global Epoch: 51 | Local Epoch: 6 | Generator Loss: 15.042938 | Discriminator Loss: 0.040265\n","| Global Epoch: 51 | Local Epoch: 7 | Generator Loss: 15.075537 | Discriminator Loss: 0.035443\n","| Global Epoch: 51 | Local Epoch: 8 | Generator Loss: 15.128432 | Discriminator Loss: 0.031664\n","| Global Epoch: 51 | Local Epoch: 9 | Generator Loss: 15.164307 | Discriminator Loss: 0.028627\n","| Global Epoch: 51 | Local Epoch: 0 | Generator Loss: 15.268570 | Discriminator Loss: 0.243258\n","| Global Epoch: 51 | Local Epoch: 1 | Generator Loss: 14.921908 | Discriminator Loss: 0.134533\n","| Global Epoch: 51 | Local Epoch: 2 | Generator Loss: 14.853750 | Discriminator Loss: 0.092398\n","| Global Epoch: 51 | Local Epoch: 3 | Generator Loss: 14.915938 | Discriminator Loss: 0.070417\n","| Global Epoch: 51 | Local Epoch: 4 | Generator Loss: 15.026708 | Discriminator Loss: 0.056987\n","| Global Epoch: 51 | Local Epoch: 5 | Generator Loss: 15.032843 | Discriminator Loss: 0.047892\n","| Global Epoch: 51 | Local Epoch: 6 | Generator Loss: 15.092168 | Discriminator Loss: 0.041387\n","| Global Epoch: 51 | Local Epoch: 7 | Generator Loss: 15.151482 | Discriminator Loss: 0.036445\n","| Global Epoch: 51 | Local Epoch: 8 | Generator Loss: 15.169138 | Discriminator Loss: 0.032577\n","| Global Epoch: 51 | Local Epoch: 9 | Generator Loss: 15.132270 | Discriminator Loss: 0.037721\n","| Global Epoch: 51 | Local Epoch: 0 | Generator Loss: 14.505008 | Discriminator Loss: 0.451444\n","| Global Epoch: 51 | Local Epoch: 1 | Generator Loss: 14.079961 | Discriminator Loss: 0.254601\n","| Global Epoch: 51 | Local Epoch: 2 | Generator Loss: 14.097798 | Discriminator Loss: 0.175420\n","| Global Epoch: 51 | Local Epoch: 3 | Generator Loss: 14.194886 | Discriminator Loss: 0.133682\n","| Global Epoch: 51 | Local Epoch: 4 | Generator Loss: 14.373374 | Discriminator Loss: 0.108531\n","| Global Epoch: 51 | Local Epoch: 5 | Generator Loss: 14.490283 | Discriminator Loss: 0.091214\n","| Global Epoch: 51 | Local Epoch: 6 | Generator Loss: 14.675260 | Discriminator Loss: 0.078843\n","| Global Epoch: 51 | Local Epoch: 7 | Generator Loss: 14.765388 | Discriminator Loss: 0.069366\n","| Global Epoch: 51 | Local Epoch: 8 | Generator Loss: 14.826799 | Discriminator Loss: 0.061984\n","| Global Epoch: 51 | Local Epoch: 9 | Generator Loss: 14.925831 | Discriminator Loss: 0.056079\n","| Global Epoch: 51 | Local Epoch: 0 | Generator Loss: 16.285462 | Discriminator Loss: 0.193194\n","| Global Epoch: 51 | Local Epoch: 1 | Generator Loss: 15.685374 | Discriminator Loss: 0.108314\n","| Global Epoch: 51 | Local Epoch: 2 | Generator Loss: 15.386846 | Discriminator Loss: 0.074419\n","| Global Epoch: 51 | Local Epoch: 3 | Generator Loss: 15.406747 | Discriminator Loss: 0.056799\n","| Global Epoch: 51 | Local Epoch: 4 | Generator Loss: 15.429782 | Discriminator Loss: 0.045968\n","| Global Epoch: 51 | Local Epoch: 5 | Generator Loss: 15.424530 | Discriminator Loss: 0.038660\n","| Global Epoch: 51 | Local Epoch: 6 | Generator Loss: 15.416978 | Discriminator Loss: 0.033402\n","| Global Epoch: 51 | Local Epoch: 7 | Generator Loss: 15.505946 | Discriminator Loss: 0.029595\n","| Global Epoch: 51 | Local Epoch: 8 | Generator Loss: 15.512831 | Discriminator Loss: 0.026776\n","| Global Epoch: 51 | Local Epoch: 9 | Generator Loss: 15.478161 | Discriminator Loss: 0.024252\n","After 52 epoch global training, averged local generator loss is: 16.0522, averged local discrimitor loss is: 0.0822\n","Averged validation loss is: 4.6000\n","| Global Epoch: 52 | Local Epoch: 0 | Generator Loss: 15.392038 | Discriminator Loss: 0.077312\n","| Global Epoch: 52 | Local Epoch: 1 | Generator Loss: 15.347263 | Discriminator Loss: 0.044957\n","| Global Epoch: 52 | Local Epoch: 2 | Generator Loss: 15.459874 | Discriminator Loss: 0.031085\n","| Global Epoch: 52 | Local Epoch: 3 | Generator Loss: 15.483729 | Discriminator Loss: 0.023720\n","| Global Epoch: 52 | Local Epoch: 4 | Generator Loss: 15.469167 | Discriminator Loss: 0.019288\n","| Global Epoch: 52 | Local Epoch: 5 | Generator Loss: 15.304460 | Discriminator Loss: 0.024572\n","| Global Epoch: 52 | Local Epoch: 6 | Generator Loss: 15.264709 | Discriminator Loss: 0.025118\n","| Global Epoch: 52 | Local Epoch: 7 | Generator Loss: 15.259003 | Discriminator Loss: 0.023712\n","| Global Epoch: 52 | Local Epoch: 8 | Generator Loss: 15.282848 | Discriminator Loss: 0.021563\n","| Global Epoch: 52 | Local Epoch: 9 | Generator Loss: 15.285455 | Discriminator Loss: 0.020032\n","| Global Epoch: 52 | Local Epoch: 0 | Generator Loss: 15.239201 | Discriminator Loss: 0.152949\n","| Global Epoch: 52 | Local Epoch: 1 | Generator Loss: 15.233810 | Discriminator Loss: 0.084672\n","| Global Epoch: 52 | Local Epoch: 2 | Generator Loss: 15.164809 | Discriminator Loss: 0.060947\n","| Global Epoch: 52 | Local Epoch: 3 | Generator Loss: 14.853710 | Discriminator Loss: 0.056304\n","| Global Epoch: 52 | Local Epoch: 4 | Generator Loss: 14.846728 | Discriminator Loss: 0.048673\n","| Global Epoch: 52 | Local Epoch: 5 | Generator Loss: 14.877265 | Discriminator Loss: 0.043019\n","| Global Epoch: 52 | Local Epoch: 6 | Generator Loss: 14.780407 | Discriminator Loss: 0.051423\n","| Global Epoch: 52 | Local Epoch: 7 | Generator Loss: 14.600652 | Discriminator Loss: 0.077197\n","| Global Epoch: 52 | Local Epoch: 8 | Generator Loss: 14.476638 | Discriminator Loss: 0.081206\n","| Global Epoch: 52 | Local Epoch: 9 | Generator Loss: 14.497044 | Discriminator Loss: 0.084392\n","| Global Epoch: 52 | Local Epoch: 0 | Generator Loss: 14.678154 | Discriminator Loss: 0.484126\n","| Global Epoch: 52 | Local Epoch: 1 | Generator Loss: 13.850776 | Discriminator Loss: 0.285958\n","| Global Epoch: 52 | Local Epoch: 2 | Generator Loss: 13.991124 | Discriminator Loss: 0.197277\n","| Global Epoch: 52 | Local Epoch: 3 | Generator Loss: 14.356774 | Discriminator Loss: 0.154077\n","| Global Epoch: 52 | Local Epoch: 4 | Generator Loss: 14.518329 | Discriminator Loss: 0.124751\n","| Global Epoch: 52 | Local Epoch: 5 | Generator Loss: 14.599971 | Discriminator Loss: 0.104756\n","| Global Epoch: 52 | Local Epoch: 6 | Generator Loss: 14.648886 | Discriminator Loss: 0.090355\n","| Global Epoch: 52 | Local Epoch: 7 | Generator Loss: 14.678703 | Discriminator Loss: 0.079486\n","| Global Epoch: 52 | Local Epoch: 8 | Generator Loss: 14.741412 | Discriminator Loss: 0.071058\n","| Global Epoch: 52 | Local Epoch: 9 | Generator Loss: 14.814331 | Discriminator Loss: 0.066650\n","| Global Epoch: 52 | Local Epoch: 0 | Generator Loss: 15.178359 | Discriminator Loss: 0.200709\n","| Global Epoch: 52 | Local Epoch: 1 | Generator Loss: 15.074927 | Discriminator Loss: 0.110468\n","| Global Epoch: 52 | Local Epoch: 2 | Generator Loss: 14.980433 | Discriminator Loss: 0.080659\n","| Global Epoch: 52 | Local Epoch: 3 | Generator Loss: 15.122967 | Discriminator Loss: 0.063184\n","| Global Epoch: 52 | Local Epoch: 4 | Generator Loss: 15.120522 | Discriminator Loss: 0.051276\n","| Global Epoch: 52 | Local Epoch: 5 | Generator Loss: 15.247852 | Discriminator Loss: 0.043254\n","| Global Epoch: 52 | Local Epoch: 6 | Generator Loss: 15.341833 | Discriminator Loss: 0.037417\n","| Global Epoch: 52 | Local Epoch: 7 | Generator Loss: 15.397265 | Discriminator Loss: 0.032958\n","| Global Epoch: 52 | Local Epoch: 8 | Generator Loss: 15.433616 | Discriminator Loss: 0.029468\n","| Global Epoch: 52 | Local Epoch: 9 | Generator Loss: 15.431758 | Discriminator Loss: 0.026651\n","| Global Epoch: 52 | Local Epoch: 0 | Generator Loss: 15.583698 | Discriminator Loss: 0.188671\n","| Global Epoch: 52 | Local Epoch: 1 | Generator Loss: 15.866119 | Discriminator Loss: 0.109063\n","| Global Epoch: 52 | Local Epoch: 2 | Generator Loss: 15.684410 | Discriminator Loss: 0.074823\n","| Global Epoch: 52 | Local Epoch: 3 | Generator Loss: 15.505944 | Discriminator Loss: 0.056946\n","| Global Epoch: 52 | Local Epoch: 4 | Generator Loss: 15.408464 | Discriminator Loss: 0.046045\n","| Global Epoch: 52 | Local Epoch: 5 | Generator Loss: 15.335991 | Discriminator Loss: 0.038712\n","| Global Epoch: 52 | Local Epoch: 6 | Generator Loss: 15.334022 | Discriminator Loss: 0.033415\n","| Global Epoch: 52 | Local Epoch: 7 | Generator Loss: 15.381920 | Discriminator Loss: 0.029427\n","| Global Epoch: 52 | Local Epoch: 8 | Generator Loss: 15.407641 | Discriminator Loss: 0.026295\n","| Global Epoch: 52 | Local Epoch: 9 | Generator Loss: 15.418826 | Discriminator Loss: 0.023776\n","After 53 epoch global training, averged local generator loss is: 16.0402, averged local discrimitor loss is: 0.0811\n","Averged validation loss is: 5.1628\n","| Global Epoch: 53 | Local Epoch: 0 | Generator Loss: 15.158902 | Discriminator Loss: 0.010777\n","| Global Epoch: 53 | Local Epoch: 1 | Generator Loss: 15.218606 | Discriminator Loss: 0.013335\n","| Global Epoch: 53 | Local Epoch: 2 | Generator Loss: 15.263623 | Discriminator Loss: 0.012079\n","| Global Epoch: 53 | Local Epoch: 3 | Generator Loss: 15.448814 | Discriminator Loss: 0.010316\n","| Global Epoch: 53 | Local Epoch: 4 | Generator Loss: 15.644596 | Discriminator Loss: 0.008963\n","| Global Epoch: 53 | Local Epoch: 5 | Generator Loss: 15.892752 | Discriminator Loss: 0.008105\n","| Global Epoch: 53 | Local Epoch: 6 | Generator Loss: 16.129968 | Discriminator Loss: 0.007598\n","| Global Epoch: 53 | Local Epoch: 7 | Generator Loss: 16.171837 | Discriminator Loss: 0.008656\n","| Global Epoch: 53 | Local Epoch: 8 | Generator Loss: 16.296265 | Discriminator Loss: 0.010498\n","| Global Epoch: 53 | Local Epoch: 9 | Generator Loss: 16.291632 | Discriminator Loss: 0.012038\n","| Global Epoch: 53 | Local Epoch: 0 | Generator Loss: 16.294110 | Discriminator Loss: 0.413450\n","| Global Epoch: 53 | Local Epoch: 1 | Generator Loss: 16.085460 | Discriminator Loss: 0.226459\n","| Global Epoch: 53 | Local Epoch: 2 | Generator Loss: 16.022184 | Discriminator Loss: 0.155245\n","| Global Epoch: 53 | Local Epoch: 3 | Generator Loss: 16.091005 | Discriminator Loss: 0.118053\n","| Global Epoch: 53 | Local Epoch: 4 | Generator Loss: 16.103242 | Discriminator Loss: 0.095330\n","| Global Epoch: 53 | Local Epoch: 5 | Generator Loss: 16.157377 | Discriminator Loss: 0.080314\n","| Global Epoch: 53 | Local Epoch: 6 | Generator Loss: 16.176056 | Discriminator Loss: 0.069824\n","| Global Epoch: 53 | Local Epoch: 7 | Generator Loss: 16.205822 | Discriminator Loss: 0.061462\n","| Global Epoch: 53 | Local Epoch: 8 | Generator Loss: 16.263803 | Discriminator Loss: 0.054896\n","| Global Epoch: 53 | Local Epoch: 9 | Generator Loss: 16.344314 | Discriminator Loss: 0.049600\n","| Global Epoch: 53 | Local Epoch: 0 | Generator Loss: 16.829575 | Discriminator Loss: 0.310988\n","| Global Epoch: 53 | Local Epoch: 1 | Generator Loss: 16.063596 | Discriminator Loss: 0.173724\n","| Global Epoch: 53 | Local Epoch: 2 | Generator Loss: 15.945590 | Discriminator Loss: 0.118692\n","| Global Epoch: 53 | Local Epoch: 3 | Generator Loss: 15.770730 | Discriminator Loss: 0.090225\n","| Global Epoch: 53 | Local Epoch: 4 | Generator Loss: 15.770597 | Discriminator Loss: 0.073054\n","| Global Epoch: 53 | Local Epoch: 5 | Generator Loss: 15.742456 | Discriminator Loss: 0.061371\n","| Global Epoch: 53 | Local Epoch: 6 | Generator Loss: 15.711866 | Discriminator Loss: 0.052914\n","| Global Epoch: 53 | Local Epoch: 7 | Generator Loss: 15.679947 | Discriminator Loss: 0.046527\n","| Global Epoch: 53 | Local Epoch: 8 | Generator Loss: 15.696450 | Discriminator Loss: 0.041537\n","| Global Epoch: 53 | Local Epoch: 9 | Generator Loss: 15.794636 | Discriminator Loss: 0.037687\n","| Global Epoch: 53 | Local Epoch: 0 | Generator Loss: 16.085681 | Discriminator Loss: 0.077938\n","| Global Epoch: 53 | Local Epoch: 1 | Generator Loss: 15.861461 | Discriminator Loss: 0.046525\n","| Global Epoch: 53 | Local Epoch: 2 | Generator Loss: 15.657801 | Discriminator Loss: 0.032028\n","| Global Epoch: 53 | Local Epoch: 3 | Generator Loss: 15.732859 | Discriminator Loss: 0.024405\n","| Global Epoch: 53 | Local Epoch: 4 | Generator Loss: 15.722210 | Discriminator Loss: 0.019761\n","| Global Epoch: 53 | Local Epoch: 5 | Generator Loss: 15.645037 | Discriminator Loss: 0.028534\n","| Global Epoch: 53 | Local Epoch: 6 | Generator Loss: 15.725859 | Discriminator Loss: 0.029708\n","| Global Epoch: 53 | Local Epoch: 7 | Generator Loss: 15.774762 | Discriminator Loss: 0.027434\n","| Global Epoch: 53 | Local Epoch: 8 | Generator Loss: 15.669636 | Discriminator Loss: 0.032550\n","| Global Epoch: 53 | Local Epoch: 9 | Generator Loss: 15.607234 | Discriminator Loss: 0.029946\n","| Global Epoch: 53 | Local Epoch: 0 | Generator Loss: 15.440994 | Discriminator Loss: 0.169930\n","| Global Epoch: 53 | Local Epoch: 1 | Generator Loss: 15.349139 | Discriminator Loss: 0.096840\n","| Global Epoch: 53 | Local Epoch: 2 | Generator Loss: 15.508936 | Discriminator Loss: 0.067763\n","| Global Epoch: 53 | Local Epoch: 3 | Generator Loss: 15.517701 | Discriminator Loss: 0.051627\n","| Global Epoch: 53 | Local Epoch: 4 | Generator Loss: 15.533301 | Discriminator Loss: 0.041761\n","| Global Epoch: 53 | Local Epoch: 5 | Generator Loss: 15.603644 | Discriminator Loss: 0.035311\n","| Global Epoch: 53 | Local Epoch: 6 | Generator Loss: 15.690581 | Discriminator Loss: 0.030563\n","| Global Epoch: 53 | Local Epoch: 7 | Generator Loss: 15.685655 | Discriminator Loss: 0.026927\n","| Global Epoch: 53 | Local Epoch: 8 | Generator Loss: 15.760017 | Discriminator Loss: 0.024090\n","| Global Epoch: 53 | Local Epoch: 9 | Generator Loss: 15.792288 | Discriminator Loss: 0.021798\n","After 54 epoch global training, averged local generator loss is: 16.0356, averged local discrimitor loss is: 0.0800\n","Averged validation loss is: 6.4894\n","| Global Epoch: 54 | Local Epoch: 0 | Generator Loss: 15.693686 | Discriminator Loss: 0.073855\n","| Global Epoch: 54 | Local Epoch: 1 | Generator Loss: 15.711962 | Discriminator Loss: 0.042087\n","| Global Epoch: 54 | Local Epoch: 2 | Generator Loss: 15.726410 | Discriminator Loss: 0.029030\n","| Global Epoch: 54 | Local Epoch: 3 | Generator Loss: 15.751361 | Discriminator Loss: 0.022152\n","| Global Epoch: 54 | Local Epoch: 4 | Generator Loss: 15.770364 | Discriminator Loss: 0.017960\n","| Global Epoch: 54 | Local Epoch: 5 | Generator Loss: 15.955810 | Discriminator Loss: 0.015132\n","| Global Epoch: 54 | Local Epoch: 6 | Generator Loss: 16.060343 | Discriminator Loss: 0.013083\n","| Global Epoch: 54 | Local Epoch: 7 | Generator Loss: 16.099021 | Discriminator Loss: 0.011532\n","| Global Epoch: 54 | Local Epoch: 8 | Generator Loss: 16.114260 | Discriminator Loss: 0.010322\n","| Global Epoch: 54 | Local Epoch: 9 | Generator Loss: 16.132168 | Discriminator Loss: 0.009344\n","| Global Epoch: 54 | Local Epoch: 0 | Generator Loss: 16.391862 | Discriminator Loss: 0.195793\n","| Global Epoch: 54 | Local Epoch: 1 | Generator Loss: 15.658007 | Discriminator Loss: 0.113956\n","| Global Epoch: 54 | Local Epoch: 2 | Generator Loss: 15.510791 | Discriminator Loss: 0.079026\n","| Global Epoch: 54 | Local Epoch: 3 | Generator Loss: 15.580103 | Discriminator Loss: 0.060739\n","| Global Epoch: 54 | Local Epoch: 4 | Generator Loss: 15.583433 | Discriminator Loss: 0.049216\n","| Global Epoch: 54 | Local Epoch: 5 | Generator Loss: 15.679765 | Discriminator Loss: 0.041535\n","| Global Epoch: 54 | Local Epoch: 6 | Generator Loss: 15.765965 | Discriminator Loss: 0.036004\n","| Global Epoch: 54 | Local Epoch: 7 | Generator Loss: 15.835714 | Discriminator Loss: 0.031744\n","| Global Epoch: 54 | Local Epoch: 8 | Generator Loss: 15.822847 | Discriminator Loss: 0.028387\n","| Global Epoch: 54 | Local Epoch: 9 | Generator Loss: 15.809339 | Discriminator Loss: 0.025678\n","| Global Epoch: 54 | Local Epoch: 0 | Generator Loss: 15.377657 | Discriminator Loss: 0.119414\n","| Global Epoch: 54 | Local Epoch: 1 | Generator Loss: 15.211945 | Discriminator Loss: 0.070855\n","| Global Epoch: 54 | Local Epoch: 2 | Generator Loss: 15.196273 | Discriminator Loss: 0.049137\n","| Global Epoch: 54 | Local Epoch: 3 | Generator Loss: 15.477546 | Discriminator Loss: 0.037497\n","| Global Epoch: 54 | Local Epoch: 4 | Generator Loss: 15.645441 | Discriminator Loss: 0.030394\n","| Global Epoch: 54 | Local Epoch: 5 | Generator Loss: 15.742169 | Discriminator Loss: 0.025583\n","| Global Epoch: 54 | Local Epoch: 6 | Generator Loss: 15.899807 | Discriminator Loss: 0.022117\n","| Global Epoch: 54 | Local Epoch: 7 | Generator Loss: 16.043265 | Discriminator Loss: 0.019502\n","| Global Epoch: 54 | Local Epoch: 8 | Generator Loss: 16.127991 | Discriminator Loss: 0.017441\n","| Global Epoch: 54 | Local Epoch: 9 | Generator Loss: 16.139164 | Discriminator Loss: 0.015777\n","| Global Epoch: 54 | Local Epoch: 0 | Generator Loss: 16.059842 | Discriminator Loss: 0.105562\n","| Global Epoch: 54 | Local Epoch: 1 | Generator Loss: 15.593104 | Discriminator Loss: 0.061791\n","| Global Epoch: 54 | Local Epoch: 2 | Generator Loss: 15.348315 | Discriminator Loss: 0.042541\n","| Global Epoch: 54 | Local Epoch: 3 | Generator Loss: 15.411155 | Discriminator Loss: 0.035187\n","| Global Epoch: 54 | Local Epoch: 4 | Generator Loss: 15.229747 | Discriminator Loss: 0.044614\n","| Global Epoch: 54 | Local Epoch: 5 | Generator Loss: 15.241533 | Discriminator Loss: 0.039021\n","| Global Epoch: 54 | Local Epoch: 6 | Generator Loss: 15.263498 | Discriminator Loss: 0.033893\n","| Global Epoch: 54 | Local Epoch: 7 | Generator Loss: 15.337931 | Discriminator Loss: 0.030160\n","| Global Epoch: 54 | Local Epoch: 8 | Generator Loss: 15.306012 | Discriminator Loss: 0.027926\n","| Global Epoch: 54 | Local Epoch: 9 | Generator Loss: 15.334987 | Discriminator Loss: 0.026126\n","| Global Epoch: 54 | Local Epoch: 0 | Generator Loss: 16.107919 | Discriminator Loss: 0.101623\n","| Global Epoch: 54 | Local Epoch: 1 | Generator Loss: 15.748814 | Discriminator Loss: 0.058237\n","| Global Epoch: 54 | Local Epoch: 2 | Generator Loss: 15.701982 | Discriminator Loss: 0.040332\n","| Global Epoch: 54 | Local Epoch: 3 | Generator Loss: 15.700642 | Discriminator Loss: 0.030776\n","| Global Epoch: 54 | Local Epoch: 4 | Generator Loss: 15.698112 | Discriminator Loss: 0.024911\n","| Global Epoch: 54 | Local Epoch: 5 | Generator Loss: 15.727396 | Discriminator Loss: 0.020958\n","| Global Epoch: 54 | Local Epoch: 6 | Generator Loss: 15.795830 | Discriminator Loss: 0.018110\n","| Global Epoch: 54 | Local Epoch: 7 | Generator Loss: 15.944137 | Discriminator Loss: 0.015967\n","| Global Epoch: 54 | Local Epoch: 8 | Generator Loss: 15.995180 | Discriminator Loss: 0.014704\n","| Global Epoch: 54 | Local Epoch: 9 | Generator Loss: 16.029760 | Discriminator Loss: 0.014889\n","After 55 epoch global training, averged local generator loss is: 16.0355, averged local discrimitor loss is: 0.0788\n","Averged validation loss is: 6.0147\n","| Global Epoch: 55 | Local Epoch: 0 | Generator Loss: 15.725324 | Discriminator Loss: 0.127401\n","| Global Epoch: 55 | Local Epoch: 1 | Generator Loss: 15.513731 | Discriminator Loss: 0.073523\n","| Global Epoch: 55 | Local Epoch: 2 | Generator Loss: 15.526036 | Discriminator Loss: 0.050725\n","| Global Epoch: 55 | Local Epoch: 3 | Generator Loss: 15.302524 | Discriminator Loss: 0.039861\n","| Global Epoch: 55 | Local Epoch: 4 | Generator Loss: 15.141139 | Discriminator Loss: 0.032381\n","| Global Epoch: 55 | Local Epoch: 5 | Generator Loss: 15.293817 | Discriminator Loss: 0.027291\n","| Global Epoch: 55 | Local Epoch: 6 | Generator Loss: 15.405388 | Discriminator Loss: 0.023571\n","| Global Epoch: 55 | Local Epoch: 7 | Generator Loss: 15.521674 | Discriminator Loss: 0.020774\n","| Global Epoch: 55 | Local Epoch: 8 | Generator Loss: 15.632838 | Discriminator Loss: 0.018566\n","| Global Epoch: 55 | Local Epoch: 9 | Generator Loss: 15.685976 | Discriminator Loss: 0.016783\n","| Global Epoch: 55 | Local Epoch: 0 | Generator Loss: 15.919041 | Discriminator Loss: 0.088915\n","| Global Epoch: 55 | Local Epoch: 1 | Generator Loss: 15.276532 | Discriminator Loss: 0.052311\n","| Global Epoch: 55 | Local Epoch: 2 | Generator Loss: 15.345464 | Discriminator Loss: 0.036085\n","| Global Epoch: 55 | Local Epoch: 3 | Generator Loss: 15.536276 | Discriminator Loss: 0.027551\n","| Global Epoch: 55 | Local Epoch: 4 | Generator Loss: 15.743837 | Discriminator Loss: 0.022933\n","| Global Epoch: 55 | Local Epoch: 5 | Generator Loss: 15.932331 | Discriminator Loss: 0.020382\n","| Global Epoch: 55 | Local Epoch: 6 | Generator Loss: 15.991134 | Discriminator Loss: 0.018127\n","| Global Epoch: 55 | Local Epoch: 7 | Generator Loss: 16.142177 | Discriminator Loss: 0.016870\n","| Global Epoch: 55 | Local Epoch: 8 | Generator Loss: 16.184661 | Discriminator Loss: 0.016318\n","| Global Epoch: 55 | Local Epoch: 9 | Generator Loss: 16.200528 | Discriminator Loss: 0.018607\n","| Global Epoch: 55 | Local Epoch: 0 | Generator Loss: 14.571021 | Discriminator Loss: 0.686806\n","| Global Epoch: 55 | Local Epoch: 1 | Generator Loss: 14.502925 | Discriminator Loss: 0.367963\n","| Global Epoch: 55 | Local Epoch: 2 | Generator Loss: 14.737109 | Discriminator Loss: 0.247760\n","| Global Epoch: 55 | Local Epoch: 3 | Generator Loss: 14.738349 | Discriminator Loss: 0.195287\n","| Global Epoch: 55 | Local Epoch: 4 | Generator Loss: 15.049954 | Discriminator Loss: 0.156976\n","| Global Epoch: 55 | Local Epoch: 5 | Generator Loss: 15.462957 | Discriminator Loss: 0.131881\n","| Global Epoch: 55 | Local Epoch: 6 | Generator Loss: 15.815649 | Discriminator Loss: 0.113552\n","| Global Epoch: 55 | Local Epoch: 7 | Generator Loss: 16.086565 | Discriminator Loss: 0.099853\n","| Global Epoch: 55 | Local Epoch: 8 | Generator Loss: 16.292406 | Discriminator Loss: 0.089054\n","| Global Epoch: 55 | Local Epoch: 9 | Generator Loss: 16.395993 | Discriminator Loss: 0.080235\n","| Global Epoch: 55 | Local Epoch: 0 | Generator Loss: 15.939741 | Discriminator Loss: 0.128824\n","| Global Epoch: 55 | Local Epoch: 1 | Generator Loss: 15.873008 | Discriminator Loss: 0.070108\n","| Global Epoch: 55 | Local Epoch: 2 | Generator Loss: 15.628373 | Discriminator Loss: 0.048198\n","| Global Epoch: 55 | Local Epoch: 3 | Generator Loss: 15.841328 | Discriminator Loss: 0.036595\n","| Global Epoch: 55 | Local Epoch: 4 | Generator Loss: 15.938644 | Discriminator Loss: 0.029518\n","| Global Epoch: 55 | Local Epoch: 5 | Generator Loss: 15.942820 | Discriminator Loss: 0.029902\n","| Global Epoch: 55 | Local Epoch: 6 | Generator Loss: 15.970881 | Discriminator Loss: 0.030848\n","| Global Epoch: 55 | Local Epoch: 7 | Generator Loss: 16.024015 | Discriminator Loss: 0.028726\n","| Global Epoch: 55 | Local Epoch: 8 | Generator Loss: 16.150857 | Discriminator Loss: 0.026097\n","| Global Epoch: 55 | Local Epoch: 9 | Generator Loss: 16.230357 | Discriminator Loss: 0.028007\n","| Global Epoch: 55 | Local Epoch: 0 | Generator Loss: 15.781880 | Discriminator Loss: 0.349012\n","| Global Epoch: 55 | Local Epoch: 1 | Generator Loss: 15.143598 | Discriminator Loss: 0.196512\n","| Global Epoch: 55 | Local Epoch: 2 | Generator Loss: 15.377490 | Discriminator Loss: 0.136083\n","| Global Epoch: 55 | Local Epoch: 3 | Generator Loss: 15.227638 | Discriminator Loss: 0.104443\n","| Global Epoch: 55 | Local Epoch: 4 | Generator Loss: 15.260657 | Discriminator Loss: 0.084456\n","| Global Epoch: 55 | Local Epoch: 5 | Generator Loss: 15.380176 | Discriminator Loss: 0.070966\n","| Global Epoch: 55 | Local Epoch: 6 | Generator Loss: 15.567013 | Discriminator Loss: 0.061251\n","| Global Epoch: 55 | Local Epoch: 7 | Generator Loss: 15.655718 | Discriminator Loss: 0.053870\n","| Global Epoch: 55 | Local Epoch: 8 | Generator Loss: 15.708950 | Discriminator Loss: 0.048097\n","| Global Epoch: 55 | Local Epoch: 9 | Generator Loss: 15.743289 | Discriminator Loss: 0.043452\n","After 56 epoch global training, averged local generator loss is: 16.0303, averged local discrimitor loss is: 0.0782\n","Averged validation loss is: 5.1937\n","| Global Epoch: 56 | Local Epoch: 0 | Generator Loss: 15.906967 | Discriminator Loss: 0.121751\n","| Global Epoch: 56 | Local Epoch: 1 | Generator Loss: 15.829169 | Discriminator Loss: 0.179766\n","| Global Epoch: 56 | Local Epoch: 2 | Generator Loss: 15.038751 | Discriminator Loss: 0.146740\n","| Global Epoch: 56 | Local Epoch: 3 | Generator Loss: 14.846881 | Discriminator Loss: 0.114422\n","| Global Epoch: 56 | Local Epoch: 4 | Generator Loss: 14.841021 | Discriminator Loss: 0.098086\n","| Global Epoch: 56 | Local Epoch: 5 | Generator Loss: 14.793843 | Discriminator Loss: 0.083332\n","| Global Epoch: 56 | Local Epoch: 6 | Generator Loss: 14.769180 | Discriminator Loss: 0.074468\n","| Global Epoch: 56 | Local Epoch: 7 | Generator Loss: 14.487439 | Discriminator Loss: 0.086382\n","| Global Epoch: 56 | Local Epoch: 8 | Generator Loss: 14.333090 | Discriminator Loss: 0.088021\n","| Global Epoch: 56 | Local Epoch: 9 | Generator Loss: 14.378988 | Discriminator Loss: 0.081624\n","| Global Epoch: 56 | Local Epoch: 0 | Generator Loss: 14.264596 | Discriminator Loss: 0.135006\n","| Global Epoch: 56 | Local Epoch: 1 | Generator Loss: 14.272199 | Discriminator Loss: 0.074162\n","| Global Epoch: 56 | Local Epoch: 2 | Generator Loss: 14.356425 | Discriminator Loss: 0.050677\n","| Global Epoch: 56 | Local Epoch: 3 | Generator Loss: 14.353283 | Discriminator Loss: 0.038559\n","| Global Epoch: 56 | Local Epoch: 4 | Generator Loss: 14.505113 | Discriminator Loss: 0.031186\n","| Global Epoch: 56 | Local Epoch: 5 | Generator Loss: 14.665427 | Discriminator Loss: 0.026211\n","| Global Epoch: 56 | Local Epoch: 6 | Generator Loss: 14.836305 | Discriminator Loss: 0.022645\n","| Global Epoch: 56 | Local Epoch: 7 | Generator Loss: 15.006774 | Discriminator Loss: 0.019961\n","| Global Epoch: 56 | Local Epoch: 8 | Generator Loss: 15.218178 | Discriminator Loss: 0.017860\n","| Global Epoch: 56 | Local Epoch: 9 | Generator Loss: 15.383530 | Discriminator Loss: 0.016163\n","| Global Epoch: 56 | Local Epoch: 0 | Generator Loss: 16.729641 | Discriminator Loss: 0.128004\n","| Global Epoch: 56 | Local Epoch: 1 | Generator Loss: 16.059235 | Discriminator Loss: 0.077318\n","| Global Epoch: 56 | Local Epoch: 2 | Generator Loss: 16.101489 | Discriminator Loss: 0.053360\n","| Global Epoch: 56 | Local Epoch: 3 | Generator Loss: 16.074399 | Discriminator Loss: 0.040603\n","| Global Epoch: 56 | Local Epoch: 4 | Generator Loss: 16.032104 | Discriminator Loss: 0.032804\n","| Global Epoch: 56 | Local Epoch: 5 | Generator Loss: 16.080667 | Discriminator Loss: 0.027649\n","| Global Epoch: 56 | Local Epoch: 6 | Generator Loss: 16.115939 | Discriminator Loss: 0.023891\n","| Global Epoch: 56 | Local Epoch: 7 | Generator Loss: 16.099425 | Discriminator Loss: 0.021028\n","| Global Epoch: 56 | Local Epoch: 8 | Generator Loss: 16.100881 | Discriminator Loss: 0.018958\n","| Global Epoch: 56 | Local Epoch: 9 | Generator Loss: 16.171819 | Discriminator Loss: 0.017165\n","| Global Epoch: 56 | Local Epoch: 0 | Generator Loss: 15.940303 | Discriminator Loss: 0.134334\n","| Global Epoch: 56 | Local Epoch: 1 | Generator Loss: 15.518807 | Discriminator Loss: 0.080950\n","| Global Epoch: 56 | Local Epoch: 2 | Generator Loss: 15.606510 | Discriminator Loss: 0.060094\n","| Global Epoch: 56 | Local Epoch: 3 | Generator Loss: 15.556104 | Discriminator Loss: 0.046222\n","| Global Epoch: 56 | Local Epoch: 4 | Generator Loss: 15.526626 | Discriminator Loss: 0.037460\n","| Global Epoch: 56 | Local Epoch: 5 | Generator Loss: 15.524440 | Discriminator Loss: 0.031503\n","| Global Epoch: 56 | Local Epoch: 6 | Generator Loss: 15.503506 | Discriminator Loss: 0.027202\n","| Global Epoch: 56 | Local Epoch: 7 | Generator Loss: 15.479443 | Discriminator Loss: 0.023938\n","| Global Epoch: 56 | Local Epoch: 8 | Generator Loss: 15.477150 | Discriminator Loss: 0.021393\n","| Global Epoch: 56 | Local Epoch: 9 | Generator Loss: 15.534577 | Discriminator Loss: 0.020844\n","| Global Epoch: 56 | Local Epoch: 0 | Generator Loss: 12.603294 | Discriminator Loss: 0.367721\n","| Global Epoch: 56 | Local Epoch: 1 | Generator Loss: 13.282067 | Discriminator Loss: 0.189744\n","| Global Epoch: 56 | Local Epoch: 2 | Generator Loss: 13.841240 | Discriminator Loss: 0.130068\n","| Global Epoch: 56 | Local Epoch: 3 | Generator Loss: 14.061802 | Discriminator Loss: 0.098581\n","| Global Epoch: 56 | Local Epoch: 4 | Generator Loss: 14.143589 | Discriminator Loss: 0.079302\n","| Global Epoch: 56 | Local Epoch: 5 | Generator Loss: 14.177056 | Discriminator Loss: 0.066418\n","| Global Epoch: 56 | Local Epoch: 6 | Generator Loss: 14.143382 | Discriminator Loss: 0.057075\n","| Global Epoch: 56 | Local Epoch: 7 | Generator Loss: 14.086710 | Discriminator Loss: 0.050085\n","| Global Epoch: 56 | Local Epoch: 8 | Generator Loss: 14.090553 | Discriminator Loss: 0.044604\n","| Global Epoch: 56 | Local Epoch: 9 | Generator Loss: 14.108764 | Discriminator Loss: 0.040212\n","After 57 epoch global training, averged local generator loss is: 15.9966, averged local discrimitor loss is: 0.0775\n","Averged validation loss is: 5.4139\n","| Global Epoch: 57 | Local Epoch: 0 | Generator Loss: 15.368214 | Discriminator Loss: 0.033608\n","| Global Epoch: 57 | Local Epoch: 1 | Generator Loss: 15.587901 | Discriminator Loss: 0.020983\n","| Global Epoch: 57 | Local Epoch: 2 | Generator Loss: 15.670867 | Discriminator Loss: 0.014694\n","| Global Epoch: 57 | Local Epoch: 3 | Generator Loss: 15.765950 | Discriminator Loss: 0.011348\n","| Global Epoch: 57 | Local Epoch: 4 | Generator Loss: 15.872092 | Discriminator Loss: 0.009298\n","| Global Epoch: 57 | Local Epoch: 5 | Generator Loss: 15.973823 | Discriminator Loss: 0.007875\n","| Global Epoch: 57 | Local Epoch: 6 | Generator Loss: 15.918128 | Discriminator Loss: 0.006833\n","| Global Epoch: 57 | Local Epoch: 7 | Generator Loss: 15.888837 | Discriminator Loss: 0.006051\n","| Global Epoch: 57 | Local Epoch: 8 | Generator Loss: 15.886290 | Discriminator Loss: 0.005461\n","| Global Epoch: 57 | Local Epoch: 9 | Generator Loss: 15.896088 | Discriminator Loss: 0.004996\n","| Global Epoch: 57 | Local Epoch: 0 | Generator Loss: 15.886058 | Discriminator Loss: 0.146068\n","| Global Epoch: 57 | Local Epoch: 1 | Generator Loss: 15.247687 | Discriminator Loss: 0.084471\n","| Global Epoch: 57 | Local Epoch: 2 | Generator Loss: 15.589421 | Discriminator Loss: 0.058623\n","| Global Epoch: 57 | Local Epoch: 3 | Generator Loss: 15.691665 | Discriminator Loss: 0.044662\n","| Global Epoch: 57 | Local Epoch: 4 | Generator Loss: 15.836185 | Discriminator Loss: 0.036129\n","| Global Epoch: 57 | Local Epoch: 5 | Generator Loss: 15.810399 | Discriminator Loss: 0.030361\n","| Global Epoch: 57 | Local Epoch: 6 | Generator Loss: 15.921271 | Discriminator Loss: 0.026271\n","| Global Epoch: 57 | Local Epoch: 7 | Generator Loss: 15.810551 | Discriminator Loss: 0.028059\n","| Global Epoch: 57 | Local Epoch: 8 | Generator Loss: 15.685155 | Discriminator Loss: 0.031180\n","| Global Epoch: 57 | Local Epoch: 9 | Generator Loss: 15.637861 | Discriminator Loss: 0.031713\n","| Global Epoch: 57 | Local Epoch: 0 | Generator Loss: 16.167301 | Discriminator Loss: 0.249474\n","| Global Epoch: 57 | Local Epoch: 1 | Generator Loss: 15.837943 | Discriminator Loss: 0.135974\n","| Global Epoch: 57 | Local Epoch: 2 | Generator Loss: 15.806913 | Discriminator Loss: 0.092604\n","| Global Epoch: 57 | Local Epoch: 3 | Generator Loss: 15.649838 | Discriminator Loss: 0.070376\n","| Global Epoch: 57 | Local Epoch: 4 | Generator Loss: 15.641132 | Discriminator Loss: 0.056892\n","| Global Epoch: 57 | Local Epoch: 5 | Generator Loss: 15.675142 | Discriminator Loss: 0.047771\n","| Global Epoch: 57 | Local Epoch: 6 | Generator Loss: 15.658260 | Discriminator Loss: 0.041214\n","| Global Epoch: 57 | Local Epoch: 7 | Generator Loss: 15.649130 | Discriminator Loss: 0.036249\n","| Global Epoch: 57 | Local Epoch: 8 | Generator Loss: 15.716915 | Discriminator Loss: 0.032388\n","| Global Epoch: 57 | Local Epoch: 9 | Generator Loss: 15.578252 | Discriminator Loss: 0.032092\n","| Global Epoch: 57 | Local Epoch: 0 | Generator Loss: 15.109427 | Discriminator Loss: 0.260429\n","| Global Epoch: 57 | Local Epoch: 1 | Generator Loss: 14.255747 | Discriminator Loss: 0.150741\n","| Global Epoch: 57 | Local Epoch: 2 | Generator Loss: 13.923853 | Discriminator Loss: 0.118236\n","| Global Epoch: 57 | Local Epoch: 3 | Generator Loss: 13.642507 | Discriminator Loss: 0.111831\n","| Global Epoch: 57 | Local Epoch: 4 | Generator Loss: 13.337302 | Discriminator Loss: 0.105534\n","| Global Epoch: 57 | Local Epoch: 5 | Generator Loss: 13.380726 | Discriminator Loss: 0.092625\n","| Global Epoch: 57 | Local Epoch: 6 | Generator Loss: 13.562057 | Discriminator Loss: 0.081079\n","| Global Epoch: 57 | Local Epoch: 7 | Generator Loss: 13.714948 | Discriminator Loss: 0.088459\n","| Global Epoch: 57 | Local Epoch: 8 | Generator Loss: 13.670352 | Discriminator Loss: 0.091377\n","| Global Epoch: 57 | Local Epoch: 9 | Generator Loss: 13.540959 | Discriminator Loss: 0.089153\n","| Global Epoch: 57 | Local Epoch: 0 | Generator Loss: 14.153403 | Discriminator Loss: 0.599122\n","| Global Epoch: 57 | Local Epoch: 1 | Generator Loss: 13.619393 | Discriminator Loss: 0.336752\n","| Global Epoch: 57 | Local Epoch: 2 | Generator Loss: 13.385449 | Discriminator Loss: 0.230936\n","| Global Epoch: 57 | Local Epoch: 3 | Generator Loss: 13.375089 | Discriminator Loss: 0.175833\n","| Global Epoch: 57 | Local Epoch: 4 | Generator Loss: 13.479511 | Discriminator Loss: 0.142124\n","| Global Epoch: 57 | Local Epoch: 5 | Generator Loss: 13.575138 | Discriminator Loss: 0.121523\n","| Global Epoch: 57 | Local Epoch: 6 | Generator Loss: 13.641146 | Discriminator Loss: 0.105695\n","| Global Epoch: 57 | Local Epoch: 7 | Generator Loss: 13.695782 | Discriminator Loss: 0.093161\n","| Global Epoch: 57 | Local Epoch: 8 | Generator Loss: 13.838418 | Discriminator Loss: 0.083273\n","| Global Epoch: 57 | Local Epoch: 9 | Generator Loss: 13.924856 | Discriminator Loss: 0.075241\n","After 58 epoch global training, averged local generator loss is: 15.9609, averged local discrimitor loss is: 0.0775\n","Averged validation loss is: 5.4273\n","| Global Epoch: 58 | Local Epoch: 0 | Generator Loss: 14.932453 | Discriminator Loss: 0.117389\n","| Global Epoch: 58 | Local Epoch: 1 | Generator Loss: 14.884941 | Discriminator Loss: 0.065981\n","| Global Epoch: 58 | Local Epoch: 2 | Generator Loss: 15.127895 | Discriminator Loss: 0.045735\n","| Global Epoch: 58 | Local Epoch: 3 | Generator Loss: 15.115524 | Discriminator Loss: 0.034922\n","| Global Epoch: 58 | Local Epoch: 4 | Generator Loss: 15.286648 | Discriminator Loss: 0.028634\n","| Global Epoch: 58 | Local Epoch: 5 | Generator Loss: 15.192769 | Discriminator Loss: 0.030883\n","| Global Epoch: 58 | Local Epoch: 6 | Generator Loss: 15.091034 | Discriminator Loss: 0.033289\n","| Global Epoch: 58 | Local Epoch: 7 | Generator Loss: 15.080396 | Discriminator Loss: 0.030021\n","| Global Epoch: 58 | Local Epoch: 8 | Generator Loss: 15.117271 | Discriminator Loss: 0.026947\n","| Global Epoch: 58 | Local Epoch: 9 | Generator Loss: 15.173975 | Discriminator Loss: 0.024395\n","| Global Epoch: 58 | Local Epoch: 0 | Generator Loss: 14.925622 | Discriminator Loss: 0.048772\n","| Global Epoch: 58 | Local Epoch: 1 | Generator Loss: 15.138042 | Discriminator Loss: 0.028305\n","| Global Epoch: 58 | Local Epoch: 2 | Generator Loss: 15.420046 | Discriminator Loss: 0.020907\n","| Global Epoch: 58 | Local Epoch: 3 | Generator Loss: 15.244079 | Discriminator Loss: 0.023477\n","| Global Epoch: 58 | Local Epoch: 4 | Generator Loss: 15.357469 | Discriminator Loss: 0.050666\n","| Global Epoch: 58 | Local Epoch: 5 | Generator Loss: 15.175031 | Discriminator Loss: 0.057925\n","| Global Epoch: 58 | Local Epoch: 6 | Generator Loss: 15.141386 | Discriminator Loss: 0.071054\n","| Global Epoch: 58 | Local Epoch: 7 | Generator Loss: 14.778203 | Discriminator Loss: 0.074327\n","| Global Epoch: 58 | Local Epoch: 8 | Generator Loss: 14.677554 | Discriminator Loss: 0.066926\n","| Global Epoch: 58 | Local Epoch: 9 | Generator Loss: 14.613910 | Discriminator Loss: 0.060508\n","| Global Epoch: 58 | Local Epoch: 0 | Generator Loss: 14.210802 | Discriminator Loss: 0.201214\n","| Global Epoch: 58 | Local Epoch: 1 | Generator Loss: 14.237207 | Discriminator Loss: 0.111032\n","| Global Epoch: 58 | Local Epoch: 2 | Generator Loss: 14.195218 | Discriminator Loss: 0.080316\n","| Global Epoch: 58 | Local Epoch: 3 | Generator Loss: 14.314943 | Discriminator Loss: 0.061301\n","| Global Epoch: 58 | Local Epoch: 4 | Generator Loss: 14.481927 | Discriminator Loss: 0.049804\n","| Global Epoch: 58 | Local Epoch: 5 | Generator Loss: 14.496742 | Discriminator Loss: 0.042251\n","| Global Epoch: 58 | Local Epoch: 6 | Generator Loss: 14.642174 | Discriminator Loss: 0.036501\n","| Global Epoch: 58 | Local Epoch: 7 | Generator Loss: 14.611398 | Discriminator Loss: 0.032174\n","| Global Epoch: 58 | Local Epoch: 8 | Generator Loss: 14.745786 | Discriminator Loss: 0.028773\n","| Global Epoch: 58 | Local Epoch: 9 | Generator Loss: 14.826908 | Discriminator Loss: 0.026017\n","| Global Epoch: 58 | Local Epoch: 0 | Generator Loss: 15.321990 | Discriminator Loss: 0.183831\n","| Global Epoch: 58 | Local Epoch: 1 | Generator Loss: 15.104000 | Discriminator Loss: 0.104413\n","| Global Epoch: 58 | Local Epoch: 2 | Generator Loss: 15.065830 | Discriminator Loss: 0.071868\n","| Global Epoch: 58 | Local Epoch: 3 | Generator Loss: 15.270656 | Discriminator Loss: 0.055842\n","| Global Epoch: 58 | Local Epoch: 4 | Generator Loss: 15.426736 | Discriminator Loss: 0.045703\n","| Global Epoch: 58 | Local Epoch: 5 | Generator Loss: 15.435246 | Discriminator Loss: 0.038482\n","| Global Epoch: 58 | Local Epoch: 6 | Generator Loss: 15.463389 | Discriminator Loss: 0.033255\n","| Global Epoch: 58 | Local Epoch: 7 | Generator Loss: 15.490396 | Discriminator Loss: 0.029297\n","| Global Epoch: 58 | Local Epoch: 8 | Generator Loss: 15.492353 | Discriminator Loss: 0.026185\n","| Global Epoch: 58 | Local Epoch: 9 | Generator Loss: 15.481463 | Discriminator Loss: 0.023690\n","| Global Epoch: 58 | Local Epoch: 0 | Generator Loss: 15.878901 | Discriminator Loss: 0.132093\n","| Global Epoch: 58 | Local Epoch: 1 | Generator Loss: 15.570404 | Discriminator Loss: 0.077837\n","| Global Epoch: 58 | Local Epoch: 2 | Generator Loss: 15.420956 | Discriminator Loss: 0.053636\n","| Global Epoch: 58 | Local Epoch: 3 | Generator Loss: 15.389921 | Discriminator Loss: 0.040889\n","| Global Epoch: 58 | Local Epoch: 4 | Generator Loss: 15.371358 | Discriminator Loss: 0.033082\n","| Global Epoch: 58 | Local Epoch: 5 | Generator Loss: 15.412791 | Discriminator Loss: 0.027814\n","| Global Epoch: 58 | Local Epoch: 6 | Generator Loss: 15.603122 | Discriminator Loss: 0.024065\n","| Global Epoch: 58 | Local Epoch: 7 | Generator Loss: 15.664026 | Discriminator Loss: 0.021191\n","| Global Epoch: 58 | Local Epoch: 8 | Generator Loss: 15.716607 | Discriminator Loss: 0.019378\n","| Global Epoch: 58 | Local Epoch: 9 | Generator Loss: 15.694243 | Discriminator Loss: 0.025421\n","After 59 epoch global training, averged local generator loss is: 15.9564, averged local discrimitor loss is: 0.0766\n","Averged validation loss is: 6.0340\n","| Global Epoch: 59 | Local Epoch: 0 | Generator Loss: 15.346979 | Discriminator Loss: 0.239833\n","| Global Epoch: 59 | Local Epoch: 1 | Generator Loss: 15.164742 | Discriminator Loss: 0.132213\n","| Global Epoch: 59 | Local Epoch: 2 | Generator Loss: 15.166557 | Discriminator Loss: 0.090737\n","| Global Epoch: 59 | Local Epoch: 3 | Generator Loss: 15.127606 | Discriminator Loss: 0.069064\n","| Global Epoch: 59 | Local Epoch: 4 | Generator Loss: 15.322065 | Discriminator Loss: 0.055900\n","| Global Epoch: 59 | Local Epoch: 5 | Generator Loss: 15.399484 | Discriminator Loss: 0.046952\n","| Global Epoch: 59 | Local Epoch: 6 | Generator Loss: 15.553984 | Discriminator Loss: 0.040510\n","| Global Epoch: 59 | Local Epoch: 7 | Generator Loss: 15.670134 | Discriminator Loss: 0.035630\n","| Global Epoch: 59 | Local Epoch: 8 | Generator Loss: 15.714300 | Discriminator Loss: 0.031817\n","| Global Epoch: 59 | Local Epoch: 9 | Generator Loss: 15.831088 | Discriminator Loss: 0.028767\n","| Global Epoch: 59 | Local Epoch: 0 | Generator Loss: 17.380455 | Discriminator Loss: 0.249653\n","| Global Epoch: 59 | Local Epoch: 1 | Generator Loss: 17.149012 | Discriminator Loss: 0.138445\n","| Global Epoch: 59 | Local Epoch: 2 | Generator Loss: 16.924797 | Discriminator Loss: 0.094508\n","| Global Epoch: 59 | Local Epoch: 3 | Generator Loss: 16.719351 | Discriminator Loss: 0.071666\n","| Global Epoch: 59 | Local Epoch: 4 | Generator Loss: 16.564505 | Discriminator Loss: 0.057823\n","| Global Epoch: 59 | Local Epoch: 5 | Generator Loss: 16.589106 | Discriminator Loss: 0.048545\n","| Global Epoch: 59 | Local Epoch: 6 | Generator Loss: 16.620106 | Discriminator Loss: 0.041869\n","| Global Epoch: 59 | Local Epoch: 7 | Generator Loss: 16.650125 | Discriminator Loss: 0.036850\n","| Global Epoch: 59 | Local Epoch: 8 | Generator Loss: 16.786812 | Discriminator Loss: 0.032912\n","| Global Epoch: 59 | Local Epoch: 9 | Generator Loss: 16.837014 | Discriminator Loss: 0.029740\n","| Global Epoch: 59 | Local Epoch: 0 | Generator Loss: 16.441185 | Discriminator Loss: 0.056638\n","| Global Epoch: 59 | Local Epoch: 1 | Generator Loss: 16.016378 | Discriminator Loss: 0.053767\n","| Global Epoch: 59 | Local Epoch: 2 | Generator Loss: 16.177202 | Discriminator Loss: 0.040534\n","| Global Epoch: 59 | Local Epoch: 3 | Generator Loss: 16.097422 | Discriminator Loss: 0.031051\n","| Global Epoch: 59 | Local Epoch: 4 | Generator Loss: 16.048099 | Discriminator Loss: 0.025177\n","| Global Epoch: 59 | Local Epoch: 5 | Generator Loss: 16.060172 | Discriminator Loss: 0.021247\n","| Global Epoch: 59 | Local Epoch: 6 | Generator Loss: 16.202806 | Discriminator Loss: 0.018420\n","| Global Epoch: 59 | Local Epoch: 7 | Generator Loss: 16.257051 | Discriminator Loss: 0.016231\n","| Global Epoch: 59 | Local Epoch: 8 | Generator Loss: 16.363806 | Discriminator Loss: 0.014632\n","| Global Epoch: 59 | Local Epoch: 9 | Generator Loss: 16.377666 | Discriminator Loss: 0.017103\n","| Global Epoch: 59 | Local Epoch: 0 | Generator Loss: 16.440313 | Discriminator Loss: 0.236103\n","| Global Epoch: 59 | Local Epoch: 1 | Generator Loss: 16.036905 | Discriminator Loss: 0.129928\n","| Global Epoch: 59 | Local Epoch: 2 | Generator Loss: 15.835452 | Discriminator Loss: 0.088711\n","| Global Epoch: 59 | Local Epoch: 3 | Generator Loss: 15.735084 | Discriminator Loss: 0.067394\n","| Global Epoch: 59 | Local Epoch: 4 | Generator Loss: 15.650994 | Discriminator Loss: 0.054433\n","| Global Epoch: 59 | Local Epoch: 5 | Generator Loss: 15.571129 | Discriminator Loss: 0.045737\n","| Global Epoch: 59 | Local Epoch: 6 | Generator Loss: 15.702880 | Discriminator Loss: 0.039598\n","| Global Epoch: 59 | Local Epoch: 7 | Generator Loss: 15.740004 | Discriminator Loss: 0.034875\n","| Global Epoch: 59 | Local Epoch: 8 | Generator Loss: 15.778776 | Discriminator Loss: 0.031156\n","| Global Epoch: 59 | Local Epoch: 9 | Generator Loss: 15.851999 | Discriminator Loss: 0.028173\n","| Global Epoch: 59 | Local Epoch: 0 | Generator Loss: 15.373444 | Discriminator Loss: 0.066055\n","| Global Epoch: 59 | Local Epoch: 1 | Generator Loss: 15.663046 | Discriminator Loss: 0.038639\n","| Global Epoch: 59 | Local Epoch: 2 | Generator Loss: 15.706111 | Discriminator Loss: 0.026724\n","| Global Epoch: 59 | Local Epoch: 3 | Generator Loss: 15.977442 | Discriminator Loss: 0.020417\n","| Global Epoch: 59 | Local Epoch: 4 | Generator Loss: 16.043684 | Discriminator Loss: 0.016536\n","| Global Epoch: 59 | Local Epoch: 5 | Generator Loss: 16.065761 | Discriminator Loss: 0.013915\n","| Global Epoch: 59 | Local Epoch: 6 | Generator Loss: 16.168963 | Discriminator Loss: 0.012048\n","| Global Epoch: 59 | Local Epoch: 7 | Generator Loss: 16.222065 | Discriminator Loss: 0.010620\n","| Global Epoch: 59 | Local Epoch: 8 | Generator Loss: 16.238693 | Discriminator Loss: 0.009501\n","| Global Epoch: 59 | Local Epoch: 9 | Generator Loss: 16.261675 | Discriminator Loss: 0.008603\n","After 60 epoch global training, averged local generator loss is: 15.9615, averged local discrimitor loss is: 0.0755\n","Averged validation loss is: 6.4782\n","| Global Epoch: 60 | Local Epoch: 0 | Generator Loss: 16.805680 | Discriminator Loss: 0.138607\n","| Global Epoch: 60 | Local Epoch: 1 | Generator Loss: 17.065341 | Discriminator Loss: 0.071846\n","| Global Epoch: 60 | Local Epoch: 2 | Generator Loss: 16.808333 | Discriminator Loss: 0.049181\n","| Global Epoch: 60 | Local Epoch: 3 | Generator Loss: 16.651118 | Discriminator Loss: 0.037737\n","| Global Epoch: 60 | Local Epoch: 4 | Generator Loss: 16.528980 | Discriminator Loss: 0.030546\n","| Global Epoch: 60 | Local Epoch: 5 | Generator Loss: 16.325534 | Discriminator Loss: 0.025603\n","| Global Epoch: 60 | Local Epoch: 6 | Generator Loss: 16.224407 | Discriminator Loss: 0.022034\n","| Global Epoch: 60 | Local Epoch: 7 | Generator Loss: 16.140527 | Discriminator Loss: 0.019347\n","| Global Epoch: 60 | Local Epoch: 8 | Generator Loss: 16.054536 | Discriminator Loss: 0.017250\n","| Global Epoch: 60 | Local Epoch: 9 | Generator Loss: 15.954756 | Discriminator Loss: 0.015624\n","| Global Epoch: 60 | Local Epoch: 0 | Generator Loss: 14.686865 | Discriminator Loss: 0.082776\n","| Global Epoch: 60 | Local Epoch: 1 | Generator Loss: 14.608478 | Discriminator Loss: 0.046075\n","| Global Epoch: 60 | Local Epoch: 2 | Generator Loss: 14.743857 | Discriminator Loss: 0.031673\n","| Global Epoch: 60 | Local Epoch: 3 | Generator Loss: 14.927247 | Discriminator Loss: 0.024877\n","| Global Epoch: 60 | Local Epoch: 4 | Generator Loss: 15.033402 | Discriminator Loss: 0.020234\n","| Global Epoch: 60 | Local Epoch: 5 | Generator Loss: 15.159810 | Discriminator Loss: 0.017054\n","| Global Epoch: 60 | Local Epoch: 6 | Generator Loss: 15.238549 | Discriminator Loss: 0.014753\n","| Global Epoch: 60 | Local Epoch: 7 | Generator Loss: 15.297968 | Discriminator Loss: 0.013007\n","| Global Epoch: 60 | Local Epoch: 8 | Generator Loss: 15.366380 | Discriminator Loss: 0.011649\n","| Global Epoch: 60 | Local Epoch: 9 | Generator Loss: 15.428661 | Discriminator Loss: 0.010547\n","| Global Epoch: 60 | Local Epoch: 0 | Generator Loss: 15.568940 | Discriminator Loss: 0.056647\n","| Global Epoch: 60 | Local Epoch: 1 | Generator Loss: 15.560384 | Discriminator Loss: 0.033776\n","| Global Epoch: 60 | Local Epoch: 2 | Generator Loss: 15.466588 | Discriminator Loss: 0.023302\n","| Global Epoch: 60 | Local Epoch: 3 | Generator Loss: 15.346577 | Discriminator Loss: 0.017756\n","| Global Epoch: 60 | Local Epoch: 4 | Generator Loss: 15.444205 | Discriminator Loss: 0.015820\n","| Global Epoch: 60 | Local Epoch: 5 | Generator Loss: 15.406956 | Discriminator Loss: 0.030608\n","| Global Epoch: 60 | Local Epoch: 6 | Generator Loss: 15.405469 | Discriminator Loss: 0.031390\n","| Global Epoch: 60 | Local Epoch: 7 | Generator Loss: 15.325907 | Discriminator Loss: 0.041205\n","| Global Epoch: 60 | Local Epoch: 8 | Generator Loss: 15.312637 | Discriminator Loss: 0.039620\n","| Global Epoch: 60 | Local Epoch: 9 | Generator Loss: 15.254296 | Discriminator Loss: 0.041749\n","| Global Epoch: 60 | Local Epoch: 0 | Generator Loss: 15.005452 | Discriminator Loss: 0.152508\n","| Global Epoch: 60 | Local Epoch: 1 | Generator Loss: 14.901101 | Discriminator Loss: 0.082618\n","| Global Epoch: 60 | Local Epoch: 2 | Generator Loss: 15.076126 | Discriminator Loss: 0.056438\n","| Global Epoch: 60 | Local Epoch: 3 | Generator Loss: 15.260829 | Discriminator Loss: 0.043034\n","| Global Epoch: 60 | Local Epoch: 4 | Generator Loss: 15.390901 | Discriminator Loss: 0.034788\n","| Global Epoch: 60 | Local Epoch: 5 | Generator Loss: 15.530592 | Discriminator Loss: 0.029238\n","| Global Epoch: 60 | Local Epoch: 6 | Generator Loss: 15.618304 | Discriminator Loss: 0.025211\n","| Global Epoch: 60 | Local Epoch: 7 | Generator Loss: 15.636113 | Discriminator Loss: 0.022170\n","| Global Epoch: 60 | Local Epoch: 8 | Generator Loss: 15.647937 | Discriminator Loss: 0.019795\n","| Global Epoch: 60 | Local Epoch: 9 | Generator Loss: 15.721778 | Discriminator Loss: 0.017893\n","| Global Epoch: 60 | Local Epoch: 0 | Generator Loss: 16.138812 | Discriminator Loss: 0.123218\n","| Global Epoch: 60 | Local Epoch: 1 | Generator Loss: 16.032898 | Discriminator Loss: 0.071348\n","| Global Epoch: 60 | Local Epoch: 2 | Generator Loss: 16.037509 | Discriminator Loss: 0.049309\n","| Global Epoch: 60 | Local Epoch: 3 | Generator Loss: 16.030066 | Discriminator Loss: 0.037663\n","| Global Epoch: 60 | Local Epoch: 4 | Generator Loss: 15.895936 | Discriminator Loss: 0.031142\n","| Global Epoch: 60 | Local Epoch: 5 | Generator Loss: 16.033615 | Discriminator Loss: 0.026371\n","| Global Epoch: 60 | Local Epoch: 6 | Generator Loss: 16.092025 | Discriminator Loss: 0.022796\n","| Global Epoch: 60 | Local Epoch: 7 | Generator Loss: 16.137113 | Discriminator Loss: 0.020091\n","| Global Epoch: 60 | Local Epoch: 8 | Generator Loss: 16.191506 | Discriminator Loss: 0.017970\n","| Global Epoch: 60 | Local Epoch: 9 | Generator Loss: 16.263184 | Discriminator Loss: 0.016263\n","After 61 epoch global training, averged local generator loss is: 15.9664, averged local discrimitor loss is: 0.0745\n","Averged validation loss is: 5.6317\n","| Global Epoch: 61 | Local Epoch: 0 | Generator Loss: 16.130128 | Discriminator Loss: 0.237667\n","| Global Epoch: 61 | Local Epoch: 1 | Generator Loss: 15.758248 | Discriminator Loss: 0.135630\n","| Global Epoch: 61 | Local Epoch: 2 | Generator Loss: 15.537438 | Discriminator Loss: 0.093129\n","| Global Epoch: 61 | Local Epoch: 3 | Generator Loss: 15.411432 | Discriminator Loss: 0.070817\n","| Global Epoch: 61 | Local Epoch: 4 | Generator Loss: 15.333335 | Discriminator Loss: 0.057196\n","| Global Epoch: 61 | Local Epoch: 5 | Generator Loss: 15.359785 | Discriminator Loss: 0.048055\n","| Global Epoch: 61 | Local Epoch: 6 | Generator Loss: 15.333766 | Discriminator Loss: 0.041456\n","| Global Epoch: 61 | Local Epoch: 7 | Generator Loss: 15.319497 | Discriminator Loss: 0.036466\n","| Global Epoch: 61 | Local Epoch: 8 | Generator Loss: 15.391379 | Discriminator Loss: 0.032601\n","| Global Epoch: 61 | Local Epoch: 9 | Generator Loss: 15.553547 | Discriminator Loss: 0.029515\n","| Global Epoch: 61 | Local Epoch: 0 | Generator Loss: 15.808788 | Discriminator Loss: 0.101717\n","| Global Epoch: 61 | Local Epoch: 1 | Generator Loss: 16.292342 | Discriminator Loss: 0.055727\n","| Global Epoch: 61 | Local Epoch: 2 | Generator Loss: 16.185535 | Discriminator Loss: 0.038110\n","| Global Epoch: 61 | Local Epoch: 3 | Generator Loss: 16.066225 | Discriminator Loss: 0.029233\n","| Global Epoch: 61 | Local Epoch: 4 | Generator Loss: 16.019952 | Discriminator Loss: 0.023667\n","| Global Epoch: 61 | Local Epoch: 5 | Generator Loss: 15.979003 | Discriminator Loss: 0.019901\n","| Global Epoch: 61 | Local Epoch: 6 | Generator Loss: 15.800212 | Discriminator Loss: 0.017169\n","| Global Epoch: 61 | Local Epoch: 7 | Generator Loss: 15.698913 | Discriminator Loss: 0.015101\n","| Global Epoch: 61 | Local Epoch: 8 | Generator Loss: 15.637598 | Discriminator Loss: 0.013481\n","| Global Epoch: 61 | Local Epoch: 9 | Generator Loss: 15.644790 | Discriminator Loss: 0.012183\n","| Global Epoch: 61 | Local Epoch: 0 | Generator Loss: 14.847421 | Discriminator Loss: 0.046006\n","| Global Epoch: 61 | Local Epoch: 1 | Generator Loss: 15.378730 | Discriminator Loss: 0.025222\n","| Global Epoch: 61 | Local Epoch: 2 | Generator Loss: 15.626863 | Discriminator Loss: 0.020556\n","| Global Epoch: 61 | Local Epoch: 3 | Generator Loss: 15.478856 | Discriminator Loss: 0.028991\n","| Global Epoch: 61 | Local Epoch: 4 | Generator Loss: 15.435840 | Discriminator Loss: 0.025483\n","| Global Epoch: 61 | Local Epoch: 5 | Generator Loss: 15.479439 | Discriminator Loss: 0.027934\n","| Global Epoch: 61 | Local Epoch: 6 | Generator Loss: 15.587855 | Discriminator Loss: 0.031282\n","| Global Epoch: 61 | Local Epoch: 7 | Generator Loss: 15.431604 | Discriminator Loss: 0.045402\n","| Global Epoch: 61 | Local Epoch: 8 | Generator Loss: 15.467290 | Discriminator Loss: 0.043215\n","| Global Epoch: 61 | Local Epoch: 9 | Generator Loss: 15.478087 | Discriminator Loss: 0.039493\n","| Global Epoch: 61 | Local Epoch: 0 | Generator Loss: 15.909424 | Discriminator Loss: 0.383996\n","| Global Epoch: 61 | Local Epoch: 1 | Generator Loss: 15.294726 | Discriminator Loss: 0.208353\n","| Global Epoch: 61 | Local Epoch: 2 | Generator Loss: 15.052538 | Discriminator Loss: 0.141736\n","| Global Epoch: 61 | Local Epoch: 3 | Generator Loss: 14.902258 | Discriminator Loss: 0.107571\n","| Global Epoch: 61 | Local Epoch: 4 | Generator Loss: 14.850007 | Discriminator Loss: 0.086936\n","| Global Epoch: 61 | Local Epoch: 5 | Generator Loss: 14.959176 | Discriminator Loss: 0.073004\n","| Global Epoch: 61 | Local Epoch: 6 | Generator Loss: 15.044588 | Discriminator Loss: 0.063005\n","| Global Epoch: 61 | Local Epoch: 7 | Generator Loss: 15.094233 | Discriminator Loss: 0.055426\n","| Global Epoch: 61 | Local Epoch: 8 | Generator Loss: 15.235964 | Discriminator Loss: 0.049538\n","| Global Epoch: 61 | Local Epoch: 9 | Generator Loss: 15.338324 | Discriminator Loss: 0.044771\n","| Global Epoch: 61 | Local Epoch: 0 | Generator Loss: 15.785833 | Discriminator Loss: 0.228896\n","| Global Epoch: 61 | Local Epoch: 1 | Generator Loss: 15.511115 | Discriminator Loss: 0.127597\n","| Global Epoch: 61 | Local Epoch: 2 | Generator Loss: 15.238739 | Discriminator Loss: 0.087623\n","| Global Epoch: 61 | Local Epoch: 3 | Generator Loss: 15.042736 | Discriminator Loss: 0.066652\n","| Global Epoch: 61 | Local Epoch: 4 | Generator Loss: 15.030361 | Discriminator Loss: 0.053870\n","| Global Epoch: 61 | Local Epoch: 5 | Generator Loss: 15.021409 | Discriminator Loss: 0.045262\n","| Global Epoch: 61 | Local Epoch: 6 | Generator Loss: 14.994285 | Discriminator Loss: 0.039070\n","| Global Epoch: 61 | Local Epoch: 7 | Generator Loss: 15.065761 | Discriminator Loss: 0.034589\n","| Global Epoch: 61 | Local Epoch: 8 | Generator Loss: 15.063266 | Discriminator Loss: 0.033013\n","| Global Epoch: 61 | Local Epoch: 9 | Generator Loss: 15.198177 | Discriminator Loss: 0.030393\n","After 62 epoch global training, averged local generator loss is: 15.9540, averged local discrimitor loss is: 0.0738\n","Averged validation loss is: 5.8352\n","| Global Epoch: 62 | Local Epoch: 0 | Generator Loss: 15.125059 | Discriminator Loss: 0.208852\n","| Global Epoch: 62 | Local Epoch: 1 | Generator Loss: 14.928045 | Discriminator Loss: 0.121541\n","| Global Epoch: 62 | Local Epoch: 2 | Generator Loss: 14.898628 | Discriminator Loss: 0.083688\n","| Global Epoch: 62 | Local Epoch: 3 | Generator Loss: 15.103783 | Discriminator Loss: 0.070746\n","| Global Epoch: 62 | Local Epoch: 4 | Generator Loss: 14.919741 | Discriminator Loss: 0.074387\n","| Global Epoch: 62 | Local Epoch: 5 | Generator Loss: 14.727395 | Discriminator Loss: 0.072097\n","| Global Epoch: 62 | Local Epoch: 6 | Generator Loss: 14.656343 | Discriminator Loss: 0.063212\n","| Global Epoch: 62 | Local Epoch: 7 | Generator Loss: 14.646904 | Discriminator Loss: 0.055806\n","| Global Epoch: 62 | Local Epoch: 8 | Generator Loss: 14.639823 | Discriminator Loss: 0.050159\n","| Global Epoch: 62 | Local Epoch: 9 | Generator Loss: 14.645771 | Discriminator Loss: 0.045365\n","| Global Epoch: 62 | Local Epoch: 0 | Generator Loss: 15.453604 | Discriminator Loss: 0.151798\n","| Global Epoch: 62 | Local Epoch: 1 | Generator Loss: 15.009606 | Discriminator Loss: 0.084026\n","| Global Epoch: 62 | Local Epoch: 2 | Generator Loss: 14.990409 | Discriminator Loss: 0.059846\n","| Global Epoch: 62 | Local Epoch: 3 | Generator Loss: 14.983194 | Discriminator Loss: 0.045640\n","| Global Epoch: 62 | Local Epoch: 4 | Generator Loss: 14.947225 | Discriminator Loss: 0.036862\n","| Global Epoch: 62 | Local Epoch: 5 | Generator Loss: 14.848110 | Discriminator Loss: 0.030993\n","| Global Epoch: 62 | Local Epoch: 6 | Generator Loss: 14.868943 | Discriminator Loss: 0.026768\n","| Global Epoch: 62 | Local Epoch: 7 | Generator Loss: 14.808462 | Discriminator Loss: 0.023517\n","| Global Epoch: 62 | Local Epoch: 8 | Generator Loss: 14.756468 | Discriminator Loss: 0.021032\n","| Global Epoch: 62 | Local Epoch: 9 | Generator Loss: 14.785755 | Discriminator Loss: 0.018984\n","| Global Epoch: 62 | Local Epoch: 0 | Generator Loss: 14.697944 | Discriminator Loss: 0.054179\n","| Global Epoch: 62 | Local Epoch: 1 | Generator Loss: 14.831125 | Discriminator Loss: 0.029431\n","| Global Epoch: 62 | Local Epoch: 2 | Generator Loss: 15.068981 | Discriminator Loss: 0.020199\n","| Global Epoch: 62 | Local Epoch: 3 | Generator Loss: 15.248773 | Discriminator Loss: 0.015432\n","| Global Epoch: 62 | Local Epoch: 4 | Generator Loss: 15.325075 | Discriminator Loss: 0.012517\n","| Global Epoch: 62 | Local Epoch: 5 | Generator Loss: 15.349614 | Discriminator Loss: 0.010708\n","| Global Epoch: 62 | Local Epoch: 6 | Generator Loss: 15.363105 | Discriminator Loss: 0.013707\n","| Global Epoch: 62 | Local Epoch: 7 | Generator Loss: 15.273595 | Discriminator Loss: 0.024337\n","| Global Epoch: 62 | Local Epoch: 8 | Generator Loss: 15.015188 | Discriminator Loss: 0.046652\n","| Global Epoch: 62 | Local Epoch: 9 | Generator Loss: 14.925206 | Discriminator Loss: 0.044757\n","| Global Epoch: 62 | Local Epoch: 0 | Generator Loss: 13.928899 | Discriminator Loss: 0.236023\n","| Global Epoch: 62 | Local Epoch: 1 | Generator Loss: 14.061495 | Discriminator Loss: 0.129128\n","| Global Epoch: 62 | Local Epoch: 2 | Generator Loss: 14.264299 | Discriminator Loss: 0.088302\n","| Global Epoch: 62 | Local Epoch: 3 | Generator Loss: 14.383520 | Discriminator Loss: 0.067368\n","| Global Epoch: 62 | Local Epoch: 4 | Generator Loss: 14.390362 | Discriminator Loss: 0.054564\n","| Global Epoch: 62 | Local Epoch: 5 | Generator Loss: 14.538358 | Discriminator Loss: 0.045936\n","| Global Epoch: 62 | Local Epoch: 6 | Generator Loss: 14.661348 | Discriminator Loss: 0.040008\n","| Global Epoch: 62 | Local Epoch: 7 | Generator Loss: 14.622136 | Discriminator Loss: 0.035358\n","| Global Epoch: 62 | Local Epoch: 8 | Generator Loss: 14.641079 | Discriminator Loss: 0.034415\n","| Global Epoch: 62 | Local Epoch: 9 | Generator Loss: 14.649954 | Discriminator Loss: 0.032068\n","| Global Epoch: 62 | Local Epoch: 0 | Generator Loss: 15.538497 | Discriminator Loss: 0.176015\n","| Global Epoch: 62 | Local Epoch: 1 | Generator Loss: 15.100171 | Discriminator Loss: 0.102613\n","| Global Epoch: 62 | Local Epoch: 2 | Generator Loss: 15.198031 | Discriminator Loss: 0.070456\n","| Global Epoch: 62 | Local Epoch: 3 | Generator Loss: 15.346959 | Discriminator Loss: 0.053647\n","| Global Epoch: 62 | Local Epoch: 4 | Generator Loss: 15.417165 | Discriminator Loss: 0.043427\n","| Global Epoch: 62 | Local Epoch: 5 | Generator Loss: 15.457484 | Discriminator Loss: 0.036503\n","| Global Epoch: 62 | Local Epoch: 6 | Generator Loss: 15.472627 | Discriminator Loss: 0.031502\n","| Global Epoch: 62 | Local Epoch: 7 | Generator Loss: 15.601231 | Discriminator Loss: 0.027774\n","| Global Epoch: 62 | Local Epoch: 8 | Generator Loss: 15.666919 | Discriminator Loss: 0.024822\n","| Global Epoch: 62 | Local Epoch: 9 | Generator Loss: 15.734376 | Discriminator Loss: 0.022585\n","After 63 epoch global training, averged local generator loss is: 15.9505, averged local discrimitor loss is: 0.0730\n","Averged validation loss is: 5.4564\n","| Global Epoch: 63 | Local Epoch: 0 | Generator Loss: 16.299743 | Discriminator Loss: 0.222888\n","| Global Epoch: 63 | Local Epoch: 1 | Generator Loss: 16.110153 | Discriminator Loss: 0.125127\n","| Global Epoch: 63 | Local Epoch: 2 | Generator Loss: 16.093096 | Discriminator Loss: 0.088115\n","| Global Epoch: 63 | Local Epoch: 3 | Generator Loss: 16.263780 | Discriminator Loss: 0.068033\n","| Global Epoch: 63 | Local Epoch: 4 | Generator Loss: 16.273567 | Discriminator Loss: 0.055328\n","| Global Epoch: 63 | Local Epoch: 5 | Generator Loss: 16.286206 | Discriminator Loss: 0.046608\n","| Global Epoch: 63 | Local Epoch: 6 | Generator Loss: 16.340820 | Discriminator Loss: 0.040259\n","| Global Epoch: 63 | Local Epoch: 7 | Generator Loss: 16.387971 | Discriminator Loss: 0.035705\n","| Global Epoch: 63 | Local Epoch: 8 | Generator Loss: 16.350327 | Discriminator Loss: 0.032532\n","| Global Epoch: 63 | Local Epoch: 9 | Generator Loss: 16.469839 | Discriminator Loss: 0.029609\n","| Global Epoch: 63 | Local Epoch: 0 | Generator Loss: 17.584026 | Discriminator Loss: 0.024979\n","| Global Epoch: 63 | Local Epoch: 1 | Generator Loss: 17.377084 | Discriminator Loss: 0.013773\n","| Global Epoch: 63 | Local Epoch: 2 | Generator Loss: 17.359756 | Discriminator Loss: 0.009543\n","| Global Epoch: 63 | Local Epoch: 3 | Generator Loss: 17.119069 | Discriminator Loss: 0.007389\n","| Global Epoch: 63 | Local Epoch: 4 | Generator Loss: 17.069910 | Discriminator Loss: 0.006032\n","| Global Epoch: 63 | Local Epoch: 5 | Generator Loss: 16.982133 | Discriminator Loss: 0.005083\n","| Global Epoch: 63 | Local Epoch: 6 | Generator Loss: 16.970717 | Discriminator Loss: 0.004399\n","| Global Epoch: 63 | Local Epoch: 7 | Generator Loss: 17.014762 | Discriminator Loss: 0.003882\n","| Global Epoch: 63 | Local Epoch: 8 | Generator Loss: 16.996157 | Discriminator Loss: 0.003470\n","| Global Epoch: 63 | Local Epoch: 9 | Generator Loss: 17.046262 | Discriminator Loss: 0.003144\n","| Global Epoch: 63 | Local Epoch: 0 | Generator Loss: 15.992814 | Discriminator Loss: 0.066702\n","| Global Epoch: 63 | Local Epoch: 1 | Generator Loss: 16.185307 | Discriminator Loss: 0.045556\n","| Global Epoch: 63 | Local Epoch: 2 | Generator Loss: 15.686757 | Discriminator Loss: 0.032575\n","| Global Epoch: 63 | Local Epoch: 3 | Generator Loss: 15.576683 | Discriminator Loss: 0.024903\n","| Global Epoch: 63 | Local Epoch: 4 | Generator Loss: 15.507506 | Discriminator Loss: 0.020172\n","| Global Epoch: 63 | Local Epoch: 5 | Generator Loss: 15.491946 | Discriminator Loss: 0.017028\n","| Global Epoch: 63 | Local Epoch: 6 | Generator Loss: 15.627365 | Discriminator Loss: 0.014782\n","| Global Epoch: 63 | Local Epoch: 7 | Generator Loss: 15.717704 | Discriminator Loss: 0.013043\n","| Global Epoch: 63 | Local Epoch: 8 | Generator Loss: 15.776194 | Discriminator Loss: 0.011674\n","| Global Epoch: 63 | Local Epoch: 9 | Generator Loss: 15.830144 | Discriminator Loss: 0.010568\n","| Global Epoch: 63 | Local Epoch: 0 | Generator Loss: 16.043761 | Discriminator Loss: 0.021418\n","| Global Epoch: 63 | Local Epoch: 1 | Generator Loss: 15.494241 | Discriminator Loss: 0.019896\n","| Global Epoch: 63 | Local Epoch: 2 | Generator Loss: 15.659532 | Discriminator Loss: 0.018935\n","| Global Epoch: 63 | Local Epoch: 3 | Generator Loss: 15.574360 | Discriminator Loss: 0.031855\n","| Global Epoch: 63 | Local Epoch: 4 | Generator Loss: 15.592883 | Discriminator Loss: 0.039209\n","| Global Epoch: 63 | Local Epoch: 5 | Generator Loss: 15.538261 | Discriminator Loss: 0.033651\n","| Global Epoch: 63 | Local Epoch: 6 | Generator Loss: 15.429319 | Discriminator Loss: 0.029283\n","| Global Epoch: 63 | Local Epoch: 7 | Generator Loss: 15.401323 | Discriminator Loss: 0.026928\n","| Global Epoch: 63 | Local Epoch: 8 | Generator Loss: 15.366248 | Discriminator Loss: 0.024844\n","| Global Epoch: 63 | Local Epoch: 9 | Generator Loss: 15.421051 | Discriminator Loss: 0.023175\n","| Global Epoch: 63 | Local Epoch: 0 | Generator Loss: 16.123169 | Discriminator Loss: 0.172321\n","| Global Epoch: 63 | Local Epoch: 1 | Generator Loss: 15.907672 | Discriminator Loss: 0.097487\n","| Global Epoch: 63 | Local Epoch: 2 | Generator Loss: 15.956957 | Discriminator Loss: 0.066771\n","| Global Epoch: 63 | Local Epoch: 3 | Generator Loss: 16.007546 | Discriminator Loss: 0.050775\n","| Global Epoch: 63 | Local Epoch: 4 | Generator Loss: 16.101926 | Discriminator Loss: 0.044743\n","| Global Epoch: 63 | Local Epoch: 5 | Generator Loss: 16.029960 | Discriminator Loss: 0.045083\n","| Global Epoch: 63 | Local Epoch: 6 | Generator Loss: 15.999738 | Discriminator Loss: 0.039758\n","| Global Epoch: 63 | Local Epoch: 7 | Generator Loss: 16.034390 | Discriminator Loss: 0.035130\n","| Global Epoch: 63 | Local Epoch: 8 | Generator Loss: 16.101460 | Discriminator Loss: 0.032534\n","| Global Epoch: 63 | Local Epoch: 9 | Generator Loss: 16.105034 | Discriminator Loss: 0.029714\n","After 64 epoch global training, averged local generator loss is: 15.9529, averged local discrimitor loss is: 0.0723\n","Averged validation loss is: 5.2808\n","| Global Epoch: 64 | Local Epoch: 0 | Generator Loss: 16.312967 | Discriminator Loss: 0.213435\n","| Global Epoch: 64 | Local Epoch: 1 | Generator Loss: 15.692531 | Discriminator Loss: 0.122955\n","| Global Epoch: 64 | Local Epoch: 2 | Generator Loss: 15.344677 | Discriminator Loss: 0.084590\n","| Global Epoch: 64 | Local Epoch: 3 | Generator Loss: 15.344890 | Discriminator Loss: 0.070208\n","| Global Epoch: 64 | Local Epoch: 4 | Generator Loss: 15.338132 | Discriminator Loss: 0.061635\n","| Global Epoch: 64 | Local Epoch: 5 | Generator Loss: 15.277685 | Discriminator Loss: 0.052149\n","| Global Epoch: 64 | Local Epoch: 6 | Generator Loss: 15.325040 | Discriminator Loss: 0.045365\n","| Global Epoch: 64 | Local Epoch: 7 | Generator Loss: 15.352543 | Discriminator Loss: 0.039960\n","| Global Epoch: 64 | Local Epoch: 8 | Generator Loss: 15.378914 | Discriminator Loss: 0.035719\n","| Global Epoch: 64 | Local Epoch: 9 | Generator Loss: 15.489467 | Discriminator Loss: 0.032409\n","| Global Epoch: 64 | Local Epoch: 0 | Generator Loss: 15.810350 | Discriminator Loss: 0.080543\n","| Global Epoch: 64 | Local Epoch: 1 | Generator Loss: 15.516388 | Discriminator Loss: 0.045936\n","| Global Epoch: 64 | Local Epoch: 2 | Generator Loss: 15.659751 | Discriminator Loss: 0.032100\n","| Global Epoch: 64 | Local Epoch: 3 | Generator Loss: 15.547954 | Discriminator Loss: 0.024582\n","| Global Epoch: 64 | Local Epoch: 4 | Generator Loss: 15.560713 | Discriminator Loss: 0.019957\n","| Global Epoch: 64 | Local Epoch: 5 | Generator Loss: 15.573909 | Discriminator Loss: 0.016812\n","| Global Epoch: 64 | Local Epoch: 6 | Generator Loss: 15.568327 | Discriminator Loss: 0.014532\n","| Global Epoch: 64 | Local Epoch: 7 | Generator Loss: 15.573899 | Discriminator Loss: 0.012809\n","| Global Epoch: 64 | Local Epoch: 8 | Generator Loss: 15.609752 | Discriminator Loss: 0.011473\n","| Global Epoch: 64 | Local Epoch: 9 | Generator Loss: 15.685687 | Discriminator Loss: 0.010392\n","| Global Epoch: 64 | Local Epoch: 0 | Generator Loss: 15.866684 | Discriminator Loss: 0.162601\n","| Global Epoch: 64 | Local Epoch: 1 | Generator Loss: 15.626361 | Discriminator Loss: 0.092790\n","| Global Epoch: 64 | Local Epoch: 2 | Generator Loss: 15.457967 | Discriminator Loss: 0.063619\n","| Global Epoch: 64 | Local Epoch: 3 | Generator Loss: 15.466929 | Discriminator Loss: 0.048430\n","| Global Epoch: 64 | Local Epoch: 4 | Generator Loss: 15.506494 | Discriminator Loss: 0.039157\n","| Global Epoch: 64 | Local Epoch: 5 | Generator Loss: 15.677756 | Discriminator Loss: 0.032893\n","| Global Epoch: 64 | Local Epoch: 6 | Generator Loss: 15.763272 | Discriminator Loss: 0.028377\n","| Global Epoch: 64 | Local Epoch: 7 | Generator Loss: 15.793276 | Discriminator Loss: 0.024977\n","| Global Epoch: 64 | Local Epoch: 8 | Generator Loss: 15.805461 | Discriminator Loss: 0.022309\n","| Global Epoch: 64 | Local Epoch: 9 | Generator Loss: 15.843987 | Discriminator Loss: 0.020182\n","| Global Epoch: 64 | Local Epoch: 0 | Generator Loss: 16.554614 | Discriminator Loss: 0.068776\n","| Global Epoch: 64 | Local Epoch: 1 | Generator Loss: 15.966921 | Discriminator Loss: 0.038537\n","| Global Epoch: 64 | Local Epoch: 2 | Generator Loss: 15.760003 | Discriminator Loss: 0.026741\n","| Global Epoch: 64 | Local Epoch: 3 | Generator Loss: 15.740610 | Discriminator Loss: 0.020410\n","| Global Epoch: 64 | Local Epoch: 4 | Generator Loss: 15.767774 | Discriminator Loss: 0.016542\n","| Global Epoch: 64 | Local Epoch: 5 | Generator Loss: 15.689777 | Discriminator Loss: 0.013914\n","| Global Epoch: 64 | Local Epoch: 6 | Generator Loss: 15.699726 | Discriminator Loss: 0.012027\n","| Global Epoch: 64 | Local Epoch: 7 | Generator Loss: 15.672258 | Discriminator Loss: 0.010595\n","| Global Epoch: 64 | Local Epoch: 8 | Generator Loss: 15.640026 | Discriminator Loss: 0.009483\n","| Global Epoch: 64 | Local Epoch: 9 | Generator Loss: 15.670662 | Discriminator Loss: 0.008614\n","| Global Epoch: 64 | Local Epoch: 0 | Generator Loss: 16.460810 | Discriminator Loss: 0.189042\n","| Global Epoch: 64 | Local Epoch: 1 | Generator Loss: 16.098434 | Discriminator Loss: 0.106231\n","| Global Epoch: 64 | Local Epoch: 2 | Generator Loss: 15.986078 | Discriminator Loss: 0.072678\n","| Global Epoch: 64 | Local Epoch: 3 | Generator Loss: 15.972346 | Discriminator Loss: 0.055314\n","| Global Epoch: 64 | Local Epoch: 4 | Generator Loss: 16.173580 | Discriminator Loss: 0.044943\n","| Global Epoch: 64 | Local Epoch: 5 | Generator Loss: 16.175078 | Discriminator Loss: 0.037770\n","| Global Epoch: 64 | Local Epoch: 6 | Generator Loss: 16.248682 | Discriminator Loss: 0.032598\n","| Global Epoch: 64 | Local Epoch: 7 | Generator Loss: 16.273484 | Discriminator Loss: 0.028744\n","| Global Epoch: 64 | Local Epoch: 8 | Generator Loss: 16.219883 | Discriminator Loss: 0.025824\n","| Global Epoch: 64 | Local Epoch: 9 | Generator Loss: 16.182801 | Discriminator Loss: 0.023351\n","After 65 epoch global training, averged local generator loss is: 15.9565, averged local discrimitor loss is: 0.0715\n","Averged validation loss is: 5.0382\n","| Global Epoch: 65 | Local Epoch: 0 | Generator Loss: 15.940835 | Discriminator Loss: 0.022709\n","| Global Epoch: 65 | Local Epoch: 1 | Generator Loss: 15.717908 | Discriminator Loss: 0.018418\n","| Global Epoch: 65 | Local Epoch: 2 | Generator Loss: 15.286180 | Discriminator Loss: 0.042181\n","| Global Epoch: 65 | Local Epoch: 3 | Generator Loss: 15.119938 | Discriminator Loss: 0.044453\n","| Global Epoch: 65 | Local Epoch: 4 | Generator Loss: 15.221939 | Discriminator Loss: 0.036427\n","| Global Epoch: 65 | Local Epoch: 5 | Generator Loss: 15.305131 | Discriminator Loss: 0.033202\n","| Global Epoch: 65 | Local Epoch: 6 | Generator Loss: 15.108675 | Discriminator Loss: 0.038602\n","| Global Epoch: 65 | Local Epoch: 7 | Generator Loss: 15.152899 | Discriminator Loss: 0.038554\n","| Global Epoch: 65 | Local Epoch: 8 | Generator Loss: 15.198354 | Discriminator Loss: 0.040672\n","| Global Epoch: 65 | Local Epoch: 9 | Generator Loss: 15.293400 | Discriminator Loss: 0.038159\n","| Global Epoch: 65 | Local Epoch: 0 | Generator Loss: 14.982796 | Discriminator Loss: 0.133062\n","| Global Epoch: 65 | Local Epoch: 1 | Generator Loss: 14.963462 | Discriminator Loss: 0.073251\n","| Global Epoch: 65 | Local Epoch: 2 | Generator Loss: 15.085603 | Discriminator Loss: 0.050113\n","| Global Epoch: 65 | Local Epoch: 3 | Generator Loss: 15.195612 | Discriminator Loss: 0.038145\n","| Global Epoch: 65 | Local Epoch: 4 | Generator Loss: 15.237855 | Discriminator Loss: 0.030850\n","| Global Epoch: 65 | Local Epoch: 5 | Generator Loss: 15.274358 | Discriminator Loss: 0.025931\n","| Global Epoch: 65 | Local Epoch: 6 | Generator Loss: 15.421562 | Discriminator Loss: 0.022412\n","| Global Epoch: 65 | Local Epoch: 7 | Generator Loss: 15.527552 | Discriminator Loss: 0.019730\n","| Global Epoch: 65 | Local Epoch: 8 | Generator Loss: 15.649063 | Discriminator Loss: 0.017633\n","| Global Epoch: 65 | Local Epoch: 9 | Generator Loss: 15.767615 | Discriminator Loss: 0.015989\n","| Global Epoch: 65 | Local Epoch: 0 | Generator Loss: 16.457747 | Discriminator Loss: 0.283453\n","| Global Epoch: 65 | Local Epoch: 1 | Generator Loss: 15.801193 | Discriminator Loss: 0.156399\n","| Global Epoch: 65 | Local Epoch: 2 | Generator Loss: 15.506168 | Discriminator Loss: 0.107205\n","| Global Epoch: 65 | Local Epoch: 3 | Generator Loss: 15.362558 | Discriminator Loss: 0.081541\n","| Global Epoch: 65 | Local Epoch: 4 | Generator Loss: 15.287426 | Discriminator Loss: 0.065868\n","| Global Epoch: 65 | Local Epoch: 5 | Generator Loss: 15.390174 | Discriminator Loss: 0.055457\n","| Global Epoch: 65 | Local Epoch: 6 | Generator Loss: 15.414129 | Discriminator Loss: 0.047861\n","| Global Epoch: 65 | Local Epoch: 7 | Generator Loss: 15.516253 | Discriminator Loss: 0.042294\n","| Global Epoch: 65 | Local Epoch: 8 | Generator Loss: 15.637040 | Discriminator Loss: 0.037815\n","| Global Epoch: 65 | Local Epoch: 9 | Generator Loss: 15.695010 | Discriminator Loss: 0.034181\n","| Global Epoch: 65 | Local Epoch: 0 | Generator Loss: 15.926928 | Discriminator Loss: 0.094750\n","| Global Epoch: 65 | Local Epoch: 1 | Generator Loss: 15.562371 | Discriminator Loss: 0.055865\n","| Global Epoch: 65 | Local Epoch: 2 | Generator Loss: 15.398804 | Discriminator Loss: 0.038520\n","| Global Epoch: 65 | Local Epoch: 3 | Generator Loss: 15.532189 | Discriminator Loss: 0.030203\n","| Global Epoch: 65 | Local Epoch: 4 | Generator Loss: 15.703149 | Discriminator Loss: 0.024606\n","| Global Epoch: 65 | Local Epoch: 5 | Generator Loss: 15.773644 | Discriminator Loss: 0.020724\n","| Global Epoch: 65 | Local Epoch: 6 | Generator Loss: 15.779852 | Discriminator Loss: 0.017908\n","| Global Epoch: 65 | Local Epoch: 7 | Generator Loss: 15.883372 | Discriminator Loss: 0.015847\n","| Global Epoch: 65 | Local Epoch: 8 | Generator Loss: 15.944888 | Discriminator Loss: 0.014183\n","| Global Epoch: 65 | Local Epoch: 9 | Generator Loss: 15.977341 | Discriminator Loss: 0.012835\n","| Global Epoch: 65 | Local Epoch: 0 | Generator Loss: 15.921878 | Discriminator Loss: 0.080814\n","| Global Epoch: 65 | Local Epoch: 1 | Generator Loss: 16.167495 | Discriminator Loss: 0.046887\n","| Global Epoch: 65 | Local Epoch: 2 | Generator Loss: 16.232006 | Discriminator Loss: 0.032450\n","| Global Epoch: 65 | Local Epoch: 3 | Generator Loss: 16.165659 | Discriminator Loss: 0.024770\n","| Global Epoch: 65 | Local Epoch: 4 | Generator Loss: 16.140836 | Discriminator Loss: 0.020073\n","| Global Epoch: 65 | Local Epoch: 5 | Generator Loss: 16.180288 | Discriminator Loss: 0.016976\n","| Global Epoch: 65 | Local Epoch: 6 | Generator Loss: 16.113292 | Discriminator Loss: 0.019682\n","| Global Epoch: 65 | Local Epoch: 7 | Generator Loss: 15.930989 | Discriminator Loss: 0.030880\n","| Global Epoch: 65 | Local Epoch: 8 | Generator Loss: 15.824552 | Discriminator Loss: 0.033219\n","| Global Epoch: 65 | Local Epoch: 9 | Generator Loss: 15.711324 | Discriminator Loss: 0.032155\n","After 66 epoch global training, averged local generator loss is: 15.9528, averged local discrimitor loss is: 0.0710\n","Averged validation loss is: 6.0375\n","| Global Epoch: 66 | Local Epoch: 0 | Generator Loss: 14.592756 | Discriminator Loss: 0.062991\n","| Global Epoch: 66 | Local Epoch: 1 | Generator Loss: 14.968118 | Discriminator Loss: 0.043429\n","| Global Epoch: 66 | Local Epoch: 2 | Generator Loss: 14.765459 | Discriminator Loss: 0.051970\n","| Global Epoch: 66 | Local Epoch: 3 | Generator Loss: 14.660164 | Discriminator Loss: 0.049988\n","| Global Epoch: 66 | Local Epoch: 4 | Generator Loss: 14.878863 | Discriminator Loss: 0.045482\n","| Global Epoch: 66 | Local Epoch: 5 | Generator Loss: 14.725097 | Discriminator Loss: 0.064612\n","| Global Epoch: 66 | Local Epoch: 6 | Generator Loss: 14.481710 | Discriminator Loss: 0.087618\n","| Global Epoch: 66 | Local Epoch: 7 | Generator Loss: 14.321511 | Discriminator Loss: 0.083099\n","| Global Epoch: 66 | Local Epoch: 8 | Generator Loss: 14.256607 | Discriminator Loss: 0.074648\n","| Global Epoch: 66 | Local Epoch: 9 | Generator Loss: 14.249204 | Discriminator Loss: 0.068403\n","| Global Epoch: 66 | Local Epoch: 0 | Generator Loss: 13.415713 | Discriminator Loss: 0.296332\n","| Global Epoch: 66 | Local Epoch: 1 | Generator Loss: 13.398113 | Discriminator Loss: 0.163338\n","| Global Epoch: 66 | Local Epoch: 2 | Generator Loss: 13.468587 | Discriminator Loss: 0.111730\n","| Global Epoch: 66 | Local Epoch: 3 | Generator Loss: 13.509524 | Discriminator Loss: 0.085016\n","| Global Epoch: 66 | Local Epoch: 4 | Generator Loss: 13.811827 | Discriminator Loss: 0.069031\n","| Global Epoch: 66 | Local Epoch: 5 | Generator Loss: 13.895670 | Discriminator Loss: 0.057997\n","| Global Epoch: 66 | Local Epoch: 6 | Generator Loss: 13.971284 | Discriminator Loss: 0.050074\n","| Global Epoch: 66 | Local Epoch: 7 | Generator Loss: 14.080397 | Discriminator Loss: 0.044070\n","| Global Epoch: 66 | Local Epoch: 8 | Generator Loss: 14.249700 | Discriminator Loss: 0.039454\n","| Global Epoch: 66 | Local Epoch: 9 | Generator Loss: 14.330348 | Discriminator Loss: 0.035680\n","| Global Epoch: 66 | Local Epoch: 0 | Generator Loss: 14.841098 | Discriminator Loss: 0.047189\n","| Global Epoch: 66 | Local Epoch: 1 | Generator Loss: 14.866228 | Discriminator Loss: 0.032352\n","| Global Epoch: 66 | Local Epoch: 2 | Generator Loss: 14.936576 | Discriminator Loss: 0.022854\n","| Global Epoch: 66 | Local Epoch: 3 | Generator Loss: 15.197418 | Discriminator Loss: 0.018658\n","| Global Epoch: 66 | Local Epoch: 4 | Generator Loss: 15.156710 | Discriminator Loss: 0.023005\n","| Global Epoch: 66 | Local Epoch: 5 | Generator Loss: 15.192102 | Discriminator Loss: 0.019947\n","| Global Epoch: 66 | Local Epoch: 6 | Generator Loss: 15.312114 | Discriminator Loss: 0.017405\n","| Global Epoch: 66 | Local Epoch: 7 | Generator Loss: 15.378810 | Discriminator Loss: 0.015365\n","| Global Epoch: 66 | Local Epoch: 8 | Generator Loss: 15.460517 | Discriminator Loss: 0.013796\n","| Global Epoch: 66 | Local Epoch: 9 | Generator Loss: 15.536561 | Discriminator Loss: 0.013004\n","| Global Epoch: 66 | Local Epoch: 0 | Generator Loss: 15.664393 | Discriminator Loss: 0.181755\n","| Global Epoch: 66 | Local Epoch: 1 | Generator Loss: 15.539981 | Discriminator Loss: 0.108200\n","| Global Epoch: 66 | Local Epoch: 2 | Generator Loss: 15.462047 | Discriminator Loss: 0.074636\n","| Global Epoch: 66 | Local Epoch: 3 | Generator Loss: 15.478350 | Discriminator Loss: 0.056918\n","| Global Epoch: 66 | Local Epoch: 4 | Generator Loss: 15.598653 | Discriminator Loss: 0.046997\n","| Global Epoch: 66 | Local Epoch: 5 | Generator Loss: 15.637517 | Discriminator Loss: 0.039632\n","| Global Epoch: 66 | Local Epoch: 6 | Generator Loss: 15.627208 | Discriminator Loss: 0.034205\n","| Global Epoch: 66 | Local Epoch: 7 | Generator Loss: 15.640005 | Discriminator Loss: 0.030095\n","| Global Epoch: 66 | Local Epoch: 8 | Generator Loss: 15.762840 | Discriminator Loss: 0.027092\n","| Global Epoch: 66 | Local Epoch: 9 | Generator Loss: 15.703754 | Discriminator Loss: 0.032295\n","| Global Epoch: 66 | Local Epoch: 0 | Generator Loss: 15.786926 | Discriminator Loss: 0.229920\n","| Global Epoch: 66 | Local Epoch: 1 | Generator Loss: 15.452812 | Discriminator Loss: 0.129651\n","| Global Epoch: 66 | Local Epoch: 2 | Generator Loss: 15.508093 | Discriminator Loss: 0.090054\n","| Global Epoch: 66 | Local Epoch: 3 | Generator Loss: 15.560132 | Discriminator Loss: 0.072274\n","| Global Epoch: 66 | Local Epoch: 4 | Generator Loss: 15.566450 | Discriminator Loss: 0.058710\n","| Global Epoch: 66 | Local Epoch: 5 | Generator Loss: 15.750974 | Discriminator Loss: 0.049817\n","| Global Epoch: 66 | Local Epoch: 6 | Generator Loss: 15.846709 | Discriminator Loss: 0.044366\n","| Global Epoch: 66 | Local Epoch: 7 | Generator Loss: 15.776967 | Discriminator Loss: 0.039134\n","| Global Epoch: 66 | Local Epoch: 8 | Generator Loss: 15.826849 | Discriminator Loss: 0.034996\n","| Global Epoch: 66 | Local Epoch: 9 | Generator Loss: 15.874034 | Discriminator Loss: 0.031636\n","After 67 epoch global training, averged local generator loss is: 15.9516, averged local discrimitor loss is: 0.0704\n","Averged validation loss is: 5.8688\n","| Global Epoch: 67 | Local Epoch: 0 | Generator Loss: 16.403242 | Discriminator Loss: 0.042190\n","| Global Epoch: 67 | Local Epoch: 1 | Generator Loss: 16.014982 | Discriminator Loss: 0.024085\n","| Global Epoch: 67 | Local Epoch: 2 | Generator Loss: 15.946232 | Discriminator Loss: 0.018849\n","| Global Epoch: 67 | Local Epoch: 3 | Generator Loss: 15.859742 | Discriminator Loss: 0.016975\n","| Global Epoch: 67 | Local Epoch: 4 | Generator Loss: 15.795826 | Discriminator Loss: 0.013946\n","| Global Epoch: 67 | Local Epoch: 5 | Generator Loss: 15.883540 | Discriminator Loss: 0.011820\n","| Global Epoch: 67 | Local Epoch: 6 | Generator Loss: 15.941142 | Discriminator Loss: 0.015741\n","| Global Epoch: 67 | Local Epoch: 7 | Generator Loss: 15.666673 | Discriminator Loss: 0.033378\n","| Global Epoch: 67 | Local Epoch: 8 | Generator Loss: 15.534828 | Discriminator Loss: 0.032412\n","| Global Epoch: 67 | Local Epoch: 9 | Generator Loss: 15.535058 | Discriminator Loss: 0.029565\n","| Global Epoch: 67 | Local Epoch: 0 | Generator Loss: 14.809518 | Discriminator Loss: 0.235378\n","| Global Epoch: 67 | Local Epoch: 1 | Generator Loss: 14.799584 | Discriminator Loss: 0.132519\n","| Global Epoch: 67 | Local Epoch: 2 | Generator Loss: 14.682692 | Discriminator Loss: 0.090803\n","| Global Epoch: 67 | Local Epoch: 3 | Generator Loss: 14.606445 | Discriminator Loss: 0.069037\n","| Global Epoch: 67 | Local Epoch: 4 | Generator Loss: 14.546429 | Discriminator Loss: 0.055854\n","| Global Epoch: 67 | Local Epoch: 5 | Generator Loss: 14.554991 | Discriminator Loss: 0.046923\n","| Global Epoch: 67 | Local Epoch: 6 | Generator Loss: 14.711790 | Discriminator Loss: 0.040599\n","| Global Epoch: 67 | Local Epoch: 7 | Generator Loss: 14.862557 | Discriminator Loss: 0.035755\n","| Global Epoch: 67 | Local Epoch: 8 | Generator Loss: 14.933656 | Discriminator Loss: 0.031952\n","| Global Epoch: 67 | Local Epoch: 9 | Generator Loss: 15.044596 | Discriminator Loss: 0.028921\n","| Global Epoch: 67 | Local Epoch: 0 | Generator Loss: 15.414872 | Discriminator Loss: 0.220534\n","| Global Epoch: 67 | Local Epoch: 1 | Generator Loss: 15.059397 | Discriminator Loss: 0.124362\n","| Global Epoch: 67 | Local Epoch: 2 | Generator Loss: 14.890832 | Discriminator Loss: 0.085126\n","| Global Epoch: 67 | Local Epoch: 3 | Generator Loss: 14.841675 | Discriminator Loss: 0.064873\n","| Global Epoch: 67 | Local Epoch: 4 | Generator Loss: 14.969137 | Discriminator Loss: 0.052591\n","| Global Epoch: 67 | Local Epoch: 5 | Generator Loss: 15.058325 | Discriminator Loss: 0.044257\n","| Global Epoch: 67 | Local Epoch: 6 | Generator Loss: 15.194844 | Discriminator Loss: 0.038294\n","| Global Epoch: 67 | Local Epoch: 7 | Generator Loss: 15.247861 | Discriminator Loss: 0.033744\n","| Global Epoch: 67 | Local Epoch: 8 | Generator Loss: 15.278914 | Discriminator Loss: 0.030166\n","| Global Epoch: 67 | Local Epoch: 9 | Generator Loss: 15.295678 | Discriminator Loss: 0.027292\n","| Global Epoch: 67 | Local Epoch: 0 | Generator Loss: 15.216089 | Discriminator Loss: 0.055615\n","| Global Epoch: 67 | Local Epoch: 1 | Generator Loss: 15.260686 | Discriminator Loss: 0.053674\n","| Global Epoch: 67 | Local Epoch: 2 | Generator Loss: 15.202298 | Discriminator Loss: 0.082150\n","| Global Epoch: 67 | Local Epoch: 3 | Generator Loss: 14.811263 | Discriminator Loss: 0.102231\n","| Global Epoch: 67 | Local Epoch: 4 | Generator Loss: 14.630420 | Discriminator Loss: 0.114738\n","| Global Epoch: 67 | Local Epoch: 5 | Generator Loss: 14.553220 | Discriminator Loss: 0.118962\n","| Global Epoch: 67 | Local Epoch: 6 | Generator Loss: 14.243700 | Discriminator Loss: 0.122585\n","| Global Epoch: 67 | Local Epoch: 7 | Generator Loss: 13.948507 | Discriminator Loss: 0.121816\n","| Global Epoch: 67 | Local Epoch: 8 | Generator Loss: 13.807713 | Discriminator Loss: 0.115873\n","| Global Epoch: 67 | Local Epoch: 9 | Generator Loss: 13.684209 | Discriminator Loss: 0.110610\n","| Global Epoch: 67 | Local Epoch: 0 | Generator Loss: 13.048515 | Discriminator Loss: 0.544636\n","| Global Epoch: 67 | Local Epoch: 1 | Generator Loss: 12.980896 | Discriminator Loss: 0.310584\n","| Global Epoch: 67 | Local Epoch: 2 | Generator Loss: 13.005646 | Discriminator Loss: 0.213991\n","| Global Epoch: 67 | Local Epoch: 3 | Generator Loss: 13.053186 | Discriminator Loss: 0.163429\n","| Global Epoch: 67 | Local Epoch: 4 | Generator Loss: 13.165307 | Discriminator Loss: 0.132235\n","| Global Epoch: 67 | Local Epoch: 5 | Generator Loss: 13.224798 | Discriminator Loss: 0.111097\n","| Global Epoch: 67 | Local Epoch: 6 | Generator Loss: 13.237304 | Discriminator Loss: 0.095869\n","| Global Epoch: 67 | Local Epoch: 7 | Generator Loss: 13.270398 | Discriminator Loss: 0.084364\n","| Global Epoch: 67 | Local Epoch: 8 | Generator Loss: 13.322724 | Discriminator Loss: 0.075363\n","| Global Epoch: 67 | Local Epoch: 9 | Generator Loss: 13.372769 | Discriminator Loss: 0.068110\n","After 68 epoch global training, averged local generator loss is: 15.9137, averged local discrimitor loss is: 0.0703\n","Averged validation loss is: 4.4786\n","| Global Epoch: 68 | Local Epoch: 0 | Generator Loss: 14.090632 | Discriminator Loss: 0.044602\n","| Global Epoch: 68 | Local Epoch: 1 | Generator Loss: 14.249117 | Discriminator Loss: 0.033134\n","| Global Epoch: 68 | Local Epoch: 2 | Generator Loss: 14.593513 | Discriminator Loss: 0.026841\n","| Global Epoch: 68 | Local Epoch: 3 | Generator Loss: 14.866979 | Discriminator Loss: 0.020707\n","| Global Epoch: 68 | Local Epoch: 4 | Generator Loss: 15.121202 | Discriminator Loss: 0.016828\n","| Global Epoch: 68 | Local Epoch: 5 | Generator Loss: 15.369219 | Discriminator Loss: 0.014586\n","| Global Epoch: 68 | Local Epoch: 6 | Generator Loss: 15.418711 | Discriminator Loss: 0.016880\n","| Global Epoch: 68 | Local Epoch: 7 | Generator Loss: 15.426238 | Discriminator Loss: 0.016297\n","| Global Epoch: 68 | Local Epoch: 8 | Generator Loss: 15.309089 | Discriminator Loss: 0.016906\n","| Global Epoch: 68 | Local Epoch: 9 | Generator Loss: 15.264577 | Discriminator Loss: 0.015566\n","| Global Epoch: 68 | Local Epoch: 0 | Generator Loss: 15.720419 | Discriminator Loss: 0.164888\n","| Global Epoch: 68 | Local Epoch: 1 | Generator Loss: 15.367131 | Discriminator Loss: 0.092321\n","| Global Epoch: 68 | Local Epoch: 2 | Generator Loss: 15.218680 | Discriminator Loss: 0.063541\n","| Global Epoch: 68 | Local Epoch: 3 | Generator Loss: 15.429665 | Discriminator Loss: 0.048856\n","| Global Epoch: 68 | Local Epoch: 4 | Generator Loss: 15.468557 | Discriminator Loss: 0.039686\n","| Global Epoch: 68 | Local Epoch: 5 | Generator Loss: 15.488436 | Discriminator Loss: 0.033449\n","| Global Epoch: 68 | Local Epoch: 6 | Generator Loss: 15.514744 | Discriminator Loss: 0.028946\n","| Global Epoch: 68 | Local Epoch: 7 | Generator Loss: 15.564417 | Discriminator Loss: 0.025536\n","| Global Epoch: 68 | Local Epoch: 8 | Generator Loss: 15.566663 | Discriminator Loss: 0.022848\n","| Global Epoch: 68 | Local Epoch: 9 | Generator Loss: 15.568323 | Discriminator Loss: 0.020682\n","| Global Epoch: 68 | Local Epoch: 0 | Generator Loss: 15.591930 | Discriminator Loss: 0.036097\n","| Global Epoch: 68 | Local Epoch: 1 | Generator Loss: 15.837391 | Discriminator Loss: 0.022761\n","| Global Epoch: 68 | Local Epoch: 2 | Generator Loss: 15.951677 | Discriminator Loss: 0.021162\n","| Global Epoch: 68 | Local Epoch: 3 | Generator Loss: 16.084205 | Discriminator Loss: 0.035888\n","| Global Epoch: 68 | Local Epoch: 4 | Generator Loss: 15.788565 | Discriminator Loss: 0.046672\n","| Global Epoch: 68 | Local Epoch: 5 | Generator Loss: 15.490086 | Discriminator Loss: 0.048422\n","| Global Epoch: 68 | Local Epoch: 6 | Generator Loss: 15.500685 | Discriminator Loss: 0.044019\n","| Global Epoch: 68 | Local Epoch: 7 | Generator Loss: 15.496936 | Discriminator Loss: 0.039124\n","| Global Epoch: 68 | Local Epoch: 8 | Generator Loss: 15.473878 | Discriminator Loss: 0.035028\n","| Global Epoch: 68 | Local Epoch: 9 | Generator Loss: 15.467621 | Discriminator Loss: 0.031709\n","| Global Epoch: 68 | Local Epoch: 0 | Generator Loss: 15.086228 | Discriminator Loss: 0.201965\n","| Global Epoch: 68 | Local Epoch: 1 | Generator Loss: 14.755018 | Discriminator Loss: 0.115485\n","| Global Epoch: 68 | Local Epoch: 2 | Generator Loss: 14.924935 | Discriminator Loss: 0.079727\n","| Global Epoch: 68 | Local Epoch: 3 | Generator Loss: 15.157716 | Discriminator Loss: 0.061495\n","| Global Epoch: 68 | Local Epoch: 4 | Generator Loss: 15.225480 | Discriminator Loss: 0.049894\n","| Global Epoch: 68 | Local Epoch: 5 | Generator Loss: 15.304013 | Discriminator Loss: 0.041920\n","| Global Epoch: 68 | Local Epoch: 6 | Generator Loss: 15.328539 | Discriminator Loss: 0.036208\n","| Global Epoch: 68 | Local Epoch: 7 | Generator Loss: 15.471318 | Discriminator Loss: 0.032179\n","| Global Epoch: 68 | Local Epoch: 8 | Generator Loss: 15.574429 | Discriminator Loss: 0.028815\n","| Global Epoch: 68 | Local Epoch: 9 | Generator Loss: 15.592801 | Discriminator Loss: 0.031984\n","| Global Epoch: 68 | Local Epoch: 0 | Generator Loss: 15.824876 | Discriminator Loss: 0.456062\n","| Global Epoch: 68 | Local Epoch: 1 | Generator Loss: 15.663356 | Discriminator Loss: 0.238542\n","| Global Epoch: 68 | Local Epoch: 2 | Generator Loss: 15.515907 | Discriminator Loss: 0.162290\n","| Global Epoch: 68 | Local Epoch: 3 | Generator Loss: 15.540408 | Discriminator Loss: 0.123034\n","| Global Epoch: 68 | Local Epoch: 4 | Generator Loss: 15.502884 | Discriminator Loss: 0.099257\n","| Global Epoch: 68 | Local Epoch: 5 | Generator Loss: 15.477580 | Discriminator Loss: 0.083177\n","| Global Epoch: 68 | Local Epoch: 6 | Generator Loss: 15.511443 | Discriminator Loss: 0.071695\n","| Global Epoch: 68 | Local Epoch: 7 | Generator Loss: 15.453789 | Discriminator Loss: 0.062895\n","| Global Epoch: 68 | Local Epoch: 8 | Generator Loss: 15.521265 | Discriminator Loss: 0.056057\n","| Global Epoch: 68 | Local Epoch: 9 | Generator Loss: 15.527613 | Discriminator Loss: 0.050546\n","After 69 epoch global training, averged local generator loss is: 15.9081, averged local discrimitor loss is: 0.0700\n","Averged validation loss is: 6.4958\n","| Global Epoch: 69 | Local Epoch: 0 | Generator Loss: 15.165336 | Discriminator Loss: 0.000763\n","| Global Epoch: 69 | Local Epoch: 1 | Generator Loss: 14.321557 | Discriminator Loss: 0.000524\n","| Global Epoch: 69 | Local Epoch: 2 | Generator Loss: 14.861943 | Discriminator Loss: 0.000451\n","| Global Epoch: 69 | Local Epoch: 3 | Generator Loss: 15.279872 | Discriminator Loss: 0.000366\n","| Global Epoch: 69 | Local Epoch: 4 | Generator Loss: 15.670415 | Discriminator Loss: 0.000326\n","| Global Epoch: 69 | Local Epoch: 5 | Generator Loss: 15.928200 | Discriminator Loss: 0.000285\n","| Global Epoch: 69 | Local Epoch: 6 | Generator Loss: 16.085622 | Discriminator Loss: 0.000253\n","| Global Epoch: 69 | Local Epoch: 7 | Generator Loss: 16.207971 | Discriminator Loss: 0.000225\n","| Global Epoch: 69 | Local Epoch: 8 | Generator Loss: 16.270108 | Discriminator Loss: 0.000207\n","| Global Epoch: 69 | Local Epoch: 9 | Generator Loss: 16.285621 | Discriminator Loss: 0.000189\n","| Global Epoch: 69 | Local Epoch: 0 | Generator Loss: 16.106827 | Discriminator Loss: 0.314077\n","| Global Epoch: 69 | Local Epoch: 1 | Generator Loss: 15.665345 | Discriminator Loss: 0.172022\n","| Global Epoch: 69 | Local Epoch: 2 | Generator Loss: 15.423747 | Discriminator Loss: 0.117713\n","| Global Epoch: 69 | Local Epoch: 3 | Generator Loss: 15.341030 | Discriminator Loss: 0.089705\n","| Global Epoch: 69 | Local Epoch: 4 | Generator Loss: 15.354382 | Discriminator Loss: 0.072556\n","| Global Epoch: 69 | Local Epoch: 5 | Generator Loss: 15.380468 | Discriminator Loss: 0.060960\n","| Global Epoch: 69 | Local Epoch: 6 | Generator Loss: 15.381693 | Discriminator Loss: 0.052596\n","| Global Epoch: 69 | Local Epoch: 7 | Generator Loss: 15.412940 | Discriminator Loss: 0.046284\n","| Global Epoch: 69 | Local Epoch: 8 | Generator Loss: 15.446641 | Discriminator Loss: 0.041340\n","| Global Epoch: 69 | Local Epoch: 9 | Generator Loss: 15.556410 | Discriminator Loss: 0.037414\n","| Global Epoch: 69 | Local Epoch: 0 | Generator Loss: 16.299264 | Discriminator Loss: 0.109299\n","| Global Epoch: 69 | Local Epoch: 1 | Generator Loss: 15.985955 | Discriminator Loss: 0.062653\n","| Global Epoch: 69 | Local Epoch: 2 | Generator Loss: 16.185385 | Discriminator Loss: 0.043879\n","| Global Epoch: 69 | Local Epoch: 3 | Generator Loss: 16.128304 | Discriminator Loss: 0.033723\n","| Global Epoch: 69 | Local Epoch: 4 | Generator Loss: 16.020705 | Discriminator Loss: 0.027373\n","| Global Epoch: 69 | Local Epoch: 5 | Generator Loss: 16.029011 | Discriminator Loss: 0.023066\n","| Global Epoch: 69 | Local Epoch: 6 | Generator Loss: 16.035163 | Discriminator Loss: 0.019961\n","| Global Epoch: 69 | Local Epoch: 7 | Generator Loss: 16.043114 | Discriminator Loss: 0.017619\n","| Global Epoch: 69 | Local Epoch: 8 | Generator Loss: 16.062638 | Discriminator Loss: 0.015772\n","| Global Epoch: 69 | Local Epoch: 9 | Generator Loss: 16.070969 | Discriminator Loss: 0.014783\n","| Global Epoch: 69 | Local Epoch: 0 | Generator Loss: 16.274953 | Discriminator Loss: 0.168489\n","| Global Epoch: 69 | Local Epoch: 1 | Generator Loss: 16.054982 | Discriminator Loss: 0.094182\n","| Global Epoch: 69 | Local Epoch: 2 | Generator Loss: 15.849126 | Discriminator Loss: 0.064568\n","| Global Epoch: 69 | Local Epoch: 3 | Generator Loss: 15.715328 | Discriminator Loss: 0.049219\n","| Global Epoch: 69 | Local Epoch: 4 | Generator Loss: 15.651041 | Discriminator Loss: 0.039807\n","| Global Epoch: 69 | Local Epoch: 5 | Generator Loss: 15.754683 | Discriminator Loss: 0.033512\n","| Global Epoch: 69 | Local Epoch: 6 | Generator Loss: 15.860783 | Discriminator Loss: 0.028941\n","| Global Epoch: 69 | Local Epoch: 7 | Generator Loss: 15.878138 | Discriminator Loss: 0.025495\n","| Global Epoch: 69 | Local Epoch: 8 | Generator Loss: 15.878032 | Discriminator Loss: 0.022799\n","| Global Epoch: 69 | Local Epoch: 9 | Generator Loss: 15.919713 | Discriminator Loss: 0.020629\n","| Global Epoch: 69 | Local Epoch: 0 | Generator Loss: 16.222977 | Discriminator Loss: 0.285828\n","| Global Epoch: 69 | Local Epoch: 1 | Generator Loss: 15.676332 | Discriminator Loss: 0.162946\n","| Global Epoch: 69 | Local Epoch: 2 | Generator Loss: 15.382741 | Discriminator Loss: 0.111924\n","| Global Epoch: 69 | Local Epoch: 3 | Generator Loss: 15.343028 | Discriminator Loss: 0.085377\n","| Global Epoch: 69 | Local Epoch: 4 | Generator Loss: 15.275172 | Discriminator Loss: 0.069055\n","| Global Epoch: 69 | Local Epoch: 5 | Generator Loss: 15.228541 | Discriminator Loss: 0.058041\n","| Global Epoch: 69 | Local Epoch: 6 | Generator Loss: 15.349569 | Discriminator Loss: 0.050148\n","| Global Epoch: 69 | Local Epoch: 7 | Generator Loss: 15.467321 | Discriminator Loss: 0.044149\n","| Global Epoch: 69 | Local Epoch: 8 | Generator Loss: 15.509989 | Discriminator Loss: 0.039446\n","| Global Epoch: 69 | Local Epoch: 9 | Generator Loss: 15.548698 | Discriminator Loss: 0.035665\n","After 70 epoch global training, averged local generator loss is: 15.9029, averged local discrimitor loss is: 0.0696\n","Averged validation loss is: 4.6221\n","| Global Epoch: 70 | Local Epoch: 0 | Generator Loss: 15.943928 | Discriminator Loss: 0.045348\n","| Global Epoch: 70 | Local Epoch: 1 | Generator Loss: 15.685629 | Discriminator Loss: 0.028516\n","| Global Epoch: 70 | Local Epoch: 2 | Generator Loss: 15.521461 | Discriminator Loss: 0.024572\n","| Global Epoch: 70 | Local Epoch: 3 | Generator Loss: 15.648831 | Discriminator Loss: 0.019175\n","| Global Epoch: 70 | Local Epoch: 4 | Generator Loss: 15.802021 | Discriminator Loss: 0.015736\n","| Global Epoch: 70 | Local Epoch: 5 | Generator Loss: 15.944688 | Discriminator Loss: 0.013385\n","| Global Epoch: 70 | Local Epoch: 6 | Generator Loss: 16.004390 | Discriminator Loss: 0.011605\n","| Global Epoch: 70 | Local Epoch: 7 | Generator Loss: 16.057351 | Discriminator Loss: 0.014562\n","| Global Epoch: 70 | Local Epoch: 8 | Generator Loss: 15.685036 | Discriminator Loss: 0.045797\n","| Global Epoch: 70 | Local Epoch: 9 | Generator Loss: 15.546517 | Discriminator Loss: 0.045442\n","| Global Epoch: 70 | Local Epoch: 0 | Generator Loss: 14.099795 | Discriminator Loss: 0.108332\n","| Global Epoch: 70 | Local Epoch: 1 | Generator Loss: 13.704240 | Discriminator Loss: 0.057316\n","| Global Epoch: 70 | Local Epoch: 2 | Generator Loss: 13.316494 | Discriminator Loss: 0.039709\n","| Global Epoch: 70 | Local Epoch: 3 | Generator Loss: 13.263712 | Discriminator Loss: 0.030633\n","| Global Epoch: 70 | Local Epoch: 4 | Generator Loss: 13.249520 | Discriminator Loss: 0.024734\n","| Global Epoch: 70 | Local Epoch: 5 | Generator Loss: 13.248347 | Discriminator Loss: 0.020817\n","| Global Epoch: 70 | Local Epoch: 6 | Generator Loss: 13.238602 | Discriminator Loss: 0.017959\n","| Global Epoch: 70 | Local Epoch: 7 | Generator Loss: 13.392636 | Discriminator Loss: 0.023657\n","| Global Epoch: 70 | Local Epoch: 8 | Generator Loss: 13.769348 | Discriminator Loss: 0.022865\n","| Global Epoch: 70 | Local Epoch: 9 | Generator Loss: 13.958506 | Discriminator Loss: 0.021492\n","| Global Epoch: 70 | Local Epoch: 0 | Generator Loss: 14.105040 | Discriminator Loss: 0.217013\n","| Global Epoch: 70 | Local Epoch: 1 | Generator Loss: 14.060271 | Discriminator Loss: 0.119827\n","| Global Epoch: 70 | Local Epoch: 2 | Generator Loss: 14.328106 | Discriminator Loss: 0.085440\n","| Global Epoch: 70 | Local Epoch: 3 | Generator Loss: 14.580687 | Discriminator Loss: 0.065771\n","| Global Epoch: 70 | Local Epoch: 4 | Generator Loss: 14.723451 | Discriminator Loss: 0.053269\n","| Global Epoch: 70 | Local Epoch: 5 | Generator Loss: 14.785802 | Discriminator Loss: 0.044780\n","| Global Epoch: 70 | Local Epoch: 6 | Generator Loss: 14.853386 | Discriminator Loss: 0.038718\n","| Global Epoch: 70 | Local Epoch: 7 | Generator Loss: 14.991319 | Discriminator Loss: 0.034156\n","| Global Epoch: 70 | Local Epoch: 8 | Generator Loss: 15.028882 | Discriminator Loss: 0.030545\n","| Global Epoch: 70 | Local Epoch: 9 | Generator Loss: 15.072777 | Discriminator Loss: 0.027631\n","| Global Epoch: 70 | Local Epoch: 0 | Generator Loss: 14.988365 | Discriminator Loss: 0.058718\n","| Global Epoch: 70 | Local Epoch: 1 | Generator Loss: 14.731246 | Discriminator Loss: 0.043623\n","| Global Epoch: 70 | Local Epoch: 2 | Generator Loss: 14.377195 | Discriminator Loss: 0.053894\n","| Global Epoch: 70 | Local Epoch: 3 | Generator Loss: 14.345557 | Discriminator Loss: 0.048335\n","| Global Epoch: 70 | Local Epoch: 4 | Generator Loss: 14.417758 | Discriminator Loss: 0.039388\n","| Global Epoch: 70 | Local Epoch: 5 | Generator Loss: 14.570046 | Discriminator Loss: 0.056367\n","| Global Epoch: 70 | Local Epoch: 6 | Generator Loss: 14.266622 | Discriminator Loss: 0.068270\n","| Global Epoch: 70 | Local Epoch: 7 | Generator Loss: 14.214567 | Discriminator Loss: 0.060806\n","| Global Epoch: 70 | Local Epoch: 8 | Generator Loss: 14.159742 | Discriminator Loss: 0.054547\n","| Global Epoch: 70 | Local Epoch: 9 | Generator Loss: 14.194444 | Discriminator Loss: 0.050522\n","| Global Epoch: 70 | Local Epoch: 0 | Generator Loss: 13.864123 | Discriminator Loss: 0.265772\n","| Global Epoch: 70 | Local Epoch: 1 | Generator Loss: 13.875423 | Discriminator Loss: 0.152718\n","| Global Epoch: 70 | Local Epoch: 2 | Generator Loss: 13.914690 | Discriminator Loss: 0.105388\n","| Global Epoch: 70 | Local Epoch: 3 | Generator Loss: 13.938908 | Discriminator Loss: 0.080477\n","| Global Epoch: 70 | Local Epoch: 4 | Generator Loss: 13.988210 | Discriminator Loss: 0.065191\n","| Global Epoch: 70 | Local Epoch: 5 | Generator Loss: 14.094088 | Discriminator Loss: 0.055204\n","| Global Epoch: 70 | Local Epoch: 6 | Generator Loss: 14.146068 | Discriminator Loss: 0.047717\n","| Global Epoch: 70 | Local Epoch: 7 | Generator Loss: 14.202345 | Discriminator Loss: 0.042040\n","| Global Epoch: 70 | Local Epoch: 8 | Generator Loss: 14.249728 | Discriminator Loss: 0.037596\n","| Global Epoch: 70 | Local Epoch: 9 | Generator Loss: 14.312420 | Discriminator Loss: 0.034004\n","After 71 epoch global training, averged local generator loss is: 15.8805, averged local discrimitor loss is: 0.0691\n","Averged validation loss is: 5.8257\n","| Global Epoch: 71 | Local Epoch: 0 | Generator Loss: 14.537289 | Discriminator Loss: 0.024346\n","| Global Epoch: 71 | Local Epoch: 1 | Generator Loss: 14.458446 | Discriminator Loss: 0.012870\n","| Global Epoch: 71 | Local Epoch: 2 | Generator Loss: 14.302971 | Discriminator Loss: 0.009056\n","| Global Epoch: 71 | Local Epoch: 3 | Generator Loss: 14.348050 | Discriminator Loss: 0.007320\n","| Global Epoch: 71 | Local Epoch: 4 | Generator Loss: 14.478353 | Discriminator Loss: 0.005956\n","| Global Epoch: 71 | Local Epoch: 5 | Generator Loss: 14.674971 | Discriminator Loss: 0.005021\n","| Global Epoch: 71 | Local Epoch: 6 | Generator Loss: 14.792000 | Discriminator Loss: 0.004367\n","| Global Epoch: 71 | Local Epoch: 7 | Generator Loss: 14.902694 | Discriminator Loss: 0.003856\n","| Global Epoch: 71 | Local Epoch: 8 | Generator Loss: 15.042276 | Discriminator Loss: 0.003458\n","| Global Epoch: 71 | Local Epoch: 9 | Generator Loss: 15.168981 | Discriminator Loss: 0.003131\n","| Global Epoch: 71 | Local Epoch: 0 | Generator Loss: 15.335285 | Discriminator Loss: 0.299487\n","| Global Epoch: 71 | Local Epoch: 1 | Generator Loss: 14.919537 | Discriminator Loss: 0.165626\n","| Global Epoch: 71 | Local Epoch: 2 | Generator Loss: 14.892643 | Discriminator Loss: 0.113735\n","| Global Epoch: 71 | Local Epoch: 3 | Generator Loss: 14.958504 | Discriminator Loss: 0.086707\n","| Global Epoch: 71 | Local Epoch: 4 | Generator Loss: 14.918432 | Discriminator Loss: 0.070014\n","| Global Epoch: 71 | Local Epoch: 5 | Generator Loss: 14.837036 | Discriminator Loss: 0.058783\n","| Global Epoch: 71 | Local Epoch: 6 | Generator Loss: 14.875648 | Discriminator Loss: 0.050773\n","| Global Epoch: 71 | Local Epoch: 7 | Generator Loss: 14.970316 | Discriminator Loss: 0.044691\n","| Global Epoch: 71 | Local Epoch: 8 | Generator Loss: 15.089679 | Discriminator Loss: 0.039986\n","| Global Epoch: 71 | Local Epoch: 9 | Generator Loss: 15.137980 | Discriminator Loss: 0.036160\n","| Global Epoch: 71 | Local Epoch: 0 | Generator Loss: 14.702426 | Discriminator Loss: 0.137126\n","| Global Epoch: 71 | Local Epoch: 1 | Generator Loss: 14.167362 | Discriminator Loss: 0.079660\n","| Global Epoch: 71 | Local Epoch: 2 | Generator Loss: 14.189482 | Discriminator Loss: 0.055007\n","| Global Epoch: 71 | Local Epoch: 3 | Generator Loss: 14.267989 | Discriminator Loss: 0.042009\n","| Global Epoch: 71 | Local Epoch: 4 | Generator Loss: 14.424417 | Discriminator Loss: 0.034072\n","| Global Epoch: 71 | Local Epoch: 5 | Generator Loss: 14.514292 | Discriminator Loss: 0.028674\n","| Global Epoch: 71 | Local Epoch: 6 | Generator Loss: 14.674268 | Discriminator Loss: 0.024790\n","| Global Epoch: 71 | Local Epoch: 7 | Generator Loss: 14.783495 | Discriminator Loss: 0.021848\n","| Global Epoch: 71 | Local Epoch: 8 | Generator Loss: 14.875797 | Discriminator Loss: 0.019537\n","| Global Epoch: 71 | Local Epoch: 9 | Generator Loss: 15.010543 | Discriminator Loss: 0.017695\n","| Global Epoch: 71 | Local Epoch: 0 | Generator Loss: 15.577000 | Discriminator Loss: 0.032371\n","| Global Epoch: 71 | Local Epoch: 1 | Generator Loss: 15.515274 | Discriminator Loss: 0.019724\n","| Global Epoch: 71 | Local Epoch: 2 | Generator Loss: 15.572720 | Discriminator Loss: 0.013859\n","| Global Epoch: 71 | Local Epoch: 3 | Generator Loss: 15.467805 | Discriminator Loss: 0.010643\n","| Global Epoch: 71 | Local Epoch: 4 | Generator Loss: 15.623507 | Discriminator Loss: 0.008814\n","| Global Epoch: 71 | Local Epoch: 5 | Generator Loss: 15.472279 | Discriminator Loss: 0.026984\n","| Global Epoch: 71 | Local Epoch: 6 | Generator Loss: 15.405261 | Discriminator Loss: 0.027125\n","| Global Epoch: 71 | Local Epoch: 7 | Generator Loss: 15.321059 | Discriminator Loss: 0.024299\n","| Global Epoch: 71 | Local Epoch: 8 | Generator Loss: 15.304617 | Discriminator Loss: 0.021764\n","| Global Epoch: 71 | Local Epoch: 9 | Generator Loss: 15.328550 | Discriminator Loss: 0.019725\n","| Global Epoch: 71 | Local Epoch: 0 | Generator Loss: 15.592071 | Discriminator Loss: 0.129603\n","| Global Epoch: 71 | Local Epoch: 1 | Generator Loss: 15.419118 | Discriminator Loss: 0.073675\n","| Global Epoch: 71 | Local Epoch: 2 | Generator Loss: 15.236312 | Discriminator Loss: 0.050660\n","| Global Epoch: 71 | Local Epoch: 3 | Generator Loss: 15.173004 | Discriminator Loss: 0.038671\n","| Global Epoch: 71 | Local Epoch: 4 | Generator Loss: 15.168316 | Discriminator Loss: 0.031331\n","| Global Epoch: 71 | Local Epoch: 5 | Generator Loss: 15.178285 | Discriminator Loss: 0.026392\n","| Global Epoch: 71 | Local Epoch: 6 | Generator Loss: 15.191180 | Discriminator Loss: 0.027565\n","| Global Epoch: 71 | Local Epoch: 7 | Generator Loss: 15.081089 | Discriminator Loss: 0.026313\n","| Global Epoch: 71 | Local Epoch: 8 | Generator Loss: 15.049673 | Discriminator Loss: 0.023936\n","| Global Epoch: 71 | Local Epoch: 9 | Generator Loss: 15.103104 | Discriminator Loss: 0.021739\n","After 72 epoch global training, averged local generator loss is: 15.8697, averged local discrimitor loss is: 0.0684\n","Averged validation loss is: 5.1470\n","| Global Epoch: 72 | Local Epoch: 0 | Generator Loss: 15.081731 | Discriminator Loss: 0.099464\n","| Global Epoch: 72 | Local Epoch: 1 | Generator Loss: 14.899454 | Discriminator Loss: 0.057162\n","| Global Epoch: 72 | Local Epoch: 2 | Generator Loss: 15.233679 | Discriminator Loss: 0.039705\n","| Global Epoch: 72 | Local Epoch: 3 | Generator Loss: 15.347421 | Discriminator Loss: 0.030336\n","| Global Epoch: 72 | Local Epoch: 4 | Generator Loss: 15.502759 | Discriminator Loss: 0.024588\n","| Global Epoch: 72 | Local Epoch: 5 | Generator Loss: 15.568305 | Discriminator Loss: 0.020708\n","| Global Epoch: 72 | Local Epoch: 6 | Generator Loss: 15.727653 | Discriminator Loss: 0.018041\n","| Global Epoch: 72 | Local Epoch: 7 | Generator Loss: 15.821228 | Discriminator Loss: 0.015922\n","| Global Epoch: 72 | Local Epoch: 8 | Generator Loss: 15.893790 | Discriminator Loss: 0.014248\n","| Global Epoch: 72 | Local Epoch: 9 | Generator Loss: 15.950790 | Discriminator Loss: 0.012898\n","| Global Epoch: 72 | Local Epoch: 0 | Generator Loss: 16.892339 | Discriminator Loss: 0.279516\n","| Global Epoch: 72 | Local Epoch: 1 | Generator Loss: 16.023920 | Discriminator Loss: 0.154775\n","| Global Epoch: 72 | Local Epoch: 2 | Generator Loss: 15.906231 | Discriminator Loss: 0.105855\n","| Global Epoch: 72 | Local Epoch: 3 | Generator Loss: 15.932675 | Discriminator Loss: 0.080536\n","| Global Epoch: 72 | Local Epoch: 4 | Generator Loss: 15.932019 | Discriminator Loss: 0.065323\n","| Global Epoch: 72 | Local Epoch: 5 | Generator Loss: 15.964146 | Discriminator Loss: 0.054874\n","| Global Epoch: 72 | Local Epoch: 6 | Generator Loss: 15.978090 | Discriminator Loss: 0.047394\n","| Global Epoch: 72 | Local Epoch: 7 | Generator Loss: 15.976702 | Discriminator Loss: 0.041706\n","| Global Epoch: 72 | Local Epoch: 8 | Generator Loss: 15.992149 | Discriminator Loss: 0.037291\n","| Global Epoch: 72 | Local Epoch: 9 | Generator Loss: 16.013455 | Discriminator Loss: 0.033717\n","| Global Epoch: 72 | Local Epoch: 0 | Generator Loss: 15.732282 | Discriminator Loss: 0.038696\n","| Global Epoch: 72 | Local Epoch: 1 | Generator Loss: 15.601646 | Discriminator Loss: 0.022452\n","| Global Epoch: 72 | Local Epoch: 2 | Generator Loss: 15.870793 | Discriminator Loss: 0.017669\n","| Global Epoch: 72 | Local Epoch: 3 | Generator Loss: 15.843602 | Discriminator Loss: 0.023435\n","| Global Epoch: 72 | Local Epoch: 4 | Generator Loss: 15.949726 | Discriminator Loss: 0.027380\n","| Global Epoch: 72 | Local Epoch: 5 | Generator Loss: 15.735799 | Discriminator Loss: 0.033474\n","| Global Epoch: 72 | Local Epoch: 6 | Generator Loss: 15.603972 | Discriminator Loss: 0.029469\n","| Global Epoch: 72 | Local Epoch: 7 | Generator Loss: 15.630634 | Discriminator Loss: 0.037848\n","| Global Epoch: 72 | Local Epoch: 8 | Generator Loss: 15.485491 | Discriminator Loss: 0.046511\n","| Global Epoch: 72 | Local Epoch: 9 | Generator Loss: 15.381426 | Discriminator Loss: 0.047365\n","| Global Epoch: 72 | Local Epoch: 0 | Generator Loss: 14.964218 | Discriminator Loss: 0.362402\n","| Global Epoch: 72 | Local Epoch: 1 | Generator Loss: 15.189449 | Discriminator Loss: 0.203619\n","| Global Epoch: 72 | Local Epoch: 2 | Generator Loss: 15.039271 | Discriminator Loss: 0.139223\n","| Global Epoch: 72 | Local Epoch: 3 | Generator Loss: 15.062771 | Discriminator Loss: 0.105933\n","| Global Epoch: 72 | Local Epoch: 4 | Generator Loss: 15.236238 | Discriminator Loss: 0.085903\n","| Global Epoch: 72 | Local Epoch: 5 | Generator Loss: 15.266935 | Discriminator Loss: 0.072213\n","| Global Epoch: 72 | Local Epoch: 6 | Generator Loss: 15.338698 | Discriminator Loss: 0.062828\n","| Global Epoch: 72 | Local Epoch: 7 | Generator Loss: 15.315550 | Discriminator Loss: 0.057636\n","| Global Epoch: 72 | Local Epoch: 8 | Generator Loss: 15.455296 | Discriminator Loss: 0.052141\n","| Global Epoch: 72 | Local Epoch: 9 | Generator Loss: 15.544776 | Discriminator Loss: 0.047215\n","| Global Epoch: 72 | Local Epoch: 0 | Generator Loss: 15.457051 | Discriminator Loss: 0.098189\n","| Global Epoch: 72 | Local Epoch: 1 | Generator Loss: 15.370042 | Discriminator Loss: 0.055044\n","| Global Epoch: 72 | Local Epoch: 2 | Generator Loss: 15.411540 | Discriminator Loss: 0.038110\n","| Global Epoch: 72 | Local Epoch: 3 | Generator Loss: 15.315020 | Discriminator Loss: 0.029113\n","| Global Epoch: 72 | Local Epoch: 4 | Generator Loss: 15.302505 | Discriminator Loss: 0.023736\n","| Global Epoch: 72 | Local Epoch: 5 | Generator Loss: 15.352486 | Discriminator Loss: 0.020175\n","| Global Epoch: 72 | Local Epoch: 6 | Generator Loss: 15.332038 | Discriminator Loss: 0.017467\n","| Global Epoch: 72 | Local Epoch: 7 | Generator Loss: 15.419361 | Discriminator Loss: 0.015422\n","| Global Epoch: 72 | Local Epoch: 8 | Generator Loss: 15.460323 | Discriminator Loss: 0.013805\n","| Global Epoch: 72 | Local Epoch: 9 | Generator Loss: 15.486053 | Discriminator Loss: 0.012499\n","After 73 epoch global training, averged local generator loss is: 15.8645, averged local discrimitor loss is: 0.0676\n","Averged validation loss is: 5.3712\n","| Global Epoch: 73 | Local Epoch: 0 | Generator Loss: 14.993518 | Discriminator Loss: 0.009058\n","| Global Epoch: 73 | Local Epoch: 1 | Generator Loss: 14.994555 | Discriminator Loss: 0.013954\n","| Global Epoch: 73 | Local Epoch: 2 | Generator Loss: 15.137472 | Discriminator Loss: 0.015864\n","| Global Epoch: 73 | Local Epoch: 3 | Generator Loss: 15.295570 | Discriminator Loss: 0.016351\n","| Global Epoch: 73 | Local Epoch: 4 | Generator Loss: 15.460221 | Discriminator Loss: 0.014862\n","| Global Epoch: 73 | Local Epoch: 5 | Generator Loss: 15.646788 | Discriminator Loss: 0.013117\n","| Global Epoch: 73 | Local Epoch: 6 | Generator Loss: 15.700809 | Discriminator Loss: 0.011499\n","| Global Epoch: 73 | Local Epoch: 7 | Generator Loss: 15.815699 | Discriminator Loss: 0.010145\n","| Global Epoch: 73 | Local Epoch: 8 | Generator Loss: 15.867430 | Discriminator Loss: 0.009076\n","| Global Epoch: 73 | Local Epoch: 9 | Generator Loss: 15.903958 | Discriminator Loss: 0.008218\n","| Global Epoch: 73 | Local Epoch: 0 | Generator Loss: 16.923693 | Discriminator Loss: 0.324832\n","| Global Epoch: 73 | Local Epoch: 1 | Generator Loss: 16.380937 | Discriminator Loss: 0.177045\n","| Global Epoch: 73 | Local Epoch: 2 | Generator Loss: 16.157872 | Discriminator Loss: 0.120400\n","| Global Epoch: 73 | Local Epoch: 3 | Generator Loss: 16.023843 | Discriminator Loss: 0.091189\n","| Global Epoch: 73 | Local Epoch: 4 | Generator Loss: 15.904718 | Discriminator Loss: 0.073531\n","| Global Epoch: 73 | Local Epoch: 5 | Generator Loss: 16.018669 | Discriminator Loss: 0.062066\n","| Global Epoch: 73 | Local Epoch: 6 | Generator Loss: 15.897974 | Discriminator Loss: 0.053552\n","| Global Epoch: 73 | Local Epoch: 7 | Generator Loss: 15.849533 | Discriminator Loss: 0.047081\n","| Global Epoch: 73 | Local Epoch: 8 | Generator Loss: 15.924622 | Discriminator Loss: 0.042193\n","| Global Epoch: 73 | Local Epoch: 9 | Generator Loss: 15.945511 | Discriminator Loss: 0.038135\n","| Global Epoch: 73 | Local Epoch: 0 | Generator Loss: 16.370975 | Discriminator Loss: 0.185793\n","| Global Epoch: 73 | Local Epoch: 1 | Generator Loss: 15.944839 | Discriminator Loss: 0.105152\n","| Global Epoch: 73 | Local Epoch: 2 | Generator Loss: 15.863863 | Discriminator Loss: 0.072789\n","| Global Epoch: 73 | Local Epoch: 3 | Generator Loss: 15.877731 | Discriminator Loss: 0.055507\n","| Global Epoch: 73 | Local Epoch: 4 | Generator Loss: 15.834122 | Discriminator Loss: 0.044895\n","| Global Epoch: 73 | Local Epoch: 5 | Generator Loss: 15.781387 | Discriminator Loss: 0.037702\n","| Global Epoch: 73 | Local Epoch: 6 | Generator Loss: 15.735241 | Discriminator Loss: 0.032532\n","| Global Epoch: 73 | Local Epoch: 7 | Generator Loss: 15.690359 | Discriminator Loss: 0.028637\n","| Global Epoch: 73 | Local Epoch: 8 | Generator Loss: 15.695830 | Discriminator Loss: 0.025585\n","| Global Epoch: 73 | Local Epoch: 9 | Generator Loss: 15.806174 | Discriminator Loss: 0.023182\n","| Global Epoch: 73 | Local Epoch: 0 | Generator Loss: 15.795928 | Discriminator Loss: 0.134258\n","| Global Epoch: 73 | Local Epoch: 1 | Generator Loss: 15.474722 | Discriminator Loss: 0.075766\n","| Global Epoch: 73 | Local Epoch: 2 | Generator Loss: 15.248020 | Discriminator Loss: 0.051980\n","| Global Epoch: 73 | Local Epoch: 3 | Generator Loss: 15.239688 | Discriminator Loss: 0.040581\n","| Global Epoch: 73 | Local Epoch: 4 | Generator Loss: 15.115883 | Discriminator Loss: 0.042096\n","| Global Epoch: 73 | Local Epoch: 5 | Generator Loss: 15.132812 | Discriminator Loss: 0.036925\n","| Global Epoch: 73 | Local Epoch: 6 | Generator Loss: 15.135022 | Discriminator Loss: 0.037806\n","| Global Epoch: 73 | Local Epoch: 7 | Generator Loss: 15.150474 | Discriminator Loss: 0.036268\n","| Global Epoch: 73 | Local Epoch: 8 | Generator Loss: 15.195052 | Discriminator Loss: 0.032895\n","| Global Epoch: 73 | Local Epoch: 9 | Generator Loss: 15.311317 | Discriminator Loss: 0.029868\n","| Global Epoch: 73 | Local Epoch: 0 | Generator Loss: 15.606938 | Discriminator Loss: 0.258163\n","| Global Epoch: 73 | Local Epoch: 1 | Generator Loss: 14.942146 | Discriminator Loss: 0.145142\n","| Global Epoch: 73 | Local Epoch: 2 | Generator Loss: 14.751404 | Discriminator Loss: 0.099437\n","| Global Epoch: 73 | Local Epoch: 3 | Generator Loss: 14.652115 | Discriminator Loss: 0.075691\n","| Global Epoch: 73 | Local Epoch: 4 | Generator Loss: 14.599889 | Discriminator Loss: 0.061217\n","| Global Epoch: 73 | Local Epoch: 5 | Generator Loss: 14.644526 | Discriminator Loss: 0.051443\n","| Global Epoch: 73 | Local Epoch: 6 | Generator Loss: 14.687388 | Discriminator Loss: 0.044391\n","| Global Epoch: 73 | Local Epoch: 7 | Generator Loss: 14.748937 | Discriminator Loss: 0.039064\n","| Global Epoch: 73 | Local Epoch: 8 | Generator Loss: 14.796157 | Discriminator Loss: 0.034901\n","| Global Epoch: 73 | Local Epoch: 9 | Generator Loss: 14.863128 | Discriminator Loss: 0.031556\n","After 74 epoch global training, averged local generator loss is: 15.8509, averged local discrimitor loss is: 0.0671\n","Averged validation loss is: 4.8479\n","| Global Epoch: 74 | Local Epoch: 0 | Generator Loss: 15.753983 | Discriminator Loss: 0.086462\n","| Global Epoch: 74 | Local Epoch: 1 | Generator Loss: 15.365181 | Discriminator Loss: 0.049824\n","| Global Epoch: 74 | Local Epoch: 2 | Generator Loss: 15.240777 | Discriminator Loss: 0.034212\n","| Global Epoch: 74 | Local Epoch: 3 | Generator Loss: 15.226193 | Discriminator Loss: 0.026096\n","| Global Epoch: 74 | Local Epoch: 4 | Generator Loss: 15.228048 | Discriminator Loss: 0.021167\n","| Global Epoch: 74 | Local Epoch: 5 | Generator Loss: 15.343894 | Discriminator Loss: 0.017830\n","| Global Epoch: 74 | Local Epoch: 6 | Generator Loss: 15.447448 | Discriminator Loss: 0.015444\n","| Global Epoch: 74 | Local Epoch: 7 | Generator Loss: 15.619115 | Discriminator Loss: 0.013659\n","| Global Epoch: 74 | Local Epoch: 8 | Generator Loss: 15.727355 | Discriminator Loss: 0.012230\n","| Global Epoch: 74 | Local Epoch: 9 | Generator Loss: 15.783186 | Discriminator Loss: 0.011072\n","| Global Epoch: 74 | Local Epoch: 0 | Generator Loss: 15.651671 | Discriminator Loss: 0.068149\n","| Global Epoch: 74 | Local Epoch: 1 | Generator Loss: 15.801089 | Discriminator Loss: 0.042867\n","| Global Epoch: 74 | Local Epoch: 2 | Generator Loss: 15.872049 | Discriminator Loss: 0.029966\n","| Global Epoch: 74 | Local Epoch: 3 | Generator Loss: 15.793982 | Discriminator Loss: 0.024104\n","| Global Epoch: 74 | Local Epoch: 4 | Generator Loss: 15.842219 | Discriminator Loss: 0.019974\n","| Global Epoch: 74 | Local Epoch: 5 | Generator Loss: 15.881437 | Discriminator Loss: 0.016910\n","| Global Epoch: 74 | Local Epoch: 6 | Generator Loss: 15.792495 | Discriminator Loss: 0.019012\n","| Global Epoch: 74 | Local Epoch: 7 | Generator Loss: 15.690459 | Discriminator Loss: 0.019684\n","| Global Epoch: 74 | Local Epoch: 8 | Generator Loss: 15.707928 | Discriminator Loss: 0.018726\n","| Global Epoch: 74 | Local Epoch: 9 | Generator Loss: 15.705809 | Discriminator Loss: 0.017528\n","| Global Epoch: 74 | Local Epoch: 0 | Generator Loss: 15.678640 | Discriminator Loss: 0.123522\n","| Global Epoch: 74 | Local Epoch: 1 | Generator Loss: 15.098037 | Discriminator Loss: 0.070412\n","| Global Epoch: 74 | Local Epoch: 2 | Generator Loss: 15.145017 | Discriminator Loss: 0.048876\n","| Global Epoch: 74 | Local Epoch: 3 | Generator Loss: 15.254303 | Discriminator Loss: 0.037289\n","| Global Epoch: 74 | Local Epoch: 4 | Generator Loss: 15.252260 | Discriminator Loss: 0.030221\n","| Global Epoch: 74 | Local Epoch: 5 | Generator Loss: 15.291683 | Discriminator Loss: 0.025448\n","| Global Epoch: 74 | Local Epoch: 6 | Generator Loss: 15.333821 | Discriminator Loss: 0.022033\n","| Global Epoch: 74 | Local Epoch: 7 | Generator Loss: 15.452617 | Discriminator Loss: 0.019515\n","| Global Epoch: 74 | Local Epoch: 8 | Generator Loss: 15.517659 | Discriminator Loss: 0.017676\n","| Global Epoch: 74 | Local Epoch: 9 | Generator Loss: 15.593040 | Discriminator Loss: 0.016048\n","| Global Epoch: 74 | Local Epoch: 0 | Generator Loss: 15.623728 | Discriminator Loss: 0.072918\n","| Global Epoch: 74 | Local Epoch: 1 | Generator Loss: 15.689118 | Discriminator Loss: 0.042160\n","| Global Epoch: 74 | Local Epoch: 2 | Generator Loss: 15.647609 | Discriminator Loss: 0.029126\n","| Global Epoch: 74 | Local Epoch: 3 | Generator Loss: 15.638828 | Discriminator Loss: 0.022390\n","| Global Epoch: 74 | Local Epoch: 4 | Generator Loss: 15.806484 | Discriminator Loss: 0.018160\n","| Global Epoch: 74 | Local Epoch: 5 | Generator Loss: 15.837150 | Discriminator Loss: 0.015284\n","| Global Epoch: 74 | Local Epoch: 6 | Generator Loss: 15.915743 | Discriminator Loss: 0.013216\n","| Global Epoch: 74 | Local Epoch: 7 | Generator Loss: 15.948831 | Discriminator Loss: 0.011645\n","| Global Epoch: 74 | Local Epoch: 8 | Generator Loss: 15.958742 | Discriminator Loss: 0.010409\n","| Global Epoch: 74 | Local Epoch: 9 | Generator Loss: 16.022024 | Discriminator Loss: 0.009448\n","| Global Epoch: 74 | Local Epoch: 0 | Generator Loss: 15.608749 | Discriminator Loss: 0.054340\n","| Global Epoch: 74 | Local Epoch: 1 | Generator Loss: 15.093048 | Discriminator Loss: 0.032656\n","| Global Epoch: 74 | Local Epoch: 2 | Generator Loss: 15.093509 | Discriminator Loss: 0.022578\n","| Global Epoch: 74 | Local Epoch: 3 | Generator Loss: 15.183486 | Discriminator Loss: 0.017231\n","| Global Epoch: 74 | Local Epoch: 4 | Generator Loss: 15.361473 | Discriminator Loss: 0.013986\n","| Global Epoch: 74 | Local Epoch: 5 | Generator Loss: 15.499403 | Discriminator Loss: 0.011764\n","| Global Epoch: 74 | Local Epoch: 6 | Generator Loss: 15.576714 | Discriminator Loss: 0.025167\n","| Global Epoch: 74 | Local Epoch: 7 | Generator Loss: 15.323286 | Discriminator Loss: 0.052262\n","| Global Epoch: 74 | Local Epoch: 8 | Generator Loss: 15.222451 | Discriminator Loss: 0.051073\n","| Global Epoch: 74 | Local Epoch: 9 | Generator Loss: 15.001712 | Discriminator Loss: 0.056025\n","After 75 epoch global training, averged local generator loss is: 15.8396, averged local discrimitor loss is: 0.0670\n","Averged validation loss is: 4.4287\n","| Global Epoch: 75 | Local Epoch: 0 | Generator Loss: 14.542985 | Discriminator Loss: 0.328368\n","| Global Epoch: 75 | Local Epoch: 1 | Generator Loss: 14.937880 | Discriminator Loss: 0.190114\n","| Global Epoch: 75 | Local Epoch: 2 | Generator Loss: 14.844819 | Discriminator Loss: 0.130480\n","| Global Epoch: 75 | Local Epoch: 3 | Generator Loss: 14.824646 | Discriminator Loss: 0.099309\n","| Global Epoch: 75 | Local Epoch: 4 | Generator Loss: 14.987895 | Discriminator Loss: 0.080720\n","| Global Epoch: 75 | Local Epoch: 5 | Generator Loss: 15.068222 | Discriminator Loss: 0.068070\n","| Global Epoch: 75 | Local Epoch: 6 | Generator Loss: 15.151766 | Discriminator Loss: 0.058906\n","| Global Epoch: 75 | Local Epoch: 7 | Generator Loss: 15.305870 | Discriminator Loss: 0.052888\n","| Global Epoch: 75 | Local Epoch: 8 | Generator Loss: 15.305424 | Discriminator Loss: 0.047536\n","| Global Epoch: 75 | Local Epoch: 9 | Generator Loss: 15.309629 | Discriminator Loss: 0.043020\n","| Global Epoch: 75 | Local Epoch: 0 | Generator Loss: 14.977311 | Discriminator Loss: 0.211083\n","| Global Epoch: 75 | Local Epoch: 1 | Generator Loss: 14.363082 | Discriminator Loss: 0.116965\n","| Global Epoch: 75 | Local Epoch: 2 | Generator Loss: 14.289101 | Discriminator Loss: 0.084195\n","| Global Epoch: 75 | Local Epoch: 3 | Generator Loss: 14.360742 | Discriminator Loss: 0.064955\n","| Global Epoch: 75 | Local Epoch: 4 | Generator Loss: 14.485754 | Discriminator Loss: 0.052640\n","| Global Epoch: 75 | Local Epoch: 5 | Generator Loss: 14.656357 | Discriminator Loss: 0.044292\n","| Global Epoch: 75 | Local Epoch: 6 | Generator Loss: 14.720879 | Discriminator Loss: 0.038234\n","| Global Epoch: 75 | Local Epoch: 7 | Generator Loss: 14.768726 | Discriminator Loss: 0.033658\n","| Global Epoch: 75 | Local Epoch: 8 | Generator Loss: 14.845534 | Discriminator Loss: 0.030147\n","| Global Epoch: 75 | Local Epoch: 9 | Generator Loss: 14.910936 | Discriminator Loss: 0.027265\n","| Global Epoch: 75 | Local Epoch: 0 | Generator Loss: 15.654732 | Discriminator Loss: 0.039324\n","| Global Epoch: 75 | Local Epoch: 1 | Generator Loss: 15.414429 | Discriminator Loss: 0.024480\n","| Global Epoch: 75 | Local Epoch: 2 | Generator Loss: 15.342894 | Discriminator Loss: 0.017205\n","| Global Epoch: 75 | Local Epoch: 3 | Generator Loss: 15.394668 | Discriminator Loss: 0.013300\n","| Global Epoch: 75 | Local Epoch: 4 | Generator Loss: 15.502393 | Discriminator Loss: 0.012408\n","| Global Epoch: 75 | Local Epoch: 5 | Generator Loss: 15.563303 | Discriminator Loss: 0.015185\n","| Global Epoch: 75 | Local Epoch: 6 | Generator Loss: 15.588881 | Discriminator Loss: 0.024902\n","| Global Epoch: 75 | Local Epoch: 7 | Generator Loss: 15.373266 | Discriminator Loss: 0.033652\n","| Global Epoch: 75 | Local Epoch: 8 | Generator Loss: 15.318047 | Discriminator Loss: 0.032671\n","| Global Epoch: 75 | Local Epoch: 9 | Generator Loss: 15.349664 | Discriminator Loss: 0.030046\n","| Global Epoch: 75 | Local Epoch: 0 | Generator Loss: 15.427372 | Discriminator Loss: 0.103891\n","| Global Epoch: 75 | Local Epoch: 1 | Generator Loss: 14.674133 | Discriminator Loss: 0.060973\n","| Global Epoch: 75 | Local Epoch: 2 | Generator Loss: 14.919684 | Discriminator Loss: 0.090901\n","| Global Epoch: 75 | Local Epoch: 3 | Generator Loss: 14.600868 | Discriminator Loss: 0.090092\n","| Global Epoch: 75 | Local Epoch: 4 | Generator Loss: 14.632712 | Discriminator Loss: 0.073554\n","| Global Epoch: 75 | Local Epoch: 5 | Generator Loss: 14.776410 | Discriminator Loss: 0.071026\n","| Global Epoch: 75 | Local Epoch: 6 | Generator Loss: 14.663212 | Discriminator Loss: 0.075244\n","| Global Epoch: 75 | Local Epoch: 7 | Generator Loss: 14.619824 | Discriminator Loss: 0.068221\n","| Global Epoch: 75 | Local Epoch: 8 | Generator Loss: 14.686428 | Discriminator Loss: 0.061816\n","| Global Epoch: 75 | Local Epoch: 9 | Generator Loss: 14.809820 | Discriminator Loss: 0.056537\n","| Global Epoch: 75 | Local Epoch: 0 | Generator Loss: 15.166718 | Discriminator Loss: 0.427281\n","| Global Epoch: 75 | Local Epoch: 1 | Generator Loss: 15.005864 | Discriminator Loss: 0.221824\n","| Global Epoch: 75 | Local Epoch: 2 | Generator Loss: 14.804121 | Discriminator Loss: 0.150055\n","| Global Epoch: 75 | Local Epoch: 3 | Generator Loss: 14.617752 | Discriminator Loss: 0.113552\n","| Global Epoch: 75 | Local Epoch: 4 | Generator Loss: 14.797346 | Discriminator Loss: 0.098752\n","| Global Epoch: 75 | Local Epoch: 5 | Generator Loss: 15.013664 | Discriminator Loss: 0.083802\n","| Global Epoch: 75 | Local Epoch: 6 | Generator Loss: 15.244341 | Discriminator Loss: 0.073043\n","| Global Epoch: 75 | Local Epoch: 7 | Generator Loss: 15.439177 | Discriminator Loss: 0.064396\n","| Global Epoch: 75 | Local Epoch: 8 | Generator Loss: 15.560464 | Discriminator Loss: 0.057476\n","| Global Epoch: 75 | Local Epoch: 9 | Generator Loss: 15.615742 | Discriminator Loss: 0.051864\n","After 76 epoch global training, averged local generator loss is: 15.8367, averged local discrimitor loss is: 0.0668\n","Averged validation loss is: 7.9364\n","| Global Epoch: 76 | Local Epoch: 0 | Generator Loss: 15.029901 | Discriminator Loss: 0.239950\n","| Global Epoch: 76 | Local Epoch: 1 | Generator Loss: 14.714923 | Discriminator Loss: 0.129897\n","| Global Epoch: 76 | Local Epoch: 2 | Generator Loss: 14.612281 | Discriminator Loss: 0.088819\n","| Global Epoch: 76 | Local Epoch: 3 | Generator Loss: 14.700806 | Discriminator Loss: 0.067664\n","| Global Epoch: 76 | Local Epoch: 4 | Generator Loss: 14.893089 | Discriminator Loss: 0.054888\n","| Global Epoch: 76 | Local Epoch: 5 | Generator Loss: 14.990204 | Discriminator Loss: 0.046132\n","| Global Epoch: 76 | Local Epoch: 6 | Generator Loss: 15.105926 | Discriminator Loss: 0.039851\n","| Global Epoch: 76 | Local Epoch: 7 | Generator Loss: 15.178995 | Discriminator Loss: 0.035076\n","| Global Epoch: 76 | Local Epoch: 8 | Generator Loss: 15.208087 | Discriminator Loss: 0.031339\n","| Global Epoch: 76 | Local Epoch: 9 | Generator Loss: 15.243519 | Discriminator Loss: 0.028338\n","| Global Epoch: 76 | Local Epoch: 0 | Generator Loss: 15.630691 | Discriminator Loss: 0.030345\n","| Global Epoch: 76 | Local Epoch: 1 | Generator Loss: 15.860383 | Discriminator Loss: 0.018157\n","| Global Epoch: 76 | Local Epoch: 2 | Generator Loss: 15.834900 | Discriminator Loss: 0.012601\n","| Global Epoch: 76 | Local Epoch: 3 | Generator Loss: 15.840514 | Discriminator Loss: 0.013487\n","| Global Epoch: 76 | Local Epoch: 4 | Generator Loss: 15.881480 | Discriminator Loss: 0.013829\n","| Global Epoch: 76 | Local Epoch: 5 | Generator Loss: 15.891468 | Discriminator Loss: 0.012580\n","| Global Epoch: 76 | Local Epoch: 6 | Generator Loss: 15.887303 | Discriminator Loss: 0.011255\n","| Global Epoch: 76 | Local Epoch: 7 | Generator Loss: 15.934251 | Discriminator Loss: 0.010046\n","| Global Epoch: 76 | Local Epoch: 8 | Generator Loss: 15.987359 | Discriminator Loss: 0.009041\n","| Global Epoch: 76 | Local Epoch: 9 | Generator Loss: 15.984644 | Discriminator Loss: 0.010841\n","| Global Epoch: 76 | Local Epoch: 0 | Generator Loss: 16.285495 | Discriminator Loss: 0.323820\n","| Global Epoch: 76 | Local Epoch: 1 | Generator Loss: 16.131295 | Discriminator Loss: 0.176022\n","| Global Epoch: 76 | Local Epoch: 2 | Generator Loss: 16.058499 | Discriminator Loss: 0.120555\n","| Global Epoch: 76 | Local Epoch: 3 | Generator Loss: 16.046764 | Discriminator Loss: 0.091742\n","| Global Epoch: 76 | Local Epoch: 4 | Generator Loss: 15.963836 | Discriminator Loss: 0.074134\n","| Global Epoch: 76 | Local Epoch: 5 | Generator Loss: 15.923907 | Discriminator Loss: 0.062282\n","| Global Epoch: 76 | Local Epoch: 6 | Generator Loss: 16.012511 | Discriminator Loss: 0.053784\n","| Global Epoch: 76 | Local Epoch: 7 | Generator Loss: 16.074898 | Discriminator Loss: 0.047344\n","| Global Epoch: 76 | Local Epoch: 8 | Generator Loss: 16.125058 | Discriminator Loss: 0.042338\n","| Global Epoch: 76 | Local Epoch: 9 | Generator Loss: 16.232574 | Discriminator Loss: 0.038318\n","| Global Epoch: 76 | Local Epoch: 0 | Generator Loss: 16.276755 | Discriminator Loss: 0.176907\n","| Global Epoch: 76 | Local Epoch: 1 | Generator Loss: 15.918479 | Discriminator Loss: 0.100369\n","| Global Epoch: 76 | Local Epoch: 2 | Generator Loss: 15.608609 | Discriminator Loss: 0.071229\n","| Global Epoch: 76 | Local Epoch: 3 | Generator Loss: 15.721797 | Discriminator Loss: 0.054664\n","| Global Epoch: 76 | Local Epoch: 4 | Generator Loss: 15.787478 | Discriminator Loss: 0.044358\n","| Global Epoch: 76 | Local Epoch: 5 | Generator Loss: 15.918787 | Discriminator Loss: 0.037726\n","| Global Epoch: 76 | Local Epoch: 6 | Generator Loss: 16.019202 | Discriminator Loss: 0.032611\n","| Global Epoch: 76 | Local Epoch: 7 | Generator Loss: 16.044661 | Discriminator Loss: 0.028732\n","| Global Epoch: 76 | Local Epoch: 8 | Generator Loss: 16.082885 | Discriminator Loss: 0.025666\n","| Global Epoch: 76 | Local Epoch: 9 | Generator Loss: 16.127627 | Discriminator Loss: 0.023206\n","| Global Epoch: 76 | Local Epoch: 0 | Generator Loss: 16.374366 | Discriminator Loss: 0.139195\n","| Global Epoch: 76 | Local Epoch: 1 | Generator Loss: 16.128169 | Discriminator Loss: 0.080113\n","| Global Epoch: 76 | Local Epoch: 2 | Generator Loss: 16.004097 | Discriminator Loss: 0.055382\n","| Global Epoch: 76 | Local Epoch: 3 | Generator Loss: 16.018340 | Discriminator Loss: 0.042333\n","| Global Epoch: 76 | Local Epoch: 4 | Generator Loss: 16.103115 | Discriminator Loss: 0.034293\n","| Global Epoch: 76 | Local Epoch: 5 | Generator Loss: 16.133250 | Discriminator Loss: 0.028867\n","| Global Epoch: 76 | Local Epoch: 6 | Generator Loss: 16.147595 | Discriminator Loss: 0.024946\n","| Global Epoch: 76 | Local Epoch: 7 | Generator Loss: 16.128748 | Discriminator Loss: 0.021971\n","| Global Epoch: 76 | Local Epoch: 8 | Generator Loss: 16.164372 | Discriminator Loss: 0.019653\n","| Global Epoch: 76 | Local Epoch: 9 | Generator Loss: 16.186785 | Discriminator Loss: 0.017782\n","After 77 epoch global training, averged local generator loss is: 15.8412, averged local discrimitor loss is: 0.0662\n","Averged validation loss is: 4.6607\n","| Global Epoch: 77 | Local Epoch: 0 | Generator Loss: 15.699590 | Discriminator Loss: 0.019853\n","| Global Epoch: 77 | Local Epoch: 1 | Generator Loss: 15.460910 | Discriminator Loss: 0.026829\n","| Global Epoch: 77 | Local Epoch: 2 | Generator Loss: 15.345300 | Discriminator Loss: 0.024489\n","| Global Epoch: 77 | Local Epoch: 3 | Generator Loss: 15.393067 | Discriminator Loss: 0.020437\n","| Global Epoch: 77 | Local Epoch: 4 | Generator Loss: 15.481944 | Discriminator Loss: 0.016946\n","| Global Epoch: 77 | Local Epoch: 5 | Generator Loss: 15.520870 | Discriminator Loss: 0.014327\n","| Global Epoch: 77 | Local Epoch: 6 | Generator Loss: 15.654101 | Discriminator Loss: 0.012392\n","| Global Epoch: 77 | Local Epoch: 7 | Generator Loss: 15.707590 | Discriminator Loss: 0.010917\n","| Global Epoch: 77 | Local Epoch: 8 | Generator Loss: 15.816913 | Discriminator Loss: 0.009799\n","| Global Epoch: 77 | Local Epoch: 9 | Generator Loss: 15.860440 | Discriminator Loss: 0.008867\n","| Global Epoch: 77 | Local Epoch: 0 | Generator Loss: 16.072792 | Discriminator Loss: 0.050579\n","| Global Epoch: 77 | Local Epoch: 1 | Generator Loss: 15.988732 | Discriminator Loss: 0.029437\n","| Global Epoch: 77 | Local Epoch: 2 | Generator Loss: 15.928084 | Discriminator Loss: 0.020350\n","| Global Epoch: 77 | Local Epoch: 3 | Generator Loss: 15.899299 | Discriminator Loss: 0.015517\n","| Global Epoch: 77 | Local Epoch: 4 | Generator Loss: 15.923665 | Discriminator Loss: 0.012576\n","| Global Epoch: 77 | Local Epoch: 5 | Generator Loss: 15.946719 | Discriminator Loss: 0.010588\n","| Global Epoch: 77 | Local Epoch: 6 | Generator Loss: 16.049089 | Discriminator Loss: 0.009161\n","| Global Epoch: 77 | Local Epoch: 7 | Generator Loss: 16.085846 | Discriminator Loss: 0.008073\n","| Global Epoch: 77 | Local Epoch: 8 | Generator Loss: 16.114794 | Discriminator Loss: 0.007230\n","| Global Epoch: 77 | Local Epoch: 9 | Generator Loss: 16.010619 | Discriminator Loss: 0.022647\n","| Global Epoch: 77 | Local Epoch: 0 | Generator Loss: 15.167186 | Discriminator Loss: 0.131837\n","| Global Epoch: 77 | Local Epoch: 1 | Generator Loss: 15.295502 | Discriminator Loss: 0.070717\n","| Global Epoch: 77 | Local Epoch: 2 | Generator Loss: 15.369355 | Discriminator Loss: 0.048406\n","| Global Epoch: 77 | Local Epoch: 3 | Generator Loss: 15.391518 | Discriminator Loss: 0.036804\n","| Global Epoch: 77 | Local Epoch: 4 | Generator Loss: 15.414439 | Discriminator Loss: 0.029746\n","| Global Epoch: 77 | Local Epoch: 5 | Generator Loss: 15.577669 | Discriminator Loss: 0.025004\n","| Global Epoch: 77 | Local Epoch: 6 | Generator Loss: 15.693269 | Discriminator Loss: 0.021600\n","| Global Epoch: 77 | Local Epoch: 7 | Generator Loss: 15.613008 | Discriminator Loss: 0.030708\n","| Global Epoch: 77 | Local Epoch: 8 | Generator Loss: 15.530238 | Discriminator Loss: 0.036672\n","| Global Epoch: 77 | Local Epoch: 9 | Generator Loss: 15.424270 | Discriminator Loss: 0.036536\n","| Global Epoch: 77 | Local Epoch: 0 | Generator Loss: 14.623003 | Discriminator Loss: 0.428214\n","| Global Epoch: 77 | Local Epoch: 1 | Generator Loss: 13.883801 | Discriminator Loss: 0.240604\n","| Global Epoch: 77 | Local Epoch: 2 | Generator Loss: 13.820678 | Discriminator Loss: 0.165856\n","| Global Epoch: 77 | Local Epoch: 3 | Generator Loss: 13.917907 | Discriminator Loss: 0.126545\n","| Global Epoch: 77 | Local Epoch: 4 | Generator Loss: 13.994312 | Discriminator Loss: 0.102359\n","| Global Epoch: 77 | Local Epoch: 5 | Generator Loss: 14.115786 | Discriminator Loss: 0.086025\n","| Global Epoch: 77 | Local Epoch: 6 | Generator Loss: 14.289723 | Discriminator Loss: 0.074472\n","| Global Epoch: 77 | Local Epoch: 7 | Generator Loss: 14.403332 | Discriminator Loss: 0.065833\n","| Global Epoch: 77 | Local Epoch: 8 | Generator Loss: 14.523736 | Discriminator Loss: 0.058825\n","| Global Epoch: 77 | Local Epoch: 9 | Generator Loss: 14.603727 | Discriminator Loss: 0.053158\n","| Global Epoch: 77 | Local Epoch: 0 | Generator Loss: 15.609160 | Discriminator Loss: 0.143185\n","| Global Epoch: 77 | Local Epoch: 1 | Generator Loss: 15.357406 | Discriminator Loss: 0.080495\n","| Global Epoch: 77 | Local Epoch: 2 | Generator Loss: 15.218148 | Discriminator Loss: 0.055173\n","| Global Epoch: 77 | Local Epoch: 3 | Generator Loss: 15.144476 | Discriminator Loss: 0.042012\n","| Global Epoch: 77 | Local Epoch: 4 | Generator Loss: 15.306338 | Discriminator Loss: 0.036404\n","| Global Epoch: 77 | Local Epoch: 5 | Generator Loss: 15.343494 | Discriminator Loss: 0.030828\n","| Global Epoch: 77 | Local Epoch: 6 | Generator Loss: 15.454504 | Discriminator Loss: 0.026696\n","| Global Epoch: 77 | Local Epoch: 7 | Generator Loss: 15.517552 | Discriminator Loss: 0.023547\n","| Global Epoch: 77 | Local Epoch: 8 | Generator Loss: 15.585251 | Discriminator Loss: 0.021064\n","| Global Epoch: 77 | Local Epoch: 9 | Generator Loss: 15.641844 | Discriminator Loss: 0.019056\n","After 78 epoch global training, averged local generator loss is: 15.8387, averged local discrimitor loss is: 0.0656\n","Averged validation loss is: 6.0481\n","| Global Epoch: 78 | Local Epoch: 0 | Generator Loss: 15.519573 | Discriminator Loss: 0.039946\n","| Global Epoch: 78 | Local Epoch: 1 | Generator Loss: 15.150522 | Discriminator Loss: 0.023708\n","| Global Epoch: 78 | Local Epoch: 2 | Generator Loss: 15.178897 | Discriminator Loss: 0.050381\n","| Global Epoch: 78 | Local Epoch: 3 | Generator Loss: 14.868100 | Discriminator Loss: 0.084970\n","| Global Epoch: 78 | Local Epoch: 4 | Generator Loss: 14.291791 | Discriminator Loss: 0.085419\n","| Global Epoch: 78 | Local Epoch: 5 | Generator Loss: 14.132838 | Discriminator Loss: 0.074097\n","| Global Epoch: 78 | Local Epoch: 6 | Generator Loss: 14.058448 | Discriminator Loss: 0.069386\n","| Global Epoch: 78 | Local Epoch: 7 | Generator Loss: 14.110574 | Discriminator Loss: 0.062025\n","| Global Epoch: 78 | Local Epoch: 8 | Generator Loss: 14.093047 | Discriminator Loss: 0.067727\n","| Global Epoch: 78 | Local Epoch: 9 | Generator Loss: 14.005705 | Discriminator Loss: 0.075947\n","| Global Epoch: 78 | Local Epoch: 0 | Generator Loss: 13.884264 | Discriminator Loss: 0.469657\n","| Global Epoch: 78 | Local Epoch: 1 | Generator Loss: 13.696034 | Discriminator Loss: 0.256495\n","| Global Epoch: 78 | Local Epoch: 2 | Generator Loss: 13.602158 | Discriminator Loss: 0.175062\n","| Global Epoch: 78 | Local Epoch: 3 | Generator Loss: 13.734644 | Discriminator Loss: 0.133737\n","| Global Epoch: 78 | Local Epoch: 4 | Generator Loss: 13.883376 | Discriminator Loss: 0.108371\n","| Global Epoch: 78 | Local Epoch: 5 | Generator Loss: 13.874202 | Discriminator Loss: 0.090999\n","| Global Epoch: 78 | Local Epoch: 6 | Generator Loss: 13.859975 | Discriminator Loss: 0.078465\n","| Global Epoch: 78 | Local Epoch: 7 | Generator Loss: 13.880987 | Discriminator Loss: 0.069033\n","| Global Epoch: 78 | Local Epoch: 8 | Generator Loss: 13.969000 | Discriminator Loss: 0.061663\n","| Global Epoch: 78 | Local Epoch: 9 | Generator Loss: 14.034822 | Discriminator Loss: 0.055719\n","| Global Epoch: 78 | Local Epoch: 0 | Generator Loss: 13.616149 | Discriminator Loss: 0.076418\n","| Global Epoch: 78 | Local Epoch: 1 | Generator Loss: 13.614612 | Discriminator Loss: 0.043042\n","| Global Epoch: 78 | Local Epoch: 2 | Generator Loss: 13.860358 | Discriminator Loss: 0.029980\n","| Global Epoch: 78 | Local Epoch: 3 | Generator Loss: 14.121560 | Discriminator Loss: 0.022983\n","| Global Epoch: 78 | Local Epoch: 4 | Generator Loss: 14.247837 | Discriminator Loss: 0.020549\n","| Global Epoch: 78 | Local Epoch: 5 | Generator Loss: 14.400471 | Discriminator Loss: 0.018393\n","| Global Epoch: 78 | Local Epoch: 6 | Generator Loss: 14.470187 | Discriminator Loss: 0.016015\n","| Global Epoch: 78 | Local Epoch: 7 | Generator Loss: 14.527161 | Discriminator Loss: 0.014151\n","| Global Epoch: 78 | Local Epoch: 8 | Generator Loss: 14.538439 | Discriminator Loss: 0.017277\n","| Global Epoch: 78 | Local Epoch: 9 | Generator Loss: 14.514147 | Discriminator Loss: 0.018463\n","| Global Epoch: 78 | Local Epoch: 0 | Generator Loss: 14.964222 | Discriminator Loss: 0.393407\n","| Global Epoch: 78 | Local Epoch: 1 | Generator Loss: 14.635242 | Discriminator Loss: 0.216387\n","| Global Epoch: 78 | Local Epoch: 2 | Generator Loss: 14.578656 | Discriminator Loss: 0.149530\n","| Global Epoch: 78 | Local Epoch: 3 | Generator Loss: 14.558235 | Discriminator Loss: 0.117516\n","| Global Epoch: 78 | Local Epoch: 4 | Generator Loss: 14.611674 | Discriminator Loss: 0.095162\n","| Global Epoch: 78 | Local Epoch: 5 | Generator Loss: 14.650051 | Discriminator Loss: 0.080080\n","| Global Epoch: 78 | Local Epoch: 6 | Generator Loss: 14.629527 | Discriminator Loss: 0.069246\n","| Global Epoch: 78 | Local Epoch: 7 | Generator Loss: 14.735995 | Discriminator Loss: 0.060947\n","| Global Epoch: 78 | Local Epoch: 8 | Generator Loss: 14.850046 | Discriminator Loss: 0.054644\n","| Global Epoch: 78 | Local Epoch: 9 | Generator Loss: 14.783664 | Discriminator Loss: 0.049925\n","| Global Epoch: 78 | Local Epoch: 0 | Generator Loss: 15.982863 | Discriminator Loss: 0.140989\n","| Global Epoch: 78 | Local Epoch: 1 | Generator Loss: 15.718005 | Discriminator Loss: 0.078595\n","| Global Epoch: 78 | Local Epoch: 2 | Generator Loss: 15.697355 | Discriminator Loss: 0.053797\n","| Global Epoch: 78 | Local Epoch: 3 | Generator Loss: 15.791663 | Discriminator Loss: 0.041028\n","| Global Epoch: 78 | Local Epoch: 4 | Generator Loss: 15.757537 | Discriminator Loss: 0.033210\n","| Global Epoch: 78 | Local Epoch: 5 | Generator Loss: 15.734014 | Discriminator Loss: 0.027929\n","| Global Epoch: 78 | Local Epoch: 6 | Generator Loss: 15.733544 | Discriminator Loss: 0.024139\n","| Global Epoch: 78 | Local Epoch: 7 | Generator Loss: 15.838906 | Discriminator Loss: 0.021279\n","| Global Epoch: 78 | Local Epoch: 8 | Generator Loss: 15.888248 | Discriminator Loss: 0.019028\n","| Global Epoch: 78 | Local Epoch: 9 | Generator Loss: 15.822466 | Discriminator Loss: 0.022332\n","After 79 epoch global training, averged local generator loss is: 15.8385, averged local discrimitor loss is: 0.0650\n","Averged validation loss is: 5.9399\n","| Global Epoch: 79 | Local Epoch: 0 | Generator Loss: 15.304421 | Discriminator Loss: 0.129657\n","| Global Epoch: 79 | Local Epoch: 1 | Generator Loss: 15.065002 | Discriminator Loss: 0.074213\n","| Global Epoch: 79 | Local Epoch: 2 | Generator Loss: 15.048295 | Discriminator Loss: 0.051261\n","| Global Epoch: 79 | Local Epoch: 3 | Generator Loss: 15.006154 | Discriminator Loss: 0.039332\n","| Global Epoch: 79 | Local Epoch: 4 | Generator Loss: 15.068251 | Discriminator Loss: 0.031929\n","| Global Epoch: 79 | Local Epoch: 5 | Generator Loss: 15.086685 | Discriminator Loss: 0.026909\n","| Global Epoch: 79 | Local Epoch: 6 | Generator Loss: 15.147376 | Discriminator Loss: 0.023262\n","| Global Epoch: 79 | Local Epoch: 7 | Generator Loss: 15.210777 | Discriminator Loss: 0.020525\n","| Global Epoch: 79 | Local Epoch: 8 | Generator Loss: 15.250061 | Discriminator Loss: 0.018370\n","| Global Epoch: 79 | Local Epoch: 9 | Generator Loss: 15.305358 | Discriminator Loss: 0.016650\n","| Global Epoch: 79 | Local Epoch: 0 | Generator Loss: 15.177138 | Discriminator Loss: 0.085251\n","| Global Epoch: 79 | Local Epoch: 1 | Generator Loss: 15.294933 | Discriminator Loss: 0.047611\n","| Global Epoch: 79 | Local Epoch: 2 | Generator Loss: 15.320656 | Discriminator Loss: 0.033606\n","| Global Epoch: 79 | Local Epoch: 3 | Generator Loss: 15.673232 | Discriminator Loss: 0.025757\n","| Global Epoch: 79 | Local Epoch: 4 | Generator Loss: 15.760341 | Discriminator Loss: 0.020871\n","| Global Epoch: 79 | Local Epoch: 5 | Generator Loss: 15.854996 | Discriminator Loss: 0.017558\n","| Global Epoch: 79 | Local Epoch: 6 | Generator Loss: 15.889892 | Discriminator Loss: 0.015179\n","| Global Epoch: 79 | Local Epoch: 7 | Generator Loss: 15.927623 | Discriminator Loss: 0.013467\n","| Global Epoch: 79 | Local Epoch: 8 | Generator Loss: 16.030920 | Discriminator Loss: 0.012162\n","| Global Epoch: 79 | Local Epoch: 9 | Generator Loss: 16.045006 | Discriminator Loss: 0.011019\n","| Global Epoch: 79 | Local Epoch: 0 | Generator Loss: 16.404948 | Discriminator Loss: 0.182468\n","| Global Epoch: 79 | Local Epoch: 1 | Generator Loss: 16.381447 | Discriminator Loss: 0.104286\n","| Global Epoch: 79 | Local Epoch: 2 | Generator Loss: 16.228277 | Discriminator Loss: 0.071842\n","| Global Epoch: 79 | Local Epoch: 3 | Generator Loss: 16.138034 | Discriminator Loss: 0.054717\n","| Global Epoch: 79 | Local Epoch: 4 | Generator Loss: 16.082782 | Discriminator Loss: 0.044243\n","| Global Epoch: 79 | Local Epoch: 5 | Generator Loss: 16.050747 | Discriminator Loss: 0.037222\n","| Global Epoch: 79 | Local Epoch: 6 | Generator Loss: 16.122905 | Discriminator Loss: 0.032157\n","| Global Epoch: 79 | Local Epoch: 7 | Generator Loss: 16.182785 | Discriminator Loss: 0.028318\n","| Global Epoch: 79 | Local Epoch: 8 | Generator Loss: 16.232782 | Discriminator Loss: 0.025317\n","| Global Epoch: 79 | Local Epoch: 9 | Generator Loss: 16.246719 | Discriminator Loss: 0.022902\n","| Global Epoch: 79 | Local Epoch: 0 | Generator Loss: 16.114008 | Discriminator Loss: 0.168445\n","| Global Epoch: 79 | Local Epoch: 1 | Generator Loss: 15.643595 | Discriminator Loss: 0.095572\n","| Global Epoch: 79 | Local Epoch: 2 | Generator Loss: 15.463961 | Discriminator Loss: 0.066108\n","| Global Epoch: 79 | Local Epoch: 3 | Generator Loss: 15.445836 | Discriminator Loss: 0.050409\n","| Global Epoch: 79 | Local Epoch: 4 | Generator Loss: 15.537435 | Discriminator Loss: 0.040909\n","| Global Epoch: 79 | Local Epoch: 5 | Generator Loss: 15.602552 | Discriminator Loss: 0.034434\n","| Global Epoch: 79 | Local Epoch: 6 | Generator Loss: 15.603989 | Discriminator Loss: 0.029755\n","| Global Epoch: 79 | Local Epoch: 7 | Generator Loss: 15.588016 | Discriminator Loss: 0.026214\n","| Global Epoch: 79 | Local Epoch: 8 | Generator Loss: 15.626112 | Discriminator Loss: 0.023454\n","| Global Epoch: 79 | Local Epoch: 9 | Generator Loss: 15.679281 | Discriminator Loss: 0.021225\n","| Global Epoch: 79 | Local Epoch: 0 | Generator Loss: 15.385126 | Discriminator Loss: 0.036041\n","| Global Epoch: 79 | Local Epoch: 1 | Generator Loss: 15.666563 | Discriminator Loss: 0.021883\n","| Global Epoch: 79 | Local Epoch: 2 | Generator Loss: 15.676062 | Discriminator Loss: 0.015110\n","| Global Epoch: 79 | Local Epoch: 3 | Generator Loss: 16.010199 | Discriminator Loss: 0.011972\n","| Global Epoch: 79 | Local Epoch: 4 | Generator Loss: 15.992350 | Discriminator Loss: 0.009792\n","| Global Epoch: 79 | Local Epoch: 5 | Generator Loss: 15.951404 | Discriminator Loss: 0.008276\n","| Global Epoch: 79 | Local Epoch: 6 | Generator Loss: 15.895154 | Discriminator Loss: 0.007168\n","| Global Epoch: 79 | Local Epoch: 7 | Generator Loss: 15.971200 | Discriminator Loss: 0.006352\n","| Global Epoch: 79 | Local Epoch: 8 | Generator Loss: 16.065714 | Discriminator Loss: 0.005699\n","| Global Epoch: 79 | Local Epoch: 9 | Generator Loss: 16.154513 | Discriminator Loss: 0.005182\n","After 80 epoch global training, averged local generator loss is: 15.8424, averged local discrimitor loss is: 0.0643\n","Averged validation loss is: 6.3736\n","| Global Epoch: 80 | Local Epoch: 0 | Generator Loss: 16.271872 | Discriminator Loss: 0.031096\n","| Global Epoch: 80 | Local Epoch: 1 | Generator Loss: 15.784934 | Discriminator Loss: 0.066674\n","| Global Epoch: 80 | Local Epoch: 2 | Generator Loss: 15.134667 | Discriminator Loss: 0.076047\n","| Global Epoch: 80 | Local Epoch: 3 | Generator Loss: 15.068593 | Discriminator Loss: 0.058302\n","| Global Epoch: 80 | Local Epoch: 4 | Generator Loss: 15.036545 | Discriminator Loss: 0.047258\n","| Global Epoch: 80 | Local Epoch: 5 | Generator Loss: 15.043790 | Discriminator Loss: 0.040545\n","| Global Epoch: 80 | Local Epoch: 6 | Generator Loss: 15.117552 | Discriminator Loss: 0.034986\n","| Global Epoch: 80 | Local Epoch: 7 | Generator Loss: 15.192169 | Discriminator Loss: 0.030777\n","| Global Epoch: 80 | Local Epoch: 8 | Generator Loss: 15.280534 | Discriminator Loss: 0.027491\n","| Global Epoch: 80 | Local Epoch: 9 | Generator Loss: 15.337767 | Discriminator Loss: 0.029326\n","| Global Epoch: 80 | Local Epoch: 0 | Generator Loss: 15.204800 | Discriminator Loss: 0.044121\n","| Global Epoch: 80 | Local Epoch: 1 | Generator Loss: 15.471795 | Discriminator Loss: 0.028592\n","| Global Epoch: 80 | Local Epoch: 2 | Generator Loss: 15.608620 | Discriminator Loss: 0.020030\n","| Global Epoch: 80 | Local Epoch: 3 | Generator Loss: 15.633711 | Discriminator Loss: 0.017614\n","| Global Epoch: 80 | Local Epoch: 4 | Generator Loss: 15.589744 | Discriminator Loss: 0.015981\n","| Global Epoch: 80 | Local Epoch: 5 | Generator Loss: 15.693892 | Discriminator Loss: 0.014357\n","| Global Epoch: 80 | Local Epoch: 6 | Generator Loss: 15.692110 | Discriminator Loss: 0.013656\n","| Global Epoch: 80 | Local Epoch: 7 | Generator Loss: 15.734008 | Discriminator Loss: 0.014640\n","| Global Epoch: 80 | Local Epoch: 8 | Generator Loss: 15.800210 | Discriminator Loss: 0.018563\n","| Global Epoch: 80 | Local Epoch: 9 | Generator Loss: 15.824846 | Discriminator Loss: 0.021233\n","| Global Epoch: 80 | Local Epoch: 0 | Generator Loss: 14.906689 | Discriminator Loss: 0.225302\n","| Global Epoch: 80 | Local Epoch: 1 | Generator Loss: 14.863697 | Discriminator Loss: 0.126285\n","| Global Epoch: 80 | Local Epoch: 2 | Generator Loss: 14.739036 | Discriminator Loss: 0.086461\n","| Global Epoch: 80 | Local Epoch: 3 | Generator Loss: 14.772761 | Discriminator Loss: 0.065791\n","| Global Epoch: 80 | Local Epoch: 4 | Generator Loss: 14.778823 | Discriminator Loss: 0.053180\n","| Global Epoch: 80 | Local Epoch: 5 | Generator Loss: 14.824248 | Discriminator Loss: 0.044697\n","| Global Epoch: 80 | Local Epoch: 6 | Generator Loss: 14.887547 | Discriminator Loss: 0.038566\n","| Global Epoch: 80 | Local Epoch: 7 | Generator Loss: 14.929214 | Discriminator Loss: 0.033939\n","| Global Epoch: 80 | Local Epoch: 8 | Generator Loss: 14.964742 | Discriminator Loss: 0.030323\n","| Global Epoch: 80 | Local Epoch: 9 | Generator Loss: 15.023626 | Discriminator Loss: 0.037427\n","| Global Epoch: 80 | Local Epoch: 0 | Generator Loss: 15.429263 | Discriminator Loss: 0.276046\n","| Global Epoch: 80 | Local Epoch: 1 | Generator Loss: 15.320617 | Discriminator Loss: 0.152127\n","| Global Epoch: 80 | Local Epoch: 2 | Generator Loss: 15.208869 | Discriminator Loss: 0.103730\n","| Global Epoch: 80 | Local Epoch: 3 | Generator Loss: 15.239489 | Discriminator Loss: 0.078992\n","| Global Epoch: 80 | Local Epoch: 4 | Generator Loss: 15.305346 | Discriminator Loss: 0.063815\n","| Global Epoch: 80 | Local Epoch: 5 | Generator Loss: 15.320864 | Discriminator Loss: 0.053628\n","| Global Epoch: 80 | Local Epoch: 6 | Generator Loss: 15.392952 | Discriminator Loss: 0.046302\n","| Global Epoch: 80 | Local Epoch: 7 | Generator Loss: 15.523228 | Discriminator Loss: 0.040770\n","| Global Epoch: 80 | Local Epoch: 8 | Generator Loss: 15.627212 | Discriminator Loss: 0.036428\n","| Global Epoch: 80 | Local Epoch: 9 | Generator Loss: 15.688265 | Discriminator Loss: 0.032933\n","| Global Epoch: 80 | Local Epoch: 0 | Generator Loss: 16.050764 | Discriminator Loss: 0.191415\n","| Global Epoch: 80 | Local Epoch: 1 | Generator Loss: 16.449407 | Discriminator Loss: 0.107668\n","| Global Epoch: 80 | Local Epoch: 2 | Generator Loss: 16.305274 | Discriminator Loss: 0.073830\n","| Global Epoch: 80 | Local Epoch: 3 | Generator Loss: 16.202732 | Discriminator Loss: 0.056126\n","| Global Epoch: 80 | Local Epoch: 4 | Generator Loss: 16.136561 | Discriminator Loss: 0.045377\n","| Global Epoch: 80 | Local Epoch: 5 | Generator Loss: 16.104687 | Discriminator Loss: 0.038163\n","| Global Epoch: 80 | Local Epoch: 6 | Generator Loss: 16.180171 | Discriminator Loss: 0.032955\n","| Global Epoch: 80 | Local Epoch: 7 | Generator Loss: 16.191598 | Discriminator Loss: 0.029020\n","| Global Epoch: 80 | Local Epoch: 8 | Generator Loss: 16.220564 | Discriminator Loss: 0.025943\n","| Global Epoch: 80 | Local Epoch: 9 | Generator Loss: 16.264531 | Discriminator Loss: 0.023469\n","After 81 epoch global training, averged local generator loss is: 15.8476, averged local discrimitor loss is: 0.0638\n","Averged validation loss is: 7.0268\n","| Global Epoch: 81 | Local Epoch: 0 | Generator Loss: 15.591572 | Discriminator Loss: 0.024060\n","| Global Epoch: 81 | Local Epoch: 1 | Generator Loss: 15.866767 | Discriminator Loss: 0.023918\n","| Global Epoch: 81 | Local Epoch: 2 | Generator Loss: 15.921224 | Discriminator Loss: 0.017505\n","| Global Epoch: 81 | Local Epoch: 3 | Generator Loss: 16.150691 | Discriminator Loss: 0.013526\n","| Global Epoch: 81 | Local Epoch: 4 | Generator Loss: 16.245903 | Discriminator Loss: 0.011024\n","| Global Epoch: 81 | Local Epoch: 5 | Generator Loss: 16.293631 | Discriminator Loss: 0.009323\n","| Global Epoch: 81 | Local Epoch: 6 | Generator Loss: 16.242632 | Discriminator Loss: 0.008080\n","| Global Epoch: 81 | Local Epoch: 7 | Generator Loss: 16.267032 | Discriminator Loss: 0.007148\n","| Global Epoch: 81 | Local Epoch: 8 | Generator Loss: 16.299517 | Discriminator Loss: 0.006435\n","| Global Epoch: 81 | Local Epoch: 9 | Generator Loss: 16.344200 | Discriminator Loss: 0.005858\n","| Global Epoch: 81 | Local Epoch: 0 | Generator Loss: 16.198003 | Discriminator Loss: 0.039206\n","| Global Epoch: 81 | Local Epoch: 1 | Generator Loss: 16.063224 | Discriminator Loss: 0.024192\n","| Global Epoch: 81 | Local Epoch: 2 | Generator Loss: 16.087565 | Discriminator Loss: 0.017723\n","| Global Epoch: 81 | Local Epoch: 3 | Generator Loss: 15.909529 | Discriminator Loss: 0.013979\n","| Global Epoch: 81 | Local Epoch: 4 | Generator Loss: 15.933420 | Discriminator Loss: 0.011400\n","| Global Epoch: 81 | Local Epoch: 5 | Generator Loss: 15.907362 | Discriminator Loss: 0.009606\n","| Global Epoch: 81 | Local Epoch: 6 | Generator Loss: 16.053146 | Discriminator Loss: 0.008333\n","| Global Epoch: 81 | Local Epoch: 7 | Generator Loss: 16.147928 | Discriminator Loss: 0.007351\n","| Global Epoch: 81 | Local Epoch: 8 | Generator Loss: 16.215006 | Discriminator Loss: 0.006583\n","| Global Epoch: 81 | Local Epoch: 9 | Generator Loss: 16.255306 | Discriminator Loss: 0.005958\n","| Global Epoch: 81 | Local Epoch: 0 | Generator Loss: 16.135894 | Discriminator Loss: 0.096273\n","| Global Epoch: 81 | Local Epoch: 1 | Generator Loss: 15.945683 | Discriminator Loss: 0.054042\n","| Global Epoch: 81 | Local Epoch: 2 | Generator Loss: 15.845684 | Discriminator Loss: 0.037032\n","| Global Epoch: 81 | Local Epoch: 3 | Generator Loss: 15.746948 | Discriminator Loss: 0.028187\n","| Global Epoch: 81 | Local Epoch: 4 | Generator Loss: 15.829348 | Discriminator Loss: 0.022863\n","| Global Epoch: 81 | Local Epoch: 5 | Generator Loss: 15.899956 | Discriminator Loss: 0.019231\n","| Global Epoch: 81 | Local Epoch: 6 | Generator Loss: 15.923655 | Discriminator Loss: 0.016617\n","| Global Epoch: 81 | Local Epoch: 7 | Generator Loss: 15.925804 | Discriminator Loss: 0.014639\n","| Global Epoch: 81 | Local Epoch: 8 | Generator Loss: 16.001837 | Discriminator Loss: 0.013094\n","| Global Epoch: 81 | Local Epoch: 9 | Generator Loss: 16.036371 | Discriminator Loss: 0.011845\n","| Global Epoch: 81 | Local Epoch: 0 | Generator Loss: 16.020488 | Discriminator Loss: 0.112916\n","| Global Epoch: 81 | Local Epoch: 1 | Generator Loss: 15.620863 | Discriminator Loss: 0.064361\n","| Global Epoch: 81 | Local Epoch: 2 | Generator Loss: 15.461351 | Discriminator Loss: 0.044182\n","| Global Epoch: 81 | Local Epoch: 3 | Generator Loss: 15.461041 | Discriminator Loss: 0.033677\n","| Global Epoch: 81 | Local Epoch: 4 | Generator Loss: 15.508872 | Discriminator Loss: 0.027395\n","| Global Epoch: 81 | Local Epoch: 5 | Generator Loss: 15.597229 | Discriminator Loss: 0.023327\n","| Global Epoch: 81 | Local Epoch: 6 | Generator Loss: 15.579629 | Discriminator Loss: 0.022211\n","| Global Epoch: 81 | Local Epoch: 7 | Generator Loss: 15.566420 | Discriminator Loss: 0.020348\n","| Global Epoch: 81 | Local Epoch: 8 | Generator Loss: 15.636067 | Discriminator Loss: 0.018626\n","| Global Epoch: 81 | Local Epoch: 9 | Generator Loss: 15.674811 | Discriminator Loss: 0.020723\n","| Global Epoch: 81 | Local Epoch: 0 | Generator Loss: 15.425552 | Discriminator Loss: 0.180710\n","| Global Epoch: 81 | Local Epoch: 1 | Generator Loss: 15.207066 | Discriminator Loss: 0.097568\n","| Global Epoch: 81 | Local Epoch: 2 | Generator Loss: 15.132723 | Discriminator Loss: 0.066471\n","| Global Epoch: 81 | Local Epoch: 3 | Generator Loss: 15.174783 | Discriminator Loss: 0.050484\n","| Global Epoch: 81 | Local Epoch: 4 | Generator Loss: 15.316350 | Discriminator Loss: 0.040789\n","| Global Epoch: 81 | Local Epoch: 5 | Generator Loss: 15.438573 | Discriminator Loss: 0.034261\n","| Global Epoch: 81 | Local Epoch: 6 | Generator Loss: 15.523059 | Discriminator Loss: 0.029572\n","| Global Epoch: 81 | Local Epoch: 7 | Generator Loss: 15.548579 | Discriminator Loss: 0.026020\n","| Global Epoch: 81 | Local Epoch: 8 | Generator Loss: 15.595012 | Discriminator Loss: 0.023253\n","| Global Epoch: 81 | Local Epoch: 9 | Generator Loss: 15.695203 | Discriminator Loss: 0.021030\n","After 82 epoch global training, averged local generator loss is: 15.8458, averged local discrimitor loss is: 0.0632\n","Averged validation loss is: 5.2120\n","| Global Epoch: 82 | Local Epoch: 0 | Generator Loss: 15.791560 | Discriminator Loss: 0.013094\n","| Global Epoch: 82 | Local Epoch: 1 | Generator Loss: 15.940524 | Discriminator Loss: 0.028949\n","| Global Epoch: 82 | Local Epoch: 2 | Generator Loss: 15.668767 | Discriminator Loss: 0.025618\n","| Global Epoch: 82 | Local Epoch: 3 | Generator Loss: 15.614214 | Discriminator Loss: 0.020104\n","| Global Epoch: 82 | Local Epoch: 4 | Generator Loss: 15.826758 | Discriminator Loss: 0.019703\n","| Global Epoch: 82 | Local Epoch: 5 | Generator Loss: 15.858251 | Discriminator Loss: 0.017019\n","| Global Epoch: 82 | Local Epoch: 6 | Generator Loss: 15.974917 | Discriminator Loss: 0.014853\n","| Global Epoch: 82 | Local Epoch: 7 | Generator Loss: 16.035170 | Discriminator Loss: 0.013098\n","| Global Epoch: 82 | Local Epoch: 8 | Generator Loss: 16.055322 | Discriminator Loss: 0.011715\n","| Global Epoch: 82 | Local Epoch: 9 | Generator Loss: 16.068362 | Discriminator Loss: 0.010604\n","| Global Epoch: 82 | Local Epoch: 0 | Generator Loss: 15.993316 | Discriminator Loss: 0.162645\n","| Global Epoch: 82 | Local Epoch: 1 | Generator Loss: 15.583109 | Discriminator Loss: 0.092924\n","| Global Epoch: 82 | Local Epoch: 2 | Generator Loss: 15.566795 | Discriminator Loss: 0.063746\n","| Global Epoch: 82 | Local Epoch: 3 | Generator Loss: 15.615965 | Discriminator Loss: 0.048490\n","| Global Epoch: 82 | Local Epoch: 4 | Generator Loss: 15.668958 | Discriminator Loss: 0.039189\n","| Global Epoch: 82 | Local Epoch: 5 | Generator Loss: 15.695212 | Discriminator Loss: 0.032914\n","| Global Epoch: 82 | Local Epoch: 6 | Generator Loss: 15.691061 | Discriminator Loss: 0.028402\n","| Global Epoch: 82 | Local Epoch: 7 | Generator Loss: 15.676466 | Discriminator Loss: 0.025728\n","| Global Epoch: 82 | Local Epoch: 8 | Generator Loss: 15.663898 | Discriminator Loss: 0.023373\n","| Global Epoch: 82 | Local Epoch: 9 | Generator Loss: 15.699553 | Discriminator Loss: 0.021194\n","| Global Epoch: 82 | Local Epoch: 0 | Generator Loss: 15.979670 | Discriminator Loss: 0.167711\n","| Global Epoch: 82 | Local Epoch: 1 | Generator Loss: 15.718475 | Discriminator Loss: 0.094678\n","| Global Epoch: 82 | Local Epoch: 2 | Generator Loss: 15.601561 | Discriminator Loss: 0.066140\n","| Global Epoch: 82 | Local Epoch: 3 | Generator Loss: 15.475203 | Discriminator Loss: 0.050527\n","| Global Epoch: 82 | Local Epoch: 4 | Generator Loss: 15.430937 | Discriminator Loss: 0.040864\n","| Global Epoch: 82 | Local Epoch: 5 | Generator Loss: 15.418988 | Discriminator Loss: 0.034312\n","| Global Epoch: 82 | Local Epoch: 6 | Generator Loss: 15.420503 | Discriminator Loss: 0.029593\n","| Global Epoch: 82 | Local Epoch: 7 | Generator Loss: 15.425148 | Discriminator Loss: 0.026029\n","| Global Epoch: 82 | Local Epoch: 8 | Generator Loss: 15.443489 | Discriminator Loss: 0.023245\n","| Global Epoch: 82 | Local Epoch: 9 | Generator Loss: 15.522896 | Discriminator Loss: 0.021021\n","| Global Epoch: 82 | Local Epoch: 0 | Generator Loss: 15.305346 | Discriminator Loss: 0.056055\n","| Global Epoch: 82 | Local Epoch: 1 | Generator Loss: 15.083521 | Discriminator Loss: 0.033776\n","| Global Epoch: 82 | Local Epoch: 2 | Generator Loss: 15.252580 | Discriminator Loss: 0.023399\n","| Global Epoch: 82 | Local Epoch: 3 | Generator Loss: 15.454222 | Discriminator Loss: 0.017915\n","| Global Epoch: 82 | Local Epoch: 4 | Generator Loss: 15.544738 | Discriminator Loss: 0.014506\n","| Global Epoch: 82 | Local Epoch: 5 | Generator Loss: 15.658470 | Discriminator Loss: 0.012218\n","| Global Epoch: 82 | Local Epoch: 6 | Generator Loss: 15.732057 | Discriminator Loss: 0.010590\n","| Global Epoch: 82 | Local Epoch: 7 | Generator Loss: 15.869315 | Discriminator Loss: 0.009356\n","| Global Epoch: 82 | Local Epoch: 8 | Generator Loss: 15.915768 | Discriminator Loss: 0.008370\n","| Global Epoch: 82 | Local Epoch: 9 | Generator Loss: 15.919561 | Discriminator Loss: 0.007579\n","| Global Epoch: 82 | Local Epoch: 0 | Generator Loss: 15.751770 | Discriminator Loss: 0.037959\n","| Global Epoch: 82 | Local Epoch: 1 | Generator Loss: 15.554343 | Discriminator Loss: 0.027290\n","| Global Epoch: 82 | Local Epoch: 2 | Generator Loss: 15.568924 | Discriminator Loss: 0.019984\n","| Global Epoch: 82 | Local Epoch: 3 | Generator Loss: 15.752334 | Discriminator Loss: 0.015585\n","| Global Epoch: 82 | Local Epoch: 4 | Generator Loss: 15.795146 | Discriminator Loss: 0.017810\n","| Global Epoch: 82 | Local Epoch: 5 | Generator Loss: 15.778375 | Discriminator Loss: 0.022652\n","| Global Epoch: 82 | Local Epoch: 6 | Generator Loss: 15.696385 | Discriminator Loss: 0.024573\n","| Global Epoch: 82 | Local Epoch: 7 | Generator Loss: 15.697399 | Discriminator Loss: 0.023173\n","| Global Epoch: 82 | Local Epoch: 8 | Generator Loss: 15.773452 | Discriminator Loss: 0.020871\n","| Global Epoch: 82 | Local Epoch: 9 | Generator Loss: 15.828714 | Discriminator Loss: 0.018898\n","After 83 epoch global training, averged local generator loss is: 15.8456, averged local discrimitor loss is: 0.0627\n","Averged validation loss is: 6.1915\n","| Global Epoch: 83 | Local Epoch: 0 | Generator Loss: 16.043449 | Discriminator Loss: 0.157634\n","| Global Epoch: 83 | Local Epoch: 1 | Generator Loss: 15.783951 | Discriminator Loss: 0.088730\n","| Global Epoch: 83 | Local Epoch: 2 | Generator Loss: 15.695837 | Discriminator Loss: 0.060699\n","| Global Epoch: 83 | Local Epoch: 3 | Generator Loss: 15.791614 | Discriminator Loss: 0.047408\n","| Global Epoch: 83 | Local Epoch: 4 | Generator Loss: 15.876219 | Discriminator Loss: 0.038918\n","| Global Epoch: 83 | Local Epoch: 5 | Generator Loss: 15.942206 | Discriminator Loss: 0.032771\n","| Global Epoch: 83 | Local Epoch: 6 | Generator Loss: 16.035610 | Discriminator Loss: 0.029639\n","| Global Epoch: 83 | Local Epoch: 7 | Generator Loss: 15.905840 | Discriminator Loss: 0.034926\n","| Global Epoch: 83 | Local Epoch: 8 | Generator Loss: 15.912876 | Discriminator Loss: 0.032519\n","| Global Epoch: 83 | Local Epoch: 9 | Generator Loss: 15.925517 | Discriminator Loss: 0.029654\n","| Global Epoch: 83 | Local Epoch: 0 | Generator Loss: 15.456666 | Discriminator Loss: 0.118386\n","| Global Epoch: 83 | Local Epoch: 1 | Generator Loss: 15.399914 | Discriminator Loss: 0.066670\n","| Global Epoch: 83 | Local Epoch: 2 | Generator Loss: 15.455949 | Discriminator Loss: 0.045780\n","| Global Epoch: 83 | Local Epoch: 3 | Generator Loss: 15.398743 | Discriminator Loss: 0.035712\n","| Global Epoch: 83 | Local Epoch: 4 | Generator Loss: 15.632538 | Discriminator Loss: 0.029105\n","| Global Epoch: 83 | Local Epoch: 5 | Generator Loss: 15.764864 | Discriminator Loss: 0.025285\n","| Global Epoch: 83 | Local Epoch: 6 | Generator Loss: 15.854049 | Discriminator Loss: 0.021974\n","| Global Epoch: 83 | Local Epoch: 7 | Generator Loss: 15.897533 | Discriminator Loss: 0.019353\n","| Global Epoch: 83 | Local Epoch: 8 | Generator Loss: 15.940972 | Discriminator Loss: 0.017289\n","| Global Epoch: 83 | Local Epoch: 9 | Generator Loss: 15.974703 | Discriminator Loss: 0.015627\n","| Global Epoch: 83 | Local Epoch: 0 | Generator Loss: 16.200159 | Discriminator Loss: 0.108627\n","| Global Epoch: 83 | Local Epoch: 1 | Generator Loss: 15.939560 | Discriminator Loss: 0.066969\n","| Global Epoch: 83 | Local Epoch: 2 | Generator Loss: 15.883232 | Discriminator Loss: 0.074081\n","| Global Epoch: 83 | Local Epoch: 3 | Generator Loss: 15.478722 | Discriminator Loss: 0.092833\n","| Global Epoch: 83 | Local Epoch: 4 | Generator Loss: 15.425936 | Discriminator Loss: 0.076550\n","| Global Epoch: 83 | Local Epoch: 5 | Generator Loss: 15.368537 | Discriminator Loss: 0.064320\n","| Global Epoch: 83 | Local Epoch: 6 | Generator Loss: 15.449330 | Discriminator Loss: 0.056035\n","| Global Epoch: 83 | Local Epoch: 7 | Generator Loss: 15.425655 | Discriminator Loss: 0.049308\n","| Global Epoch: 83 | Local Epoch: 8 | Generator Loss: 15.447537 | Discriminator Loss: 0.044058\n","| Global Epoch: 83 | Local Epoch: 9 | Generator Loss: 15.449731 | Discriminator Loss: 0.039814\n","| Global Epoch: 83 | Local Epoch: 0 | Generator Loss: 15.361143 | Discriminator Loss: 0.382142\n","| Global Epoch: 83 | Local Epoch: 1 | Generator Loss: 14.995513 | Discriminator Loss: 0.209325\n","| Global Epoch: 83 | Local Epoch: 2 | Generator Loss: 14.840906 | Discriminator Loss: 0.142540\n","| Global Epoch: 83 | Local Epoch: 3 | Generator Loss: 14.751122 | Discriminator Loss: 0.108513\n","| Global Epoch: 83 | Local Epoch: 4 | Generator Loss: 14.956862 | Discriminator Loss: 0.088411\n","| Global Epoch: 83 | Local Epoch: 5 | Generator Loss: 14.984052 | Discriminator Loss: 0.074259\n","| Global Epoch: 83 | Local Epoch: 6 | Generator Loss: 15.101484 | Discriminator Loss: 0.064038\n","| Global Epoch: 83 | Local Epoch: 7 | Generator Loss: 15.152973 | Discriminator Loss: 0.056299\n","| Global Epoch: 83 | Local Epoch: 8 | Generator Loss: 15.190073 | Discriminator Loss: 0.050256\n","| Global Epoch: 83 | Local Epoch: 9 | Generator Loss: 15.189277 | Discriminator Loss: 0.045387\n","| Global Epoch: 83 | Local Epoch: 0 | Generator Loss: 16.004933 | Discriminator Loss: 0.086688\n","| Global Epoch: 83 | Local Epoch: 1 | Generator Loss: 15.647792 | Discriminator Loss: 0.049641\n","| Global Epoch: 83 | Local Epoch: 2 | Generator Loss: 15.434836 | Discriminator Loss: 0.034128\n","| Global Epoch: 83 | Local Epoch: 3 | Generator Loss: 15.372678 | Discriminator Loss: 0.026001\n","| Global Epoch: 83 | Local Epoch: 4 | Generator Loss: 15.358821 | Discriminator Loss: 0.021099\n","| Global Epoch: 83 | Local Epoch: 5 | Generator Loss: 15.396498 | Discriminator Loss: 0.017769\n","| Global Epoch: 83 | Local Epoch: 6 | Generator Loss: 15.424654 | Discriminator Loss: 0.015370\n","| Global Epoch: 83 | Local Epoch: 7 | Generator Loss: 15.479957 | Discriminator Loss: 0.013550\n","| Global Epoch: 83 | Local Epoch: 8 | Generator Loss: 15.499412 | Discriminator Loss: 0.012119\n","| Global Epoch: 83 | Local Epoch: 9 | Generator Loss: 15.508042 | Discriminator Loss: 0.010972\n","After 84 epoch global training, averged local generator loss is: 15.8415, averged local discrimitor loss is: 0.0621\n","Averged validation loss is: 4.7459\n","| Global Epoch: 84 | Local Epoch: 0 | Generator Loss: 15.149183 | Discriminator Loss: 0.102913\n","| Global Epoch: 84 | Local Epoch: 1 | Generator Loss: 15.013560 | Discriminator Loss: 0.060082\n","| Global Epoch: 84 | Local Epoch: 2 | Generator Loss: 15.010602 | Discriminator Loss: 0.041519\n","| Global Epoch: 84 | Local Epoch: 3 | Generator Loss: 14.974789 | Discriminator Loss: 0.031629\n","| Global Epoch: 84 | Local Epoch: 4 | Generator Loss: 15.194098 | Discriminator Loss: 0.025622\n","| Global Epoch: 84 | Local Epoch: 5 | Generator Loss: 15.369389 | Discriminator Loss: 0.021560\n","| Global Epoch: 84 | Local Epoch: 6 | Generator Loss: 15.454674 | Discriminator Loss: 0.018625\n","| Global Epoch: 84 | Local Epoch: 7 | Generator Loss: 15.435720 | Discriminator Loss: 0.016710\n","| Global Epoch: 84 | Local Epoch: 8 | Generator Loss: 15.430787 | Discriminator Loss: 0.016708\n","| Global Epoch: 84 | Local Epoch: 9 | Generator Loss: 15.341378 | Discriminator Loss: 0.021370\n","| Global Epoch: 84 | Local Epoch: 0 | Generator Loss: 15.278255 | Discriminator Loss: 0.214853\n","| Global Epoch: 84 | Local Epoch: 1 | Generator Loss: 14.971097 | Discriminator Loss: 0.118819\n","| Global Epoch: 84 | Local Epoch: 2 | Generator Loss: 14.963336 | Discriminator Loss: 0.082400\n","| Global Epoch: 84 | Local Epoch: 3 | Generator Loss: 15.013267 | Discriminator Loss: 0.062857\n","| Global Epoch: 84 | Local Epoch: 4 | Generator Loss: 15.102758 | Discriminator Loss: 0.056911\n","| Global Epoch: 84 | Local Epoch: 5 | Generator Loss: 15.173453 | Discriminator Loss: 0.051846\n","| Global Epoch: 84 | Local Epoch: 6 | Generator Loss: 15.310776 | Discriminator Loss: 0.045244\n","| Global Epoch: 84 | Local Epoch: 7 | Generator Loss: 15.415303 | Discriminator Loss: 0.039982\n","| Global Epoch: 84 | Local Epoch: 8 | Generator Loss: 15.407978 | Discriminator Loss: 0.035776\n","| Global Epoch: 84 | Local Epoch: 9 | Generator Loss: 15.434843 | Discriminator Loss: 0.032325\n","| Global Epoch: 84 | Local Epoch: 0 | Generator Loss: 15.805806 | Discriminator Loss: 0.066050\n","| Global Epoch: 84 | Local Epoch: 1 | Generator Loss: 16.124071 | Discriminator Loss: 0.038041\n","| Global Epoch: 84 | Local Epoch: 2 | Generator Loss: 16.066057 | Discriminator Loss: 0.026246\n","| Global Epoch: 84 | Local Epoch: 3 | Generator Loss: 16.061644 | Discriminator Loss: 0.020033\n","| Global Epoch: 84 | Local Epoch: 4 | Generator Loss: 16.057068 | Discriminator Loss: 0.016221\n","| Global Epoch: 84 | Local Epoch: 5 | Generator Loss: 16.048078 | Discriminator Loss: 0.013659\n","| Global Epoch: 84 | Local Epoch: 6 | Generator Loss: 16.070756 | Discriminator Loss: 0.011817\n","| Global Epoch: 84 | Local Epoch: 7 | Generator Loss: 16.075173 | Discriminator Loss: 0.010421\n","| Global Epoch: 84 | Local Epoch: 8 | Generator Loss: 16.076878 | Discriminator Loss: 0.009331\n","| Global Epoch: 84 | Local Epoch: 9 | Generator Loss: 16.060510 | Discriminator Loss: 0.008452\n","| Global Epoch: 84 | Local Epoch: 0 | Generator Loss: 15.764982 | Discriminator Loss: 0.087728\n","| Global Epoch: 84 | Local Epoch: 1 | Generator Loss: 15.727918 | Discriminator Loss: 0.052209\n","| Global Epoch: 84 | Local Epoch: 2 | Generator Loss: 15.629642 | Discriminator Loss: 0.036126\n","| Global Epoch: 84 | Local Epoch: 3 | Generator Loss: 15.629523 | Discriminator Loss: 0.027580\n","| Global Epoch: 84 | Local Epoch: 4 | Generator Loss: 15.668561 | Discriminator Loss: 0.022380\n","| Global Epoch: 84 | Local Epoch: 5 | Generator Loss: 15.751531 | Discriminator Loss: 0.018902\n","| Global Epoch: 84 | Local Epoch: 6 | Generator Loss: 15.890117 | Discriminator Loss: 0.016420\n","| Global Epoch: 84 | Local Epoch: 7 | Generator Loss: 15.944678 | Discriminator Loss: 0.014478\n","| Global Epoch: 84 | Local Epoch: 8 | Generator Loss: 16.006296 | Discriminator Loss: 0.012945\n","| Global Epoch: 84 | Local Epoch: 9 | Generator Loss: 16.067663 | Discriminator Loss: 0.011711\n","| Global Epoch: 84 | Local Epoch: 0 | Generator Loss: 16.155903 | Discriminator Loss: 0.052750\n","| Global Epoch: 84 | Local Epoch: 1 | Generator Loss: 15.829933 | Discriminator Loss: 0.036217\n","| Global Epoch: 84 | Local Epoch: 2 | Generator Loss: 15.568637 | Discriminator Loss: 0.034137\n","| Global Epoch: 84 | Local Epoch: 3 | Generator Loss: 15.517056 | Discriminator Loss: 0.026653\n","| Global Epoch: 84 | Local Epoch: 4 | Generator Loss: 15.586970 | Discriminator Loss: 0.022297\n","| Global Epoch: 84 | Local Epoch: 5 | Generator Loss: 15.614039 | Discriminator Loss: 0.019692\n","| Global Epoch: 84 | Local Epoch: 6 | Generator Loss: 15.462128 | Discriminator Loss: 0.026738\n","| Global Epoch: 84 | Local Epoch: 7 | Generator Loss: 15.519735 | Discriminator Loss: 0.024292\n","| Global Epoch: 84 | Local Epoch: 8 | Generator Loss: 15.527536 | Discriminator Loss: 0.021838\n","| Global Epoch: 84 | Local Epoch: 9 | Generator Loss: 15.529428 | Discriminator Loss: 0.019808\n","After 85 epoch global training, averged local generator loss is: 15.8379, averged local discrimitor loss is: 0.0616\n","Averged validation loss is: 6.2105\n","| Global Epoch: 85 | Local Epoch: 0 | Generator Loss: 15.199690 | Discriminator Loss: 0.150659\n","| Global Epoch: 85 | Local Epoch: 1 | Generator Loss: 15.128640 | Discriminator Loss: 0.085999\n","| Global Epoch: 85 | Local Epoch: 2 | Generator Loss: 14.976923 | Discriminator Loss: 0.058798\n","| Global Epoch: 85 | Local Epoch: 3 | Generator Loss: 14.889430 | Discriminator Loss: 0.044705\n","| Global Epoch: 85 | Local Epoch: 4 | Generator Loss: 14.895028 | Discriminator Loss: 0.036130\n","| Global Epoch: 85 | Local Epoch: 5 | Generator Loss: 14.923256 | Discriminator Loss: 0.030360\n","| Global Epoch: 85 | Local Epoch: 6 | Generator Loss: 14.922587 | Discriminator Loss: 0.026199\n","| Global Epoch: 85 | Local Epoch: 7 | Generator Loss: 14.906941 | Discriminator Loss: 0.023055\n","| Global Epoch: 85 | Local Epoch: 8 | Generator Loss: 15.035746 | Discriminator Loss: 0.020687\n","| Global Epoch: 85 | Local Epoch: 9 | Generator Loss: 15.179231 | Discriminator Loss: 0.018737\n","| Global Epoch: 85 | Local Epoch: 0 | Generator Loss: 16.137392 | Discriminator Loss: 0.197368\n","| Global Epoch: 85 | Local Epoch: 1 | Generator Loss: 15.704779 | Discriminator Loss: 0.108102\n","| Global Epoch: 85 | Local Epoch: 2 | Generator Loss: 15.602301 | Discriminator Loss: 0.074066\n","| Global Epoch: 85 | Local Epoch: 3 | Generator Loss: 15.417758 | Discriminator Loss: 0.056330\n","| Global Epoch: 85 | Local Epoch: 4 | Generator Loss: 15.409432 | Discriminator Loss: 0.045544\n","| Global Epoch: 85 | Local Epoch: 5 | Generator Loss: 15.497002 | Discriminator Loss: 0.038268\n","| Global Epoch: 85 | Local Epoch: 6 | Generator Loss: 15.544954 | Discriminator Loss: 0.033020\n","| Global Epoch: 85 | Local Epoch: 7 | Generator Loss: 15.549131 | Discriminator Loss: 0.029056\n","| Global Epoch: 85 | Local Epoch: 8 | Generator Loss: 15.575052 | Discriminator Loss: 0.025957\n","| Global Epoch: 85 | Local Epoch: 9 | Generator Loss: 15.619847 | Discriminator Loss: 0.023467\n","| Global Epoch: 85 | Local Epoch: 0 | Generator Loss: 15.373921 | Discriminator Loss: 0.081530\n","| Global Epoch: 85 | Local Epoch: 1 | Generator Loss: 15.067544 | Discriminator Loss: 0.059971\n","| Global Epoch: 85 | Local Epoch: 2 | Generator Loss: 15.148684 | Discriminator Loss: 0.041440\n","| Global Epoch: 85 | Local Epoch: 3 | Generator Loss: 15.174419 | Discriminator Loss: 0.032803\n","| Global Epoch: 85 | Local Epoch: 4 | Generator Loss: 15.183946 | Discriminator Loss: 0.036456\n","| Global Epoch: 85 | Local Epoch: 5 | Generator Loss: 15.189122 | Discriminator Loss: 0.031163\n","| Global Epoch: 85 | Local Epoch: 6 | Generator Loss: 15.228263 | Discriminator Loss: 0.026992\n","| Global Epoch: 85 | Local Epoch: 7 | Generator Loss: 15.267682 | Discriminator Loss: 0.023782\n","| Global Epoch: 85 | Local Epoch: 8 | Generator Loss: 15.285511 | Discriminator Loss: 0.021270\n","| Global Epoch: 85 | Local Epoch: 9 | Generator Loss: 15.294027 | Discriminator Loss: 0.032556\n","| Global Epoch: 85 | Local Epoch: 0 | Generator Loss: 14.989856 | Discriminator Loss: 0.095427\n","| Global Epoch: 85 | Local Epoch: 1 | Generator Loss: 15.051816 | Discriminator Loss: 0.053697\n","| Global Epoch: 85 | Local Epoch: 2 | Generator Loss: 15.304919 | Discriminator Loss: 0.037441\n","| Global Epoch: 85 | Local Epoch: 3 | Generator Loss: 15.500008 | Discriminator Loss: 0.028549\n","| Global Epoch: 85 | Local Epoch: 4 | Generator Loss: 15.647958 | Discriminator Loss: 0.023112\n","| Global Epoch: 85 | Local Epoch: 5 | Generator Loss: 15.815594 | Discriminator Loss: 0.019490\n","| Global Epoch: 85 | Local Epoch: 6 | Generator Loss: 15.868919 | Discriminator Loss: 0.016829\n","| Global Epoch: 85 | Local Epoch: 7 | Generator Loss: 15.940438 | Discriminator Loss: 0.014825\n","| Global Epoch: 85 | Local Epoch: 8 | Generator Loss: 15.989417 | Discriminator Loss: 0.013250\n","| Global Epoch: 85 | Local Epoch: 9 | Generator Loss: 15.910371 | Discriminator Loss: 0.024447\n","| Global Epoch: 85 | Local Epoch: 0 | Generator Loss: 15.220656 | Discriminator Loss: 0.214836\n","| Global Epoch: 85 | Local Epoch: 1 | Generator Loss: 14.890387 | Discriminator Loss: 0.117223\n","| Global Epoch: 85 | Local Epoch: 2 | Generator Loss: 14.777507 | Discriminator Loss: 0.080122\n","| Global Epoch: 85 | Local Epoch: 3 | Generator Loss: 14.982409 | Discriminator Loss: 0.061055\n","| Global Epoch: 85 | Local Epoch: 4 | Generator Loss: 15.093166 | Discriminator Loss: 0.049318\n","| Global Epoch: 85 | Local Epoch: 5 | Generator Loss: 15.111828 | Discriminator Loss: 0.041427\n","| Global Epoch: 85 | Local Epoch: 6 | Generator Loss: 15.091320 | Discriminator Loss: 0.035753\n","| Global Epoch: 85 | Local Epoch: 7 | Generator Loss: 15.117072 | Discriminator Loss: 0.032309\n","| Global Epoch: 85 | Local Epoch: 8 | Generator Loss: 15.194529 | Discriminator Loss: 0.028929\n","| Global Epoch: 85 | Local Epoch: 9 | Generator Loss: 15.207049 | Discriminator Loss: 0.026184\n","After 86 epoch global training, averged local generator loss is: 15.8305, averged local discrimitor loss is: 0.0612\n","Averged validation loss is: 4.6580\n","| Global Epoch: 86 | Local Epoch: 0 | Generator Loss: 15.746088 | Discriminator Loss: 0.142751\n","| Global Epoch: 86 | Local Epoch: 1 | Generator Loss: 15.707608 | Discriminator Loss: 0.080645\n","| Global Epoch: 86 | Local Epoch: 2 | Generator Loss: 15.567214 | Discriminator Loss: 0.055803\n","| Global Epoch: 86 | Local Epoch: 3 | Generator Loss: 15.523155 | Discriminator Loss: 0.042697\n","| Global Epoch: 86 | Local Epoch: 4 | Generator Loss: 15.639059 | Discriminator Loss: 0.034654\n","| Global Epoch: 86 | Local Epoch: 5 | Generator Loss: 15.660191 | Discriminator Loss: 0.029143\n","| Global Epoch: 86 | Local Epoch: 6 | Generator Loss: 15.785780 | Discriminator Loss: 0.025295\n","| Global Epoch: 86 | Local Epoch: 7 | Generator Loss: 15.858093 | Discriminator Loss: 0.022297\n","| Global Epoch: 86 | Local Epoch: 8 | Generator Loss: 15.897153 | Discriminator Loss: 0.019972\n","| Global Epoch: 86 | Local Epoch: 9 | Generator Loss: 16.019778 | Discriminator Loss: 0.018100\n","| Global Epoch: 86 | Local Epoch: 0 | Generator Loss: 15.984916 | Discriminator Loss: 0.043208\n","| Global Epoch: 86 | Local Epoch: 1 | Generator Loss: 15.842556 | Discriminator Loss: 0.025125\n","| Global Epoch: 86 | Local Epoch: 2 | Generator Loss: 15.740837 | Discriminator Loss: 0.017490\n","| Global Epoch: 86 | Local Epoch: 3 | Generator Loss: 15.754327 | Discriminator Loss: 0.013375\n","| Global Epoch: 86 | Local Epoch: 4 | Generator Loss: 15.782938 | Discriminator Loss: 0.010872\n","| Global Epoch: 86 | Local Epoch: 5 | Generator Loss: 15.912311 | Discriminator Loss: 0.009175\n","| Global Epoch: 86 | Local Epoch: 6 | Generator Loss: 15.975179 | Discriminator Loss: 0.007944\n","| Global Epoch: 86 | Local Epoch: 7 | Generator Loss: 16.018708 | Discriminator Loss: 0.007016\n","| Global Epoch: 86 | Local Epoch: 8 | Generator Loss: 16.058115 | Discriminator Loss: 0.006285\n","| Global Epoch: 86 | Local Epoch: 9 | Generator Loss: 16.116602 | Discriminator Loss: 0.005696\n","| Global Epoch: 86 | Local Epoch: 0 | Generator Loss: 16.274461 | Discriminator Loss: 0.083250\n","| Global Epoch: 86 | Local Epoch: 1 | Generator Loss: 15.998014 | Discriminator Loss: 0.049426\n","| Global Epoch: 86 | Local Epoch: 2 | Generator Loss: 15.771669 | Discriminator Loss: 0.034116\n","| Global Epoch: 86 | Local Epoch: 3 | Generator Loss: 15.709652 | Discriminator Loss: 0.026080\n","| Global Epoch: 86 | Local Epoch: 4 | Generator Loss: 15.645798 | Discriminator Loss: 0.021142\n","| Global Epoch: 86 | Local Epoch: 5 | Generator Loss: 15.756616 | Discriminator Loss: 0.018080\n","| Global Epoch: 86 | Local Epoch: 6 | Generator Loss: 15.769147 | Discriminator Loss: 0.015732\n","| Global Epoch: 86 | Local Epoch: 7 | Generator Loss: 15.875171 | Discriminator Loss: 0.013903\n","| Global Epoch: 86 | Local Epoch: 8 | Generator Loss: 15.917937 | Discriminator Loss: 0.012449\n","| Global Epoch: 86 | Local Epoch: 9 | Generator Loss: 15.972425 | Discriminator Loss: 0.011278\n","| Global Epoch: 86 | Local Epoch: 0 | Generator Loss: 16.194458 | Discriminator Loss: 0.095210\n","| Global Epoch: 86 | Local Epoch: 1 | Generator Loss: 15.939259 | Discriminator Loss: 0.062861\n","| Global Epoch: 86 | Local Epoch: 2 | Generator Loss: 15.489508 | Discriminator Loss: 0.043655\n","| Global Epoch: 86 | Local Epoch: 3 | Generator Loss: 15.405149 | Discriminator Loss: 0.033487\n","| Global Epoch: 86 | Local Epoch: 4 | Generator Loss: 15.406141 | Discriminator Loss: 0.027085\n","| Global Epoch: 86 | Local Epoch: 5 | Generator Loss: 15.472346 | Discriminator Loss: 0.022762\n","| Global Epoch: 86 | Local Epoch: 6 | Generator Loss: 15.515030 | Discriminator Loss: 0.019656\n","| Global Epoch: 86 | Local Epoch: 7 | Generator Loss: 15.631918 | Discriminator Loss: 0.017313\n","| Global Epoch: 86 | Local Epoch: 8 | Generator Loss: 15.683625 | Discriminator Loss: 0.015488\n","| Global Epoch: 86 | Local Epoch: 9 | Generator Loss: 15.741840 | Discriminator Loss: 0.014026\n","| Global Epoch: 86 | Local Epoch: 0 | Generator Loss: 16.166350 | Discriminator Loss: 0.038059\n","| Global Epoch: 86 | Local Epoch: 1 | Generator Loss: 15.865193 | Discriminator Loss: 0.027567\n","| Global Epoch: 86 | Local Epoch: 2 | Generator Loss: 15.580850 | Discriminator Loss: 0.019352\n","| Global Epoch: 86 | Local Epoch: 3 | Generator Loss: 15.541438 | Discriminator Loss: 0.014813\n","| Global Epoch: 86 | Local Epoch: 4 | Generator Loss: 15.517748 | Discriminator Loss: 0.012008\n","| Global Epoch: 86 | Local Epoch: 5 | Generator Loss: 15.525139 | Discriminator Loss: 0.010133\n","| Global Epoch: 86 | Local Epoch: 6 | Generator Loss: 15.545746 | Discriminator Loss: 0.008790\n","| Global Epoch: 86 | Local Epoch: 7 | Generator Loss: 15.588022 | Discriminator Loss: 0.007881\n","| Global Epoch: 86 | Local Epoch: 8 | Generator Loss: 15.639427 | Discriminator Loss: 0.007739\n","| Global Epoch: 86 | Local Epoch: 9 | Generator Loss: 15.567858 | Discriminator Loss: 0.011650\n","After 87 epoch global training, averged local generator loss is: 15.8275, averged local discrimitor loss is: 0.0606\n","Averged validation loss is: 4.4899\n","| Global Epoch: 87 | Local Epoch: 0 | Generator Loss: 15.709532 | Discriminator Loss: 0.095748\n","| Global Epoch: 87 | Local Epoch: 1 | Generator Loss: 15.727796 | Discriminator Loss: 0.053165\n","| Global Epoch: 87 | Local Epoch: 2 | Generator Loss: 15.686315 | Discriminator Loss: 0.036344\n","| Global Epoch: 87 | Local Epoch: 3 | Generator Loss: 15.690481 | Discriminator Loss: 0.027637\n","| Global Epoch: 87 | Local Epoch: 4 | Generator Loss: 15.813583 | Discriminator Loss: 0.022406\n","| Global Epoch: 87 | Local Epoch: 5 | Generator Loss: 15.880984 | Discriminator Loss: 0.018828\n","| Global Epoch: 87 | Local Epoch: 6 | Generator Loss: 15.897816 | Discriminator Loss: 0.016248\n","| Global Epoch: 87 | Local Epoch: 7 | Generator Loss: 15.865996 | Discriminator Loss: 0.014301\n","| Global Epoch: 87 | Local Epoch: 8 | Generator Loss: 15.878379 | Discriminator Loss: 0.012784\n","| Global Epoch: 87 | Local Epoch: 9 | Generator Loss: 15.923639 | Discriminator Loss: 0.011565\n","| Global Epoch: 87 | Local Epoch: 0 | Generator Loss: 15.842786 | Discriminator Loss: 0.037930\n","| Global Epoch: 87 | Local Epoch: 1 | Generator Loss: 15.690295 | Discriminator Loss: 0.022782\n","| Global Epoch: 87 | Local Epoch: 2 | Generator Loss: 15.786359 | Discriminator Loss: 0.015922\n","| Global Epoch: 87 | Local Epoch: 3 | Generator Loss: 15.833757 | Discriminator Loss: 0.012233\n","| Global Epoch: 87 | Local Epoch: 4 | Generator Loss: 15.852622 | Discriminator Loss: 0.010271\n","| Global Epoch: 87 | Local Epoch: 5 | Generator Loss: 15.913962 | Discriminator Loss: 0.008704\n","| Global Epoch: 87 | Local Epoch: 6 | Generator Loss: 15.956473 | Discriminator Loss: 0.007538\n","| Global Epoch: 87 | Local Epoch: 7 | Generator Loss: 16.031857 | Discriminator Loss: 0.006652\n","| Global Epoch: 87 | Local Epoch: 8 | Generator Loss: 16.076227 | Discriminator Loss: 0.005959\n","| Global Epoch: 87 | Local Epoch: 9 | Generator Loss: 16.124555 | Discriminator Loss: 0.005397\n","| Global Epoch: 87 | Local Epoch: 0 | Generator Loss: 16.180647 | Discriminator Loss: 0.064457\n","| Global Epoch: 87 | Local Epoch: 1 | Generator Loss: 16.007090 | Discriminator Loss: 0.038352\n","| Global Epoch: 87 | Local Epoch: 2 | Generator Loss: 15.999867 | Discriminator Loss: 0.026767\n","| Global Epoch: 87 | Local Epoch: 3 | Generator Loss: 15.964223 | Discriminator Loss: 0.020441\n","| Global Epoch: 87 | Local Epoch: 4 | Generator Loss: 16.078004 | Discriminator Loss: 0.016595\n","| Global Epoch: 87 | Local Epoch: 5 | Generator Loss: 16.157105 | Discriminator Loss: 0.013972\n","| Global Epoch: 87 | Local Epoch: 6 | Generator Loss: 16.235802 | Discriminator Loss: 0.012073\n","| Global Epoch: 87 | Local Epoch: 7 | Generator Loss: 16.305781 | Discriminator Loss: 0.010647\n","| Global Epoch: 87 | Local Epoch: 8 | Generator Loss: 16.398816 | Discriminator Loss: 0.009542\n","| Global Epoch: 87 | Local Epoch: 9 | Generator Loss: 16.485174 | Discriminator Loss: 0.008647\n","| Global Epoch: 87 | Local Epoch: 0 | Generator Loss: 16.820138 | Discriminator Loss: 0.076250\n","| Global Epoch: 87 | Local Epoch: 1 | Generator Loss: 16.495405 | Discriminator Loss: 0.048001\n","| Global Epoch: 87 | Local Epoch: 2 | Generator Loss: 16.442726 | Discriminator Loss: 0.033859\n","| Global Epoch: 87 | Local Epoch: 3 | Generator Loss: 16.421430 | Discriminator Loss: 0.025887\n","| Global Epoch: 87 | Local Epoch: 4 | Generator Loss: 16.483202 | Discriminator Loss: 0.020971\n","| Global Epoch: 87 | Local Epoch: 5 | Generator Loss: 16.451507 | Discriminator Loss: 0.017629\n","| Global Epoch: 87 | Local Epoch: 6 | Generator Loss: 16.407117 | Discriminator Loss: 0.015228\n","| Global Epoch: 87 | Local Epoch: 7 | Generator Loss: 16.372497 | Discriminator Loss: 0.013408\n","| Global Epoch: 87 | Local Epoch: 8 | Generator Loss: 16.320318 | Discriminator Loss: 0.011992\n","| Global Epoch: 87 | Local Epoch: 9 | Generator Loss: 16.283192 | Discriminator Loss: 0.010875\n","| Global Epoch: 87 | Local Epoch: 0 | Generator Loss: 15.855282 | Discriminator Loss: 0.035017\n","| Global Epoch: 87 | Local Epoch: 1 | Generator Loss: 15.623692 | Discriminator Loss: 0.021146\n","| Global Epoch: 87 | Local Epoch: 2 | Generator Loss: 15.491478 | Discriminator Loss: 0.014669\n","| Global Epoch: 87 | Local Epoch: 3 | Generator Loss: 15.478618 | Discriminator Loss: 0.011412\n","| Global Epoch: 87 | Local Epoch: 4 | Generator Loss: 15.504312 | Discriminator Loss: 0.009708\n","| Global Epoch: 87 | Local Epoch: 5 | Generator Loss: 15.431085 | Discriminator Loss: 0.008259\n","| Global Epoch: 87 | Local Epoch: 6 | Generator Loss: 15.487600 | Discriminator Loss: 0.007170\n","| Global Epoch: 87 | Local Epoch: 7 | Generator Loss: 15.487171 | Discriminator Loss: 0.007622\n","| Global Epoch: 87 | Local Epoch: 8 | Generator Loss: 15.450098 | Discriminator Loss: 0.011493\n","| Global Epoch: 87 | Local Epoch: 9 | Generator Loss: 15.382597 | Discriminator Loss: 0.019759\n","After 88 epoch global training, averged local generator loss is: 15.8225, averged local discrimitor loss is: 0.0601\n","Averged validation loss is: 4.9493\n","| Global Epoch: 88 | Local Epoch: 0 | Generator Loss: 14.519243 | Discriminator Loss: 0.174288\n","| Global Epoch: 88 | Local Epoch: 1 | Generator Loss: 14.216406 | Discriminator Loss: 0.098440\n","| Global Epoch: 88 | Local Epoch: 2 | Generator Loss: 14.383908 | Discriminator Loss: 0.067317\n","| Global Epoch: 88 | Local Epoch: 3 | Generator Loss: 14.570265 | Discriminator Loss: 0.051136\n","| Global Epoch: 88 | Local Epoch: 4 | Generator Loss: 14.648425 | Discriminator Loss: 0.041284\n","| Global Epoch: 88 | Local Epoch: 5 | Generator Loss: 14.690950 | Discriminator Loss: 0.034644\n","| Global Epoch: 88 | Local Epoch: 6 | Generator Loss: 14.740206 | Discriminator Loss: 0.029862\n","| Global Epoch: 88 | Local Epoch: 7 | Generator Loss: 14.749132 | Discriminator Loss: 0.026253\n","| Global Epoch: 88 | Local Epoch: 8 | Generator Loss: 14.876278 | Discriminator Loss: 0.023501\n","| Global Epoch: 88 | Local Epoch: 9 | Generator Loss: 14.980161 | Discriminator Loss: 0.021253\n","| Global Epoch: 88 | Local Epoch: 0 | Generator Loss: 14.940805 | Discriminator Loss: 0.107188\n","| Global Epoch: 88 | Local Epoch: 1 | Generator Loss: 14.706977 | Discriminator Loss: 0.067803\n","| Global Epoch: 88 | Local Epoch: 2 | Generator Loss: 14.539948 | Discriminator Loss: 0.053408\n","| Global Epoch: 88 | Local Epoch: 3 | Generator Loss: 14.413981 | Discriminator Loss: 0.059096\n","| Global Epoch: 88 | Local Epoch: 4 | Generator Loss: 14.428322 | Discriminator Loss: 0.048347\n","| Global Epoch: 88 | Local Epoch: 5 | Generator Loss: 14.436408 | Discriminator Loss: 0.040730\n","| Global Epoch: 88 | Local Epoch: 6 | Generator Loss: 14.497919 | Discriminator Loss: 0.035183\n","| Global Epoch: 88 | Local Epoch: 7 | Generator Loss: 14.611268 | Discriminator Loss: 0.031031\n","| Global Epoch: 88 | Local Epoch: 8 | Generator Loss: 14.701688 | Discriminator Loss: 0.027752\n","| Global Epoch: 88 | Local Epoch: 9 | Generator Loss: 14.762605 | Discriminator Loss: 0.025104\n","| Global Epoch: 88 | Local Epoch: 0 | Generator Loss: 14.635408 | Discriminator Loss: 0.072788\n","| Global Epoch: 88 | Local Epoch: 1 | Generator Loss: 14.579699 | Discriminator Loss: 0.042911\n","| Global Epoch: 88 | Local Epoch: 2 | Generator Loss: 14.594374 | Discriminator Loss: 0.036721\n","| Global Epoch: 88 | Local Epoch: 3 | Generator Loss: 14.426433 | Discriminator Loss: 0.036358\n","| Global Epoch: 88 | Local Epoch: 4 | Generator Loss: 14.494508 | Discriminator Loss: 0.030688\n","| Global Epoch: 88 | Local Epoch: 5 | Generator Loss: 14.465326 | Discriminator Loss: 0.034030\n","| Global Epoch: 88 | Local Epoch: 6 | Generator Loss: 14.591228 | Discriminator Loss: 0.030601\n","| Global Epoch: 88 | Local Epoch: 7 | Generator Loss: 14.558597 | Discriminator Loss: 0.037322\n","| Global Epoch: 88 | Local Epoch: 8 | Generator Loss: 14.573690 | Discriminator Loss: 0.034395\n","| Global Epoch: 88 | Local Epoch: 9 | Generator Loss: 14.625973 | Discriminator Loss: 0.031220\n","| Global Epoch: 88 | Local Epoch: 0 | Generator Loss: 15.215513 | Discriminator Loss: 0.266436\n","| Global Epoch: 88 | Local Epoch: 1 | Generator Loss: 14.931392 | Discriminator Loss: 0.146248\n","| Global Epoch: 88 | Local Epoch: 2 | Generator Loss: 14.912190 | Discriminator Loss: 0.099963\n","| Global Epoch: 88 | Local Epoch: 3 | Generator Loss: 14.792100 | Discriminator Loss: 0.076021\n","| Global Epoch: 88 | Local Epoch: 4 | Generator Loss: 14.734230 | Discriminator Loss: 0.061456\n","| Global Epoch: 88 | Local Epoch: 5 | Generator Loss: 14.801088 | Discriminator Loss: 0.051641\n","| Global Epoch: 88 | Local Epoch: 6 | Generator Loss: 14.883880 | Discriminator Loss: 0.044654\n","| Global Epoch: 88 | Local Epoch: 7 | Generator Loss: 14.924263 | Discriminator Loss: 0.039307\n","| Global Epoch: 88 | Local Epoch: 8 | Generator Loss: 15.014950 | Discriminator Loss: 0.035345\n","| Global Epoch: 88 | Local Epoch: 9 | Generator Loss: 15.073331 | Discriminator Loss: 0.032048\n","| Global Epoch: 88 | Local Epoch: 0 | Generator Loss: 15.093924 | Discriminator Loss: 0.130078\n","| Global Epoch: 88 | Local Epoch: 1 | Generator Loss: 14.822853 | Discriminator Loss: 0.073932\n","| Global Epoch: 88 | Local Epoch: 2 | Generator Loss: 14.781399 | Discriminator Loss: 0.051667\n","| Global Epoch: 88 | Local Epoch: 3 | Generator Loss: 14.812611 | Discriminator Loss: 0.040912\n","| Global Epoch: 88 | Local Epoch: 4 | Generator Loss: 14.786260 | Discriminator Loss: 0.033925\n","| Global Epoch: 88 | Local Epoch: 5 | Generator Loss: 14.790537 | Discriminator Loss: 0.047761\n","| Global Epoch: 88 | Local Epoch: 6 | Generator Loss: 14.559866 | Discriminator Loss: 0.064520\n","| Global Epoch: 88 | Local Epoch: 7 | Generator Loss: 14.505268 | Discriminator Loss: 0.062223\n","| Global Epoch: 88 | Local Epoch: 8 | Generator Loss: 14.356472 | Discriminator Loss: 0.064099\n","| Global Epoch: 88 | Local Epoch: 9 | Generator Loss: 14.269789 | Discriminator Loss: 0.064329\n","After 89 epoch global training, averged local generator loss is: 15.8050, averged local discrimitor loss is: 0.0602\n","Averged validation loss is: 5.4254\n","| Global Epoch: 89 | Local Epoch: 0 | Generator Loss: 13.224679 | Discriminator Loss: 0.414977\n","| Global Epoch: 89 | Local Epoch: 1 | Generator Loss: 13.187097 | Discriminator Loss: 0.229830\n","| Global Epoch: 89 | Local Epoch: 2 | Generator Loss: 13.320466 | Discriminator Loss: 0.157738\n","| Global Epoch: 89 | Local Epoch: 3 | Generator Loss: 13.400745 | Discriminator Loss: 0.120108\n","| Global Epoch: 89 | Local Epoch: 4 | Generator Loss: 13.565477 | Discriminator Loss: 0.097435\n","| Global Epoch: 89 | Local Epoch: 5 | Generator Loss: 13.740438 | Discriminator Loss: 0.081955\n","| Global Epoch: 89 | Local Epoch: 6 | Generator Loss: 13.782977 | Discriminator Loss: 0.072730\n","| Global Epoch: 89 | Local Epoch: 7 | Generator Loss: 13.895720 | Discriminator Loss: 0.064808\n","| Global Epoch: 89 | Local Epoch: 8 | Generator Loss: 13.968161 | Discriminator Loss: 0.058005\n","| Global Epoch: 89 | Local Epoch: 9 | Generator Loss: 13.971464 | Discriminator Loss: 0.055864\n","| Global Epoch: 89 | Local Epoch: 0 | Generator Loss: 14.458297 | Discriminator Loss: 0.180344\n","| Global Epoch: 89 | Local Epoch: 1 | Generator Loss: 14.568982 | Discriminator Loss: 0.098451\n","| Global Epoch: 89 | Local Epoch: 2 | Generator Loss: 14.604875 | Discriminator Loss: 0.067583\n","| Global Epoch: 89 | Local Epoch: 3 | Generator Loss: 14.828896 | Discriminator Loss: 0.051487\n","| Global Epoch: 89 | Local Epoch: 4 | Generator Loss: 14.957252 | Discriminator Loss: 0.041631\n","| Global Epoch: 89 | Local Epoch: 5 | Generator Loss: 14.983789 | Discriminator Loss: 0.034996\n","| Global Epoch: 89 | Local Epoch: 6 | Generator Loss: 15.083221 | Discriminator Loss: 0.030263\n","| Global Epoch: 89 | Local Epoch: 7 | Generator Loss: 15.214504 | Discriminator Loss: 0.026666\n","| Global Epoch: 89 | Local Epoch: 8 | Generator Loss: 15.264409 | Discriminator Loss: 0.023847\n","| Global Epoch: 89 | Local Epoch: 9 | Generator Loss: 15.253948 | Discriminator Loss: 0.021566\n","| Global Epoch: 89 | Local Epoch: 0 | Generator Loss: 15.199802 | Discriminator Loss: 0.131716\n","| Global Epoch: 89 | Local Epoch: 1 | Generator Loss: 15.080436 | Discriminator Loss: 0.074658\n","| Global Epoch: 89 | Local Epoch: 2 | Generator Loss: 15.324113 | Discriminator Loss: 0.051545\n","| Global Epoch: 89 | Local Epoch: 3 | Generator Loss: 15.369838 | Discriminator Loss: 0.039349\n","| Global Epoch: 89 | Local Epoch: 4 | Generator Loss: 15.367159 | Discriminator Loss: 0.031882\n","| Global Epoch: 89 | Local Epoch: 5 | Generator Loss: 15.428338 | Discriminator Loss: 0.026846\n","| Global Epoch: 89 | Local Epoch: 6 | Generator Loss: 15.455149 | Discriminator Loss: 0.023201\n","| Global Epoch: 89 | Local Epoch: 7 | Generator Loss: 15.518514 | Discriminator Loss: 0.020449\n","| Global Epoch: 89 | Local Epoch: 8 | Generator Loss: 15.561477 | Discriminator Loss: 0.018284\n","| Global Epoch: 89 | Local Epoch: 9 | Generator Loss: 15.577680 | Discriminator Loss: 0.016553\n","| Global Epoch: 89 | Local Epoch: 0 | Generator Loss: 15.305806 | Discriminator Loss: 0.141030\n","| Global Epoch: 89 | Local Epoch: 1 | Generator Loss: 14.595890 | Discriminator Loss: 0.088235\n","| Global Epoch: 89 | Local Epoch: 2 | Generator Loss: 14.655289 | Discriminator Loss: 0.061286\n","| Global Epoch: 89 | Local Epoch: 3 | Generator Loss: 15.013914 | Discriminator Loss: 0.050308\n","| Global Epoch: 89 | Local Epoch: 4 | Generator Loss: 15.173652 | Discriminator Loss: 0.040660\n","| Global Epoch: 89 | Local Epoch: 5 | Generator Loss: 15.283228 | Discriminator Loss: 0.034242\n","| Global Epoch: 89 | Local Epoch: 6 | Generator Loss: 15.091025 | Discriminator Loss: 0.030044\n","| Global Epoch: 89 | Local Epoch: 7 | Generator Loss: 15.095344 | Discriminator Loss: 0.029116\n","| Global Epoch: 89 | Local Epoch: 8 | Generator Loss: 15.116470 | Discriminator Loss: 0.026285\n","| Global Epoch: 89 | Local Epoch: 9 | Generator Loss: 15.180677 | Discriminator Loss: 0.023886\n","| Global Epoch: 89 | Local Epoch: 0 | Generator Loss: 15.362043 | Discriminator Loss: 0.661885\n","| Global Epoch: 89 | Local Epoch: 1 | Generator Loss: 16.006205 | Discriminator Loss: 0.367406\n","| Global Epoch: 89 | Local Epoch: 2 | Generator Loss: 15.898797 | Discriminator Loss: 0.248200\n","| Global Epoch: 89 | Local Epoch: 3 | Generator Loss: 16.024334 | Discriminator Loss: 0.187479\n","| Global Epoch: 89 | Local Epoch: 4 | Generator Loss: 15.923865 | Discriminator Loss: 0.150791\n","| Global Epoch: 89 | Local Epoch: 5 | Generator Loss: 15.900927 | Discriminator Loss: 0.126068\n","| Global Epoch: 89 | Local Epoch: 6 | Generator Loss: 15.907372 | Discriminator Loss: 0.108300\n","| Global Epoch: 89 | Local Epoch: 7 | Generator Loss: 15.920962 | Discriminator Loss: 0.094890\n","| Global Epoch: 89 | Local Epoch: 8 | Generator Loss: 15.885042 | Discriminator Loss: 0.084436\n","| Global Epoch: 89 | Local Epoch: 9 | Generator Loss: 15.872153 | Discriminator Loss: 0.076181\n","After 90 epoch global training, averged local generator loss is: 15.8058, averged local discrimitor loss is: 0.0604\n","Averged validation loss is: 5.9233\n","| Global Epoch: 90 | Local Epoch: 0 | Generator Loss: 14.472551 | Discriminator Loss: 0.157396\n","| Global Epoch: 90 | Local Epoch: 1 | Generator Loss: 14.849617 | Discriminator Loss: 0.087108\n","| Global Epoch: 90 | Local Epoch: 2 | Generator Loss: 15.161041 | Discriminator Loss: 0.059786\n","| Global Epoch: 90 | Local Epoch: 3 | Generator Loss: 15.361133 | Discriminator Loss: 0.045524\n","| Global Epoch: 90 | Local Epoch: 4 | Generator Loss: 15.594474 | Discriminator Loss: 0.036882\n","| Global Epoch: 90 | Local Epoch: 5 | Generator Loss: 15.834644 | Discriminator Loss: 0.031023\n","| Global Epoch: 90 | Local Epoch: 6 | Generator Loss: 15.917850 | Discriminator Loss: 0.026764\n","| Global Epoch: 90 | Local Epoch: 7 | Generator Loss: 16.029013 | Discriminator Loss: 0.023561\n","| Global Epoch: 90 | Local Epoch: 8 | Generator Loss: 16.096404 | Discriminator Loss: 0.021046\n","| Global Epoch: 90 | Local Epoch: 9 | Generator Loss: 16.175254 | Discriminator Loss: 0.019029\n","| Global Epoch: 90 | Local Epoch: 0 | Generator Loss: 16.830670 | Discriminator Loss: 0.028061\n","| Global Epoch: 90 | Local Epoch: 1 | Generator Loss: 16.622269 | Discriminator Loss: 0.015463\n","| Global Epoch: 90 | Local Epoch: 2 | Generator Loss: 16.292764 | Discriminator Loss: 0.010736\n","| Global Epoch: 90 | Local Epoch: 3 | Generator Loss: 16.240256 | Discriminator Loss: 0.008414\n","| Global Epoch: 90 | Local Epoch: 4 | Generator Loss: 16.175836 | Discriminator Loss: 0.006840\n","| Global Epoch: 90 | Local Epoch: 5 | Generator Loss: 16.135591 | Discriminator Loss: 0.005950\n","| Global Epoch: 90 | Local Epoch: 6 | Generator Loss: 16.141286 | Discriminator Loss: 0.005154\n","| Global Epoch: 90 | Local Epoch: 7 | Generator Loss: 16.238984 | Discriminator Loss: 0.004587\n","| Global Epoch: 90 | Local Epoch: 8 | Generator Loss: 16.338701 | Discriminator Loss: 0.004116\n","| Global Epoch: 90 | Local Epoch: 9 | Generator Loss: 16.416566 | Discriminator Loss: 0.003737\n","| Global Epoch: 90 | Local Epoch: 0 | Generator Loss: 16.145335 | Discriminator Loss: 0.056959\n","| Global Epoch: 90 | Local Epoch: 1 | Generator Loss: 15.654477 | Discriminator Loss: 0.035438\n","| Global Epoch: 90 | Local Epoch: 2 | Generator Loss: 16.053776 | Discriminator Loss: 0.025054\n","| Global Epoch: 90 | Local Epoch: 3 | Generator Loss: 16.113419 | Discriminator Loss: 0.019143\n","| Global Epoch: 90 | Local Epoch: 4 | Generator Loss: 16.174917 | Discriminator Loss: 0.015509\n","| Global Epoch: 90 | Local Epoch: 5 | Generator Loss: 16.261272 | Discriminator Loss: 0.013055\n","| Global Epoch: 90 | Local Epoch: 6 | Generator Loss: 16.291225 | Discriminator Loss: 0.011354\n","| Global Epoch: 90 | Local Epoch: 7 | Generator Loss: 16.221464 | Discriminator Loss: 0.010388\n","| Global Epoch: 90 | Local Epoch: 8 | Generator Loss: 16.242725 | Discriminator Loss: 0.009853\n","| Global Epoch: 90 | Local Epoch: 9 | Generator Loss: 16.283472 | Discriminator Loss: 0.010078\n","| Global Epoch: 90 | Local Epoch: 0 | Generator Loss: 16.231673 | Discriminator Loss: 0.091743\n","| Global Epoch: 90 | Local Epoch: 1 | Generator Loss: 16.173416 | Discriminator Loss: 0.051707\n","| Global Epoch: 90 | Local Epoch: 2 | Generator Loss: 16.040775 | Discriminator Loss: 0.035420\n","| Global Epoch: 90 | Local Epoch: 3 | Generator Loss: 16.064170 | Discriminator Loss: 0.027017\n","| Global Epoch: 90 | Local Epoch: 4 | Generator Loss: 16.181116 | Discriminator Loss: 0.021920\n","| Global Epoch: 90 | Local Epoch: 5 | Generator Loss: 16.282091 | Discriminator Loss: 0.018467\n","| Global Epoch: 90 | Local Epoch: 6 | Generator Loss: 16.281885 | Discriminator Loss: 0.015956\n","| Global Epoch: 90 | Local Epoch: 7 | Generator Loss: 16.282342 | Discriminator Loss: 0.014060\n","| Global Epoch: 90 | Local Epoch: 8 | Generator Loss: 16.293951 | Discriminator Loss: 0.012582\n","| Global Epoch: 90 | Local Epoch: 9 | Generator Loss: 16.322566 | Discriminator Loss: 0.011387\n","| Global Epoch: 90 | Local Epoch: 0 | Generator Loss: 16.136112 | Discriminator Loss: 0.075547\n","| Global Epoch: 90 | Local Epoch: 1 | Generator Loss: 15.932620 | Discriminator Loss: 0.044022\n","| Global Epoch: 90 | Local Epoch: 2 | Generator Loss: 15.887493 | Discriminator Loss: 0.030398\n","| Global Epoch: 90 | Local Epoch: 3 | Generator Loss: 15.890924 | Discriminator Loss: 0.023199\n","| Global Epoch: 90 | Local Epoch: 4 | Generator Loss: 15.949642 | Discriminator Loss: 0.018782\n","| Global Epoch: 90 | Local Epoch: 5 | Generator Loss: 16.025529 | Discriminator Loss: 0.015844\n","| Global Epoch: 90 | Local Epoch: 6 | Generator Loss: 15.773300 | Discriminator Loss: 0.029570\n","| Global Epoch: 90 | Local Epoch: 7 | Generator Loss: 15.631025 | Discriminator Loss: 0.034022\n","| Global Epoch: 90 | Local Epoch: 8 | Generator Loss: 15.522891 | Discriminator Loss: 0.031132\n","| Global Epoch: 90 | Local Epoch: 9 | Generator Loss: 15.472397 | Discriminator Loss: 0.029239\n","After 91 epoch global training, averged local generator loss is: 15.8021, averged local discrimitor loss is: 0.0600\n","Averged validation loss is: 4.8636\n","| Global Epoch: 91 | Local Epoch: 0 | Generator Loss: 14.751386 | Discriminator Loss: 0.100684\n","| Global Epoch: 91 | Local Epoch: 1 | Generator Loss: 15.121235 | Discriminator Loss: 0.059043\n","| Global Epoch: 91 | Local Epoch: 2 | Generator Loss: 15.230023 | Discriminator Loss: 0.040880\n","| Global Epoch: 91 | Local Epoch: 3 | Generator Loss: 15.164136 | Discriminator Loss: 0.031152\n","| Global Epoch: 91 | Local Epoch: 4 | Generator Loss: 15.165551 | Discriminator Loss: 0.025218\n","| Global Epoch: 91 | Local Epoch: 5 | Generator Loss: 15.231200 | Discriminator Loss: 0.021214\n","| Global Epoch: 91 | Local Epoch: 6 | Generator Loss: 15.285721 | Discriminator Loss: 0.018327\n","| Global Epoch: 91 | Local Epoch: 7 | Generator Loss: 15.337453 | Discriminator Loss: 0.016151\n","| Global Epoch: 91 | Local Epoch: 8 | Generator Loss: 15.391848 | Discriminator Loss: 0.014459\n","| Global Epoch: 91 | Local Epoch: 9 | Generator Loss: 15.488703 | Discriminator Loss: 0.013091\n","| Global Epoch: 91 | Local Epoch: 0 | Generator Loss: 15.677832 | Discriminator Loss: 0.144763\n","| Global Epoch: 91 | Local Epoch: 1 | Generator Loss: 15.403959 | Discriminator Loss: 0.080681\n","| Global Epoch: 91 | Local Epoch: 2 | Generator Loss: 15.331949 | Discriminator Loss: 0.055513\n","| Global Epoch: 91 | Local Epoch: 3 | Generator Loss: 15.323671 | Discriminator Loss: 0.042398\n","| Global Epoch: 91 | Local Epoch: 4 | Generator Loss: 15.437639 | Discriminator Loss: 0.034358\n","| Global Epoch: 91 | Local Epoch: 5 | Generator Loss: 15.508454 | Discriminator Loss: 0.028907\n","| Global Epoch: 91 | Local Epoch: 6 | Generator Loss: 15.564099 | Discriminator Loss: 0.024974\n","| Global Epoch: 91 | Local Epoch: 7 | Generator Loss: 15.701064 | Discriminator Loss: 0.022082\n","| Global Epoch: 91 | Local Epoch: 8 | Generator Loss: 15.794122 | Discriminator Loss: 0.019778\n","| Global Epoch: 91 | Local Epoch: 9 | Generator Loss: 15.896161 | Discriminator Loss: 0.017906\n","| Global Epoch: 91 | Local Epoch: 0 | Generator Loss: 15.920897 | Discriminator Loss: 0.034177\n","| Global Epoch: 91 | Local Epoch: 1 | Generator Loss: 15.758812 | Discriminator Loss: 0.024294\n","| Global Epoch: 91 | Local Epoch: 2 | Generator Loss: 15.994008 | Discriminator Loss: 0.020183\n","| Global Epoch: 91 | Local Epoch: 3 | Generator Loss: 15.803118 | Discriminator Loss: 0.034501\n","| Global Epoch: 91 | Local Epoch: 4 | Generator Loss: 15.764183 | Discriminator Loss: 0.030105\n","| Global Epoch: 91 | Local Epoch: 5 | Generator Loss: 15.559805 | Discriminator Loss: 0.032671\n","| Global Epoch: 91 | Local Epoch: 6 | Generator Loss: 15.492469 | Discriminator Loss: 0.028524\n","| Global Epoch: 91 | Local Epoch: 7 | Generator Loss: 15.464468 | Discriminator Loss: 0.025159\n","| Global Epoch: 91 | Local Epoch: 8 | Generator Loss: 15.456986 | Discriminator Loss: 0.022496\n","| Global Epoch: 91 | Local Epoch: 9 | Generator Loss: 15.460697 | Discriminator Loss: 0.020338\n","| Global Epoch: 91 | Local Epoch: 0 | Generator Loss: 15.380679 | Discriminator Loss: 0.163129\n","| Global Epoch: 91 | Local Epoch: 1 | Generator Loss: 15.348460 | Discriminator Loss: 0.094330\n","| Global Epoch: 91 | Local Epoch: 2 | Generator Loss: 15.241002 | Discriminator Loss: 0.064741\n","| Global Epoch: 91 | Local Epoch: 3 | Generator Loss: 15.465092 | Discriminator Loss: 0.050227\n","| Global Epoch: 91 | Local Epoch: 4 | Generator Loss: 15.165815 | Discriminator Loss: 0.042590\n","| Global Epoch: 91 | Local Epoch: 5 | Generator Loss: 15.193367 | Discriminator Loss: 0.036035\n","| Global Epoch: 91 | Local Epoch: 6 | Generator Loss: 15.202918 | Discriminator Loss: 0.031157\n","| Global Epoch: 91 | Local Epoch: 7 | Generator Loss: 15.235602 | Discriminator Loss: 0.027430\n","| Global Epoch: 91 | Local Epoch: 8 | Generator Loss: 15.266616 | Discriminator Loss: 0.024528\n","| Global Epoch: 91 | Local Epoch: 9 | Generator Loss: 15.316567 | Discriminator Loss: 0.022185\n","| Global Epoch: 91 | Local Epoch: 0 | Generator Loss: 15.262390 | Discriminator Loss: 0.131138\n","| Global Epoch: 91 | Local Epoch: 1 | Generator Loss: 15.591543 | Discriminator Loss: 0.069119\n","| Global Epoch: 91 | Local Epoch: 2 | Generator Loss: 15.724138 | Discriminator Loss: 0.047972\n","| Global Epoch: 91 | Local Epoch: 3 | Generator Loss: 15.682596 | Discriminator Loss: 0.036634\n","| Global Epoch: 91 | Local Epoch: 4 | Generator Loss: 15.727590 | Discriminator Loss: 0.029631\n","| Global Epoch: 91 | Local Epoch: 5 | Generator Loss: 15.655487 | Discriminator Loss: 0.025013\n","| Global Epoch: 91 | Local Epoch: 6 | Generator Loss: 15.697142 | Discriminator Loss: 0.021550\n","| Global Epoch: 91 | Local Epoch: 7 | Generator Loss: 15.707204 | Discriminator Loss: 0.018948\n","| Global Epoch: 91 | Local Epoch: 8 | Generator Loss: 15.683343 | Discriminator Loss: 0.016919\n","| Global Epoch: 91 | Local Epoch: 9 | Generator Loss: 15.721713 | Discriminator Loss: 0.015285\n","After 92 epoch global training, averged local generator loss is: 15.8012, averged local discrimitor loss is: 0.0595\n","Averged validation loss is: 6.9648\n","| Global Epoch: 92 | Local Epoch: 0 | Generator Loss: 15.565459 | Discriminator Loss: 0.000516\n","| Global Epoch: 92 | Local Epoch: 1 | Generator Loss: 15.909354 | Discriminator Loss: 0.000641\n","| Global Epoch: 92 | Local Epoch: 2 | Generator Loss: 16.107685 | Discriminator Loss: 0.000590\n","| Global Epoch: 92 | Local Epoch: 3 | Generator Loss: 16.226263 | Discriminator Loss: 0.000466\n","| Global Epoch: 92 | Local Epoch: 4 | Generator Loss: 16.114618 | Discriminator Loss: 0.000392\n","| Global Epoch: 92 | Local Epoch: 5 | Generator Loss: 16.035558 | Discriminator Loss: 0.000336\n","| Global Epoch: 92 | Local Epoch: 6 | Generator Loss: 15.963293 | Discriminator Loss: 0.000297\n","| Global Epoch: 92 | Local Epoch: 7 | Generator Loss: 15.916687 | Discriminator Loss: 0.000265\n","| Global Epoch: 92 | Local Epoch: 8 | Generator Loss: 15.865719 | Discriminator Loss: 0.000243\n","| Global Epoch: 92 | Local Epoch: 9 | Generator Loss: 15.856493 | Discriminator Loss: 0.000222\n","| Global Epoch: 92 | Local Epoch: 0 | Generator Loss: 15.152329 | Discriminator Loss: 0.152419\n","| Global Epoch: 92 | Local Epoch: 1 | Generator Loss: 13.963813 | Discriminator Loss: 0.130655\n","| Global Epoch: 92 | Local Epoch: 2 | Generator Loss: 13.845904 | Discriminator Loss: 0.108039\n","| Global Epoch: 92 | Local Epoch: 3 | Generator Loss: 13.529282 | Discriminator Loss: 0.086026\n","| Global Epoch: 92 | Local Epoch: 4 | Generator Loss: 13.513617 | Discriminator Loss: 0.069917\n","| Global Epoch: 92 | Local Epoch: 5 | Generator Loss: 13.524166 | Discriminator Loss: 0.059343\n","| Global Epoch: 92 | Local Epoch: 6 | Generator Loss: 13.702801 | Discriminator Loss: 0.051931\n","| Global Epoch: 92 | Local Epoch: 7 | Generator Loss: 13.794366 | Discriminator Loss: 0.045740\n","| Global Epoch: 92 | Local Epoch: 8 | Generator Loss: 13.862291 | Discriminator Loss: 0.041031\n","| Global Epoch: 92 | Local Epoch: 9 | Generator Loss: 13.932994 | Discriminator Loss: 0.037708\n","| Global Epoch: 92 | Local Epoch: 0 | Generator Loss: 14.465940 | Discriminator Loss: 0.173542\n","| Global Epoch: 92 | Local Epoch: 1 | Generator Loss: 14.294606 | Discriminator Loss: 0.094709\n","| Global Epoch: 92 | Local Epoch: 2 | Generator Loss: 14.141927 | Discriminator Loss: 0.064776\n","| Global Epoch: 92 | Local Epoch: 3 | Generator Loss: 14.159685 | Discriminator Loss: 0.049321\n","| Global Epoch: 92 | Local Epoch: 4 | Generator Loss: 14.111080 | Discriminator Loss: 0.039893\n","| Global Epoch: 92 | Local Epoch: 5 | Generator Loss: 14.085853 | Discriminator Loss: 0.033548\n","| Global Epoch: 92 | Local Epoch: 6 | Generator Loss: 14.110871 | Discriminator Loss: 0.028977\n","| Global Epoch: 92 | Local Epoch: 7 | Generator Loss: 14.216934 | Discriminator Loss: 0.025852\n","| Global Epoch: 92 | Local Epoch: 8 | Generator Loss: 14.270588 | Discriminator Loss: 0.024347\n","| Global Epoch: 92 | Local Epoch: 9 | Generator Loss: 14.299809 | Discriminator Loss: 0.022408\n","| Global Epoch: 92 | Local Epoch: 0 | Generator Loss: 14.068988 | Discriminator Loss: 0.066374\n","| Global Epoch: 92 | Local Epoch: 1 | Generator Loss: 14.054649 | Discriminator Loss: 0.038021\n","| Global Epoch: 92 | Local Epoch: 2 | Generator Loss: 14.145136 | Discriminator Loss: 0.026356\n","| Global Epoch: 92 | Local Epoch: 3 | Generator Loss: 14.408266 | Discriminator Loss: 0.020153\n","| Global Epoch: 92 | Local Epoch: 4 | Generator Loss: 14.553228 | Discriminator Loss: 0.016322\n","| Global Epoch: 92 | Local Epoch: 5 | Generator Loss: 14.677389 | Discriminator Loss: 0.013753\n","| Global Epoch: 92 | Local Epoch: 6 | Generator Loss: 14.778524 | Discriminator Loss: 0.011897\n","| Global Epoch: 92 | Local Epoch: 7 | Generator Loss: 14.832714 | Discriminator Loss: 0.010490\n","| Global Epoch: 92 | Local Epoch: 8 | Generator Loss: 14.849246 | Discriminator Loss: 0.009415\n","| Global Epoch: 92 | Local Epoch: 9 | Generator Loss: 14.934037 | Discriminator Loss: 0.008532\n","| Global Epoch: 92 | Local Epoch: 0 | Generator Loss: 15.580794 | Discriminator Loss: 0.072317\n","| Global Epoch: 92 | Local Epoch: 1 | Generator Loss: 15.454362 | Discriminator Loss: 0.049163\n","| Global Epoch: 92 | Local Epoch: 2 | Generator Loss: 14.987959 | Discriminator Loss: 0.034835\n","| Global Epoch: 92 | Local Epoch: 3 | Generator Loss: 15.119798 | Discriminator Loss: 0.026987\n","| Global Epoch: 92 | Local Epoch: 4 | Generator Loss: 15.096312 | Discriminator Loss: 0.021910\n","| Global Epoch: 92 | Local Epoch: 5 | Generator Loss: 15.131674 | Discriminator Loss: 0.018482\n","| Global Epoch: 92 | Local Epoch: 6 | Generator Loss: 15.196213 | Discriminator Loss: 0.015988\n","| Global Epoch: 92 | Local Epoch: 7 | Generator Loss: 15.210155 | Discriminator Loss: 0.014092\n","| Global Epoch: 92 | Local Epoch: 8 | Generator Loss: 15.214346 | Discriminator Loss: 0.012615\n","| Global Epoch: 92 | Local Epoch: 9 | Generator Loss: 15.218959 | Discriminator Loss: 0.011436\n","After 93 epoch global training, averged local generator loss is: 15.7950, averged local discrimitor loss is: 0.0590\n","Averged validation loss is: 6.1818\n","| Global Epoch: 93 | Local Epoch: 0 | Generator Loss: 14.788082 | Discriminator Loss: 0.051937\n","| Global Epoch: 93 | Local Epoch: 1 | Generator Loss: 14.926815 | Discriminator Loss: 0.033087\n","| Global Epoch: 93 | Local Epoch: 2 | Generator Loss: 14.959225 | Discriminator Loss: 0.023544\n","| Global Epoch: 93 | Local Epoch: 3 | Generator Loss: 15.012444 | Discriminator Loss: 0.018067\n","| Global Epoch: 93 | Local Epoch: 4 | Generator Loss: 15.075410 | Discriminator Loss: 0.014672\n","| Global Epoch: 93 | Local Epoch: 5 | Generator Loss: 15.109134 | Discriminator Loss: 0.012365\n","| Global Epoch: 93 | Local Epoch: 6 | Generator Loss: 15.156383 | Discriminator Loss: 0.010700\n","| Global Epoch: 93 | Local Epoch: 7 | Generator Loss: 15.179686 | Discriminator Loss: 0.009438\n","| Global Epoch: 93 | Local Epoch: 8 | Generator Loss: 15.222626 | Discriminator Loss: 0.008447\n","| Global Epoch: 93 | Local Epoch: 9 | Generator Loss: 15.246304 | Discriminator Loss: 0.007650\n","| Global Epoch: 93 | Local Epoch: 0 | Generator Loss: 15.303842 | Discriminator Loss: 0.048757\n","| Global Epoch: 93 | Local Epoch: 1 | Generator Loss: 15.127675 | Discriminator Loss: 0.030267\n","| Global Epoch: 93 | Local Epoch: 2 | Generator Loss: 14.952166 | Discriminator Loss: 0.021513\n","| Global Epoch: 93 | Local Epoch: 3 | Generator Loss: 14.958693 | Discriminator Loss: 0.016508\n","| Global Epoch: 93 | Local Epoch: 4 | Generator Loss: 14.763693 | Discriminator Loss: 0.018801\n","| Global Epoch: 93 | Local Epoch: 5 | Generator Loss: 14.643973 | Discriminator Loss: 0.016629\n","| Global Epoch: 93 | Local Epoch: 6 | Generator Loss: 14.621356 | Discriminator Loss: 0.014672\n","| Global Epoch: 93 | Local Epoch: 7 | Generator Loss: 14.669033 | Discriminator Loss: 0.013010\n","| Global Epoch: 93 | Local Epoch: 8 | Generator Loss: 14.704899 | Discriminator Loss: 0.011651\n","| Global Epoch: 93 | Local Epoch: 9 | Generator Loss: 14.719634 | Discriminator Loss: 0.010544\n","| Global Epoch: 93 | Local Epoch: 0 | Generator Loss: 14.570478 | Discriminator Loss: 0.231463\n","| Global Epoch: 93 | Local Epoch: 1 | Generator Loss: 14.394203 | Discriminator Loss: 0.126174\n","| Global Epoch: 93 | Local Epoch: 2 | Generator Loss: 14.314590 | Discriminator Loss: 0.086071\n","| Global Epoch: 93 | Local Epoch: 3 | Generator Loss: 14.275220 | Discriminator Loss: 0.065328\n","| Global Epoch: 93 | Local Epoch: 4 | Generator Loss: 14.286042 | Discriminator Loss: 0.052754\n","| Global Epoch: 93 | Local Epoch: 5 | Generator Loss: 14.367499 | Discriminator Loss: 0.044343\n","| Global Epoch: 93 | Local Epoch: 6 | Generator Loss: 14.409861 | Discriminator Loss: 0.038252\n","| Global Epoch: 93 | Local Epoch: 7 | Generator Loss: 14.442719 | Discriminator Loss: 0.033648\n","| Global Epoch: 93 | Local Epoch: 8 | Generator Loss: 14.526837 | Discriminator Loss: 0.030320\n","| Global Epoch: 93 | Local Epoch: 9 | Generator Loss: 14.603980 | Discriminator Loss: 0.027812\n","| Global Epoch: 93 | Local Epoch: 0 | Generator Loss: 15.146476 | Discriminator Loss: 0.130532\n","| Global Epoch: 93 | Local Epoch: 1 | Generator Loss: 14.947982 | Discriminator Loss: 0.073194\n","| Global Epoch: 93 | Local Epoch: 2 | Generator Loss: 14.942117 | Discriminator Loss: 0.050286\n","| Global Epoch: 93 | Local Epoch: 3 | Generator Loss: 14.978731 | Discriminator Loss: 0.038344\n","| Global Epoch: 93 | Local Epoch: 4 | Generator Loss: 15.005596 | Discriminator Loss: 0.031054\n","| Global Epoch: 93 | Local Epoch: 5 | Generator Loss: 15.087877 | Discriminator Loss: 0.026138\n","| Global Epoch: 93 | Local Epoch: 6 | Generator Loss: 15.210496 | Discriminator Loss: 0.022611\n","| Global Epoch: 93 | Local Epoch: 7 | Generator Loss: 15.307296 | Discriminator Loss: 0.019927\n","| Global Epoch: 93 | Local Epoch: 8 | Generator Loss: 15.356004 | Discriminator Loss: 0.017812\n","| Global Epoch: 93 | Local Epoch: 9 | Generator Loss: 15.371161 | Discriminator Loss: 0.016108\n","| Global Epoch: 93 | Local Epoch: 0 | Generator Loss: 15.298018 | Discriminator Loss: 0.032504\n","| Global Epoch: 93 | Local Epoch: 1 | Generator Loss: 15.287984 | Discriminator Loss: 0.019283\n","| Global Epoch: 93 | Local Epoch: 2 | Generator Loss: 15.306480 | Discriminator Loss: 0.013646\n","| Global Epoch: 93 | Local Epoch: 3 | Generator Loss: 15.226853 | Discriminator Loss: 0.010493\n","| Global Epoch: 93 | Local Epoch: 4 | Generator Loss: 15.287275 | Discriminator Loss: 0.008764\n","| Global Epoch: 93 | Local Epoch: 5 | Generator Loss: 15.093096 | Discriminator Loss: 0.024698\n","| Global Epoch: 93 | Local Epoch: 6 | Generator Loss: 15.077081 | Discriminator Loss: 0.025427\n","| Global Epoch: 93 | Local Epoch: 7 | Generator Loss: 15.051962 | Discriminator Loss: 0.022718\n","| Global Epoch: 93 | Local Epoch: 8 | Generator Loss: 15.055440 | Discriminator Loss: 0.020344\n","| Global Epoch: 93 | Local Epoch: 9 | Generator Loss: 15.079496 | Discriminator Loss: 0.018599\n","After 94 epoch global training, averged local generator loss is: 15.7873, averged local discrimitor loss is: 0.0586\n","Averged validation loss is: 6.1032\n","| Global Epoch: 94 | Local Epoch: 0 | Generator Loss: 14.579801 | Discriminator Loss: 0.080123\n","| Global Epoch: 94 | Local Epoch: 1 | Generator Loss: 14.523341 | Discriminator Loss: 0.046539\n","| Global Epoch: 94 | Local Epoch: 2 | Generator Loss: 14.742314 | Discriminator Loss: 0.032692\n","| Global Epoch: 94 | Local Epoch: 3 | Generator Loss: 14.536200 | Discriminator Loss: 0.026545\n","| Global Epoch: 94 | Local Epoch: 4 | Generator Loss: 14.524297 | Discriminator Loss: 0.021587\n","| Global Epoch: 94 | Local Epoch: 5 | Generator Loss: 14.572102 | Discriminator Loss: 0.018185\n","| Global Epoch: 94 | Local Epoch: 6 | Generator Loss: 14.677351 | Discriminator Loss: 0.015714\n","| Global Epoch: 94 | Local Epoch: 7 | Generator Loss: 14.792736 | Discriminator Loss: 0.013861\n","| Global Epoch: 94 | Local Epoch: 8 | Generator Loss: 14.849277 | Discriminator Loss: 0.012392\n","| Global Epoch: 94 | Local Epoch: 9 | Generator Loss: 14.889594 | Discriminator Loss: 0.011209\n","| Global Epoch: 94 | Local Epoch: 0 | Generator Loss: 14.793425 | Discriminator Loss: 0.071978\n","| Global Epoch: 94 | Local Epoch: 1 | Generator Loss: 14.578246 | Discriminator Loss: 0.053413\n","| Global Epoch: 94 | Local Epoch: 2 | Generator Loss: 14.482426 | Discriminator Loss: 0.038853\n","| Global Epoch: 94 | Local Epoch: 3 | Generator Loss: 14.642918 | Discriminator Loss: 0.029804\n","| Global Epoch: 94 | Local Epoch: 4 | Generator Loss: 14.785831 | Discriminator Loss: 0.033421\n","| Global Epoch: 94 | Local Epoch: 5 | Generator Loss: 14.555120 | Discriminator Loss: 0.055182\n","| Global Epoch: 94 | Local Epoch: 6 | Generator Loss: 14.411675 | Discriminator Loss: 0.052722\n","| Global Epoch: 94 | Local Epoch: 7 | Generator Loss: 14.398909 | Discriminator Loss: 0.049447\n","| Global Epoch: 94 | Local Epoch: 8 | Generator Loss: 14.296626 | Discriminator Loss: 0.048663\n","| Global Epoch: 94 | Local Epoch: 9 | Generator Loss: 14.282978 | Discriminator Loss: 0.044580\n","| Global Epoch: 94 | Local Epoch: 0 | Generator Loss: 14.446701 | Discriminator Loss: 0.064260\n","| Global Epoch: 94 | Local Epoch: 1 | Generator Loss: 14.513122 | Discriminator Loss: 0.038530\n","| Global Epoch: 94 | Local Epoch: 2 | Generator Loss: 14.756613 | Discriminator Loss: 0.026680\n","| Global Epoch: 94 | Local Epoch: 3 | Generator Loss: 14.844443 | Discriminator Loss: 0.020338\n","| Global Epoch: 94 | Local Epoch: 4 | Generator Loss: 14.918348 | Discriminator Loss: 0.016463\n","| Global Epoch: 94 | Local Epoch: 5 | Generator Loss: 14.958924 | Discriminator Loss: 0.013850\n","| Global Epoch: 94 | Local Epoch: 6 | Generator Loss: 14.990287 | Discriminator Loss: 0.011970\n","| Global Epoch: 94 | Local Epoch: 7 | Generator Loss: 15.064521 | Discriminator Loss: 0.010592\n","| Global Epoch: 94 | Local Epoch: 8 | Generator Loss: 15.093959 | Discriminator Loss: 0.011805\n","| Global Epoch: 94 | Local Epoch: 9 | Generator Loss: 15.017484 | Discriminator Loss: 0.022953\n","| Global Epoch: 94 | Local Epoch: 0 | Generator Loss: 13.243232 | Discriminator Loss: 0.204647\n","| Global Epoch: 94 | Local Epoch: 1 | Generator Loss: 13.394190 | Discriminator Loss: 0.116631\n","| Global Epoch: 94 | Local Epoch: 2 | Generator Loss: 13.404633 | Discriminator Loss: 0.080030\n","| Global Epoch: 94 | Local Epoch: 3 | Generator Loss: 13.522966 | Discriminator Loss: 0.061361\n","| Global Epoch: 94 | Local Epoch: 4 | Generator Loss: 13.665603 | Discriminator Loss: 0.049725\n","| Global Epoch: 94 | Local Epoch: 5 | Generator Loss: 13.887572 | Discriminator Loss: 0.042935\n","| Global Epoch: 94 | Local Epoch: 6 | Generator Loss: 13.966285 | Discriminator Loss: 0.040191\n","| Global Epoch: 94 | Local Epoch: 7 | Generator Loss: 14.018582 | Discriminator Loss: 0.038257\n","| Global Epoch: 94 | Local Epoch: 8 | Generator Loss: 14.039445 | Discriminator Loss: 0.036315\n","| Global Epoch: 94 | Local Epoch: 9 | Generator Loss: 14.100277 | Discriminator Loss: 0.033908\n","| Global Epoch: 94 | Local Epoch: 0 | Generator Loss: 14.963194 | Discriminator Loss: 0.302343\n","| Global Epoch: 94 | Local Epoch: 1 | Generator Loss: 15.092268 | Discriminator Loss: 0.158106\n","| Global Epoch: 94 | Local Epoch: 2 | Generator Loss: 14.996152 | Discriminator Loss: 0.107631\n","| Global Epoch: 94 | Local Epoch: 3 | Generator Loss: 14.802474 | Discriminator Loss: 0.081422\n","| Global Epoch: 94 | Local Epoch: 4 | Generator Loss: 14.816344 | Discriminator Loss: 0.065639\n","| Global Epoch: 94 | Local Epoch: 5 | Generator Loss: 14.721791 | Discriminator Loss: 0.054959\n","| Global Epoch: 94 | Local Epoch: 6 | Generator Loss: 14.673610 | Discriminator Loss: 0.047280\n","| Global Epoch: 94 | Local Epoch: 7 | Generator Loss: 14.631718 | Discriminator Loss: 0.041566\n","| Global Epoch: 94 | Local Epoch: 8 | Generator Loss: 14.595475 | Discriminator Loss: 0.037126\n","| Global Epoch: 94 | Local Epoch: 9 | Generator Loss: 14.572938 | Discriminator Loss: 0.033490\n","After 95 epoch global training, averged local generator loss is: 15.7746, averged local discrimitor loss is: 0.0583\n","Averged validation loss is: 6.0529\n","| Global Epoch: 95 | Local Epoch: 0 | Generator Loss: 14.610512 | Discriminator Loss: 0.117112\n","| Global Epoch: 95 | Local Epoch: 1 | Generator Loss: 14.410929 | Discriminator Loss: 0.074125\n","| Global Epoch: 95 | Local Epoch: 2 | Generator Loss: 14.336379 | Discriminator Loss: 0.064087\n","| Global Epoch: 95 | Local Epoch: 3 | Generator Loss: 14.340990 | Discriminator Loss: 0.050276\n","| Global Epoch: 95 | Local Epoch: 4 | Generator Loss: 14.287522 | Discriminator Loss: 0.040869\n","| Global Epoch: 95 | Local Epoch: 5 | Generator Loss: 14.435602 | Discriminator Loss: 0.034473\n","| Global Epoch: 95 | Local Epoch: 6 | Generator Loss: 14.533702 | Discriminator Loss: 0.032876\n","| Global Epoch: 95 | Local Epoch: 7 | Generator Loss: 14.349172 | Discriminator Loss: 0.040664\n","| Global Epoch: 95 | Local Epoch: 8 | Generator Loss: 14.348019 | Discriminator Loss: 0.037048\n","| Global Epoch: 95 | Local Epoch: 9 | Generator Loss: 14.399097 | Discriminator Loss: 0.033740\n","| Global Epoch: 95 | Local Epoch: 0 | Generator Loss: 14.458270 | Discriminator Loss: 0.058753\n","| Global Epoch: 95 | Local Epoch: 1 | Generator Loss: 14.763790 | Discriminator Loss: 0.034713\n","| Global Epoch: 95 | Local Epoch: 2 | Generator Loss: 15.066130 | Discriminator Loss: 0.024774\n","| Global Epoch: 95 | Local Epoch: 3 | Generator Loss: 15.084548 | Discriminator Loss: 0.020265\n","| Global Epoch: 95 | Local Epoch: 4 | Generator Loss: 15.109644 | Discriminator Loss: 0.018081\n","| Global Epoch: 95 | Local Epoch: 5 | Generator Loss: 15.133880 | Discriminator Loss: 0.016994\n","| Global Epoch: 95 | Local Epoch: 6 | Generator Loss: 14.983744 | Discriminator Loss: 0.042584\n","| Global Epoch: 95 | Local Epoch: 7 | Generator Loss: 14.775590 | Discriminator Loss: 0.043421\n","| Global Epoch: 95 | Local Epoch: 8 | Generator Loss: 14.657936 | Discriminator Loss: 0.039295\n","| Global Epoch: 95 | Local Epoch: 9 | Generator Loss: 14.621262 | Discriminator Loss: 0.035679\n","| Global Epoch: 95 | Local Epoch: 0 | Generator Loss: 14.448353 | Discriminator Loss: 0.323458\n","| Global Epoch: 95 | Local Epoch: 1 | Generator Loss: 14.116588 | Discriminator Loss: 0.178988\n","| Global Epoch: 95 | Local Epoch: 2 | Generator Loss: 13.966007 | Discriminator Loss: 0.122708\n","| Global Epoch: 95 | Local Epoch: 3 | Generator Loss: 13.927798 | Discriminator Loss: 0.093979\n","| Global Epoch: 95 | Local Epoch: 4 | Generator Loss: 13.997210 | Discriminator Loss: 0.076430\n","| Global Epoch: 95 | Local Epoch: 5 | Generator Loss: 14.205196 | Discriminator Loss: 0.064381\n","| Global Epoch: 95 | Local Epoch: 6 | Generator Loss: 14.375358 | Discriminator Loss: 0.055713\n","| Global Epoch: 95 | Local Epoch: 7 | Generator Loss: 14.462507 | Discriminator Loss: 0.049060\n","| Global Epoch: 95 | Local Epoch: 8 | Generator Loss: 14.529421 | Discriminator Loss: 0.043848\n","| Global Epoch: 95 | Local Epoch: 9 | Generator Loss: 14.601642 | Discriminator Loss: 0.039843\n","| Global Epoch: 95 | Local Epoch: 0 | Generator Loss: 15.426181 | Discriminator Loss: 0.155780\n","| Global Epoch: 95 | Local Epoch: 1 | Generator Loss: 15.579985 | Discriminator Loss: 0.088250\n","| Global Epoch: 95 | Local Epoch: 2 | Generator Loss: 15.610140 | Discriminator Loss: 0.060831\n","| Global Epoch: 95 | Local Epoch: 3 | Generator Loss: 15.524048 | Discriminator Loss: 0.046382\n","| Global Epoch: 95 | Local Epoch: 4 | Generator Loss: 15.512799 | Discriminator Loss: 0.037552\n","| Global Epoch: 95 | Local Epoch: 5 | Generator Loss: 15.490216 | Discriminator Loss: 0.031592\n","| Global Epoch: 95 | Local Epoch: 6 | Generator Loss: 15.440433 | Discriminator Loss: 0.027282\n","| Global Epoch: 95 | Local Epoch: 7 | Generator Loss: 15.459428 | Discriminator Loss: 0.024047\n","| Global Epoch: 95 | Local Epoch: 8 | Generator Loss: 15.519907 | Discriminator Loss: 0.021501\n","| Global Epoch: 95 | Local Epoch: 9 | Generator Loss: 15.565990 | Discriminator Loss: 0.019610\n","| Global Epoch: 95 | Local Epoch: 0 | Generator Loss: 15.725254 | Discriminator Loss: 0.236727\n","| Global Epoch: 95 | Local Epoch: 1 | Generator Loss: 15.544669 | Discriminator Loss: 0.133113\n","| Global Epoch: 95 | Local Epoch: 2 | Generator Loss: 15.332515 | Discriminator Loss: 0.091567\n","| Global Epoch: 95 | Local Epoch: 3 | Generator Loss: 15.228718 | Discriminator Loss: 0.069902\n","| Global Epoch: 95 | Local Epoch: 4 | Generator Loss: 15.142509 | Discriminator Loss: 0.056555\n","| Global Epoch: 95 | Local Epoch: 5 | Generator Loss: 15.140282 | Discriminator Loss: 0.048065\n","| Global Epoch: 95 | Local Epoch: 6 | Generator Loss: 15.100079 | Discriminator Loss: 0.041579\n","| Global Epoch: 95 | Local Epoch: 7 | Generator Loss: 15.097230 | Discriminator Loss: 0.036645\n","| Global Epoch: 95 | Local Epoch: 8 | Generator Loss: 15.132947 | Discriminator Loss: 0.032759\n","| Global Epoch: 95 | Local Epoch: 9 | Generator Loss: 15.198409 | Discriminator Loss: 0.029632\n","After 96 epoch global training, averged local generator loss is: 15.7686, averged local discrimitor loss is: 0.0580\n","Averged validation loss is: 6.1929\n","| Global Epoch: 96 | Local Epoch: 0 | Generator Loss: 16.087297 | Discriminator Loss: 0.075157\n","| Global Epoch: 96 | Local Epoch: 1 | Generator Loss: 15.908550 | Discriminator Loss: 0.042164\n","| Global Epoch: 96 | Local Epoch: 2 | Generator Loss: 16.016531 | Discriminator Loss: 0.028749\n","| Global Epoch: 96 | Local Epoch: 3 | Generator Loss: 15.993998 | Discriminator Loss: 0.022013\n","| Global Epoch: 96 | Local Epoch: 4 | Generator Loss: 15.970693 | Discriminator Loss: 0.017885\n","| Global Epoch: 96 | Local Epoch: 5 | Generator Loss: 15.910117 | Discriminator Loss: 0.015021\n","| Global Epoch: 96 | Local Epoch: 6 | Generator Loss: 15.949297 | Discriminator Loss: 0.012955\n","| Global Epoch: 96 | Local Epoch: 7 | Generator Loss: 15.927025 | Discriminator Loss: 0.011402\n","| Global Epoch: 96 | Local Epoch: 8 | Generator Loss: 15.899867 | Discriminator Loss: 0.010181\n","| Global Epoch: 96 | Local Epoch: 9 | Generator Loss: 15.888977 | Discriminator Loss: 0.009199\n","| Global Epoch: 96 | Local Epoch: 0 | Generator Loss: 15.282526 | Discriminator Loss: 0.032641\n","| Global Epoch: 96 | Local Epoch: 1 | Generator Loss: 15.516488 | Discriminator Loss: 0.020685\n","| Global Epoch: 96 | Local Epoch: 2 | Generator Loss: 15.658782 | Discriminator Loss: 0.014405\n","| Global Epoch: 96 | Local Epoch: 3 | Generator Loss: 15.898068 | Discriminator Loss: 0.011158\n","| Global Epoch: 96 | Local Epoch: 4 | Generator Loss: 16.035913 | Discriminator Loss: 0.009080\n","| Global Epoch: 96 | Local Epoch: 5 | Generator Loss: 16.222449 | Discriminator Loss: 0.007703\n","| Global Epoch: 96 | Local Epoch: 6 | Generator Loss: 16.295614 | Discriminator Loss: 0.006715\n","| Global Epoch: 96 | Local Epoch: 7 | Generator Loss: 16.310142 | Discriminator Loss: 0.006941\n","| Global Epoch: 96 | Local Epoch: 8 | Generator Loss: 16.142522 | Discriminator Loss: 0.020448\n","| Global Epoch: 96 | Local Epoch: 9 | Generator Loss: 16.019838 | Discriminator Loss: 0.027317\n","| Global Epoch: 96 | Local Epoch: 0 | Generator Loss: 15.190485 | Discriminator Loss: 0.227877\n","| Global Epoch: 96 | Local Epoch: 1 | Generator Loss: 14.929593 | Discriminator Loss: 0.126889\n","| Global Epoch: 96 | Local Epoch: 2 | Generator Loss: 14.957654 | Discriminator Loss: 0.087044\n","| Global Epoch: 96 | Local Epoch: 3 | Generator Loss: 15.130061 | Discriminator Loss: 0.066817\n","| Global Epoch: 96 | Local Epoch: 4 | Generator Loss: 15.265590 | Discriminator Loss: 0.054229\n","| Global Epoch: 96 | Local Epoch: 5 | Generator Loss: 15.279753 | Discriminator Loss: 0.045623\n","| Global Epoch: 96 | Local Epoch: 6 | Generator Loss: 15.277032 | Discriminator Loss: 0.039437\n","| Global Epoch: 96 | Local Epoch: 7 | Generator Loss: 15.328897 | Discriminator Loss: 0.034762\n","| Global Epoch: 96 | Local Epoch: 8 | Generator Loss: 15.392056 | Discriminator Loss: 0.031095\n","| Global Epoch: 96 | Local Epoch: 9 | Generator Loss: 15.447476 | Discriminator Loss: 0.028129\n","| Global Epoch: 96 | Local Epoch: 0 | Generator Loss: 15.795103 | Discriminator Loss: 0.264159\n","| Global Epoch: 96 | Local Epoch: 1 | Generator Loss: 15.616651 | Discriminator Loss: 0.145576\n","| Global Epoch: 96 | Local Epoch: 2 | Generator Loss: 15.433332 | Discriminator Loss: 0.099574\n","| Global Epoch: 96 | Local Epoch: 3 | Generator Loss: 15.554992 | Discriminator Loss: 0.075930\n","| Global Epoch: 96 | Local Epoch: 4 | Generator Loss: 15.644431 | Discriminator Loss: 0.061417\n","| Global Epoch: 96 | Local Epoch: 5 | Generator Loss: 15.628733 | Discriminator Loss: 0.051597\n","| Global Epoch: 96 | Local Epoch: 6 | Generator Loss: 15.720318 | Discriminator Loss: 0.044612\n","| Global Epoch: 96 | Local Epoch: 7 | Generator Loss: 15.869487 | Discriminator Loss: 0.039293\n","| Global Epoch: 96 | Local Epoch: 8 | Generator Loss: 15.939891 | Discriminator Loss: 0.035113\n","| Global Epoch: 96 | Local Epoch: 9 | Generator Loss: 15.980233 | Discriminator Loss: 0.031747\n","| Global Epoch: 96 | Local Epoch: 0 | Generator Loss: 15.864161 | Discriminator Loss: 0.075120\n","| Global Epoch: 96 | Local Epoch: 1 | Generator Loss: 15.243806 | Discriminator Loss: 0.043866\n","| Global Epoch: 96 | Local Epoch: 2 | Generator Loss: 15.392236 | Discriminator Loss: 0.030595\n","| Global Epoch: 96 | Local Epoch: 3 | Generator Loss: 15.385979 | Discriminator Loss: 0.024275\n","| Global Epoch: 96 | Local Epoch: 4 | Generator Loss: 15.381917 | Discriminator Loss: 0.024849\n","| Global Epoch: 96 | Local Epoch: 5 | Generator Loss: 15.497759 | Discriminator Loss: 0.021516\n","| Global Epoch: 96 | Local Epoch: 6 | Generator Loss: 15.563167 | Discriminator Loss: 0.018640\n","| Global Epoch: 96 | Local Epoch: 7 | Generator Loss: 15.687601 | Discriminator Loss: 0.016677\n","| Global Epoch: 96 | Local Epoch: 8 | Generator Loss: 15.732059 | Discriminator Loss: 0.015486\n","| Global Epoch: 96 | Local Epoch: 9 | Generator Loss: 15.699923 | Discriminator Loss: 0.024091\n","After 97 epoch global training, averged local generator loss is: 15.7679, averged local discrimitor loss is: 0.0577\n","Averged validation loss is: 4.3797\n","| Global Epoch: 97 | Local Epoch: 0 | Generator Loss: 15.551105 | Discriminator Loss: 0.070938\n","| Global Epoch: 97 | Local Epoch: 1 | Generator Loss: 15.363840 | Discriminator Loss: 0.038866\n","| Global Epoch: 97 | Local Epoch: 2 | Generator Loss: 15.551851 | Discriminator Loss: 0.026721\n","| Global Epoch: 97 | Local Epoch: 3 | Generator Loss: 15.714859 | Discriminator Loss: 0.020412\n","| Global Epoch: 97 | Local Epoch: 4 | Generator Loss: 15.780358 | Discriminator Loss: 0.016548\n","| Global Epoch: 97 | Local Epoch: 5 | Generator Loss: 15.863707 | Discriminator Loss: 0.013931\n","| Global Epoch: 97 | Local Epoch: 6 | Generator Loss: 15.922479 | Discriminator Loss: 0.012057\n","| Global Epoch: 97 | Local Epoch: 7 | Generator Loss: 15.916356 | Discriminator Loss: 0.015619\n","| Global Epoch: 97 | Local Epoch: 8 | Generator Loss: 15.765871 | Discriminator Loss: 0.028476\n","| Global Epoch: 97 | Local Epoch: 9 | Generator Loss: 15.698069 | Discriminator Loss: 0.027085\n","| Global Epoch: 97 | Local Epoch: 0 | Generator Loss: 15.166198 | Discriminator Loss: 0.209661\n","| Global Epoch: 97 | Local Epoch: 1 | Generator Loss: 15.218773 | Discriminator Loss: 0.118816\n","| Global Epoch: 97 | Local Epoch: 2 | Generator Loss: 15.062802 | Discriminator Loss: 0.081681\n","| Global Epoch: 97 | Local Epoch: 3 | Generator Loss: 15.189557 | Discriminator Loss: 0.062621\n","| Global Epoch: 97 | Local Epoch: 4 | Generator Loss: 15.309633 | Discriminator Loss: 0.050899\n","| Global Epoch: 97 | Local Epoch: 5 | Generator Loss: 15.361020 | Discriminator Loss: 0.042865\n","| Global Epoch: 97 | Local Epoch: 6 | Generator Loss: 15.455576 | Discriminator Loss: 0.037073\n","| Global Epoch: 97 | Local Epoch: 7 | Generator Loss: 15.491081 | Discriminator Loss: 0.033806\n","| Global Epoch: 97 | Local Epoch: 8 | Generator Loss: 15.521310 | Discriminator Loss: 0.031162\n","| Global Epoch: 97 | Local Epoch: 9 | Generator Loss: 15.577841 | Discriminator Loss: 0.028344\n","| Global Epoch: 97 | Local Epoch: 0 | Generator Loss: 15.750275 | Discriminator Loss: 0.051229\n","| Global Epoch: 97 | Local Epoch: 1 | Generator Loss: 15.638882 | Discriminator Loss: 0.032695\n","| Global Epoch: 97 | Local Epoch: 2 | Generator Loss: 15.679224 | Discriminator Loss: 0.022722\n","| Global Epoch: 97 | Local Epoch: 3 | Generator Loss: 15.436510 | Discriminator Loss: 0.020337\n","| Global Epoch: 97 | Local Epoch: 4 | Generator Loss: 15.416165 | Discriminator Loss: 0.016837\n","| Global Epoch: 97 | Local Epoch: 5 | Generator Loss: 15.419642 | Discriminator Loss: 0.014967\n","| Global Epoch: 97 | Local Epoch: 6 | Generator Loss: 15.428392 | Discriminator Loss: 0.014133\n","| Global Epoch: 97 | Local Epoch: 7 | Generator Loss: 15.458009 | Discriminator Loss: 0.015841\n","| Global Epoch: 97 | Local Epoch: 8 | Generator Loss: 15.444133 | Discriminator Loss: 0.018840\n","| Global Epoch: 97 | Local Epoch: 9 | Generator Loss: 15.460770 | Discriminator Loss: 0.017916\n","| Global Epoch: 97 | Local Epoch: 0 | Generator Loss: 15.471596 | Discriminator Loss: 0.210159\n","| Global Epoch: 97 | Local Epoch: 1 | Generator Loss: 15.266517 | Discriminator Loss: 0.118886\n","| Global Epoch: 97 | Local Epoch: 2 | Generator Loss: 15.129433 | Discriminator Loss: 0.081567\n","| Global Epoch: 97 | Local Epoch: 3 | Generator Loss: 15.160901 | Discriminator Loss: 0.062205\n","| Global Epoch: 97 | Local Epoch: 4 | Generator Loss: 15.159861 | Discriminator Loss: 0.050406\n","| Global Epoch: 97 | Local Epoch: 5 | Generator Loss: 15.170178 | Discriminator Loss: 0.042443\n","| Global Epoch: 97 | Local Epoch: 6 | Generator Loss: 15.109390 | Discriminator Loss: 0.038687\n","| Global Epoch: 97 | Local Epoch: 7 | Generator Loss: 15.123464 | Discriminator Loss: 0.035020\n","| Global Epoch: 97 | Local Epoch: 8 | Generator Loss: 15.167104 | Discriminator Loss: 0.031410\n","| Global Epoch: 97 | Local Epoch: 9 | Generator Loss: 15.230733 | Discriminator Loss: 0.028450\n","| Global Epoch: 97 | Local Epoch: 0 | Generator Loss: 16.456419 | Discriminator Loss: 0.255644\n","| Global Epoch: 97 | Local Epoch: 1 | Generator Loss: 16.331671 | Discriminator Loss: 0.141214\n","| Global Epoch: 97 | Local Epoch: 2 | Generator Loss: 16.174301 | Discriminator Loss: 0.097372\n","| Global Epoch: 97 | Local Epoch: 3 | Generator Loss: 16.135058 | Discriminator Loss: 0.074265\n","| Global Epoch: 97 | Local Epoch: 4 | Generator Loss: 16.080016 | Discriminator Loss: 0.060110\n","| Global Epoch: 97 | Local Epoch: 5 | Generator Loss: 15.996637 | Discriminator Loss: 0.050806\n","| Global Epoch: 97 | Local Epoch: 6 | Generator Loss: 15.960239 | Discriminator Loss: 0.044288\n","| Global Epoch: 97 | Local Epoch: 7 | Generator Loss: 15.954712 | Discriminator Loss: 0.039166\n","| Global Epoch: 97 | Local Epoch: 8 | Generator Loss: 15.933313 | Discriminator Loss: 0.035392\n","| Global Epoch: 97 | Local Epoch: 9 | Generator Loss: 15.934680 | Discriminator Loss: 0.032256\n","After 98 epoch global training, averged local generator loss is: 15.7696, averged local discrimitor loss is: 0.0574\n","Averged validation loss is: 7.6570\n","| Global Epoch: 98 | Local Epoch: 0 | Generator Loss: 16.060075 | Discriminator Loss: 0.028518\n","| Global Epoch: 98 | Local Epoch: 1 | Generator Loss: 15.945436 | Discriminator Loss: 0.022366\n","| Global Epoch: 98 | Local Epoch: 2 | Generator Loss: 16.146572 | Discriminator Loss: 0.015661\n","| Global Epoch: 98 | Local Epoch: 3 | Generator Loss: 16.147988 | Discriminator Loss: 0.012496\n","| Global Epoch: 98 | Local Epoch: 4 | Generator Loss: 16.226276 | Discriminator Loss: 0.010650\n","| Global Epoch: 98 | Local Epoch: 5 | Generator Loss: 15.825255 | Discriminator Loss: 0.026696\n","| Global Epoch: 98 | Local Epoch: 6 | Generator Loss: 15.802355 | Discriminator Loss: 0.024488\n","| Global Epoch: 98 | Local Epoch: 7 | Generator Loss: 15.720036 | Discriminator Loss: 0.021687\n","| Global Epoch: 98 | Local Epoch: 8 | Generator Loss: 15.697704 | Discriminator Loss: 0.019787\n","| Global Epoch: 98 | Local Epoch: 9 | Generator Loss: 15.703977 | Discriminator Loss: 0.017979\n","| Global Epoch: 98 | Local Epoch: 0 | Generator Loss: 15.506632 | Discriminator Loss: 0.255084\n","| Global Epoch: 98 | Local Epoch: 1 | Generator Loss: 15.914349 | Discriminator Loss: 0.134052\n","| Global Epoch: 98 | Local Epoch: 2 | Generator Loss: 16.167656 | Discriminator Loss: 0.094069\n","| Global Epoch: 98 | Local Epoch: 3 | Generator Loss: 16.295473 | Discriminator Loss: 0.071885\n","| Global Epoch: 98 | Local Epoch: 4 | Generator Loss: 16.345031 | Discriminator Loss: 0.057990\n","| Global Epoch: 98 | Local Epoch: 5 | Generator Loss: 16.300056 | Discriminator Loss: 0.048540\n","| Global Epoch: 98 | Local Epoch: 6 | Generator Loss: 16.193723 | Discriminator Loss: 0.041812\n","| Global Epoch: 98 | Local Epoch: 7 | Generator Loss: 16.139507 | Discriminator Loss: 0.036707\n","| Global Epoch: 98 | Local Epoch: 8 | Generator Loss: 16.113726 | Discriminator Loss: 0.032717\n","| Global Epoch: 98 | Local Epoch: 9 | Generator Loss: 16.062356 | Discriminator Loss: 0.029497\n","| Global Epoch: 98 | Local Epoch: 0 | Generator Loss: 15.451895 | Discriminator Loss: 0.079513\n","| Global Epoch: 98 | Local Epoch: 1 | Generator Loss: 15.590159 | Discriminator Loss: 0.043995\n","| Global Epoch: 98 | Local Epoch: 2 | Generator Loss: 15.551236 | Discriminator Loss: 0.030281\n","| Global Epoch: 98 | Local Epoch: 3 | Generator Loss: 15.549366 | Discriminator Loss: 0.023148\n","| Global Epoch: 98 | Local Epoch: 4 | Generator Loss: 15.675513 | Discriminator Loss: 0.018828\n","| Global Epoch: 98 | Local Epoch: 5 | Generator Loss: 15.701101 | Discriminator Loss: 0.015863\n","| Global Epoch: 98 | Local Epoch: 6 | Generator Loss: 15.726390 | Discriminator Loss: 0.013707\n","| Global Epoch: 98 | Local Epoch: 7 | Generator Loss: 15.754300 | Discriminator Loss: 0.012073\n","| Global Epoch: 98 | Local Epoch: 8 | Generator Loss: 15.877567 | Discriminator Loss: 0.010826\n","| Global Epoch: 98 | Local Epoch: 9 | Generator Loss: 15.953015 | Discriminator Loss: 0.009807\n","| Global Epoch: 98 | Local Epoch: 0 | Generator Loss: 16.715305 | Discriminator Loss: 0.095476\n","| Global Epoch: 98 | Local Epoch: 1 | Generator Loss: 16.626192 | Discriminator Loss: 0.053734\n","| Global Epoch: 98 | Local Epoch: 2 | Generator Loss: 16.467651 | Discriminator Loss: 0.037213\n","| Global Epoch: 98 | Local Epoch: 3 | Generator Loss: 16.326572 | Discriminator Loss: 0.028462\n","| Global Epoch: 98 | Local Epoch: 4 | Generator Loss: 16.255704 | Discriminator Loss: 0.023101\n","| Global Epoch: 98 | Local Epoch: 5 | Generator Loss: 16.323415 | Discriminator Loss: 0.019484\n","| Global Epoch: 98 | Local Epoch: 6 | Generator Loss: 16.327411 | Discriminator Loss: 0.016863\n","| Global Epoch: 98 | Local Epoch: 7 | Generator Loss: 16.309511 | Discriminator Loss: 0.014879\n","| Global Epoch: 98 | Local Epoch: 8 | Generator Loss: 16.295293 | Discriminator Loss: 0.013327\n","| Global Epoch: 98 | Local Epoch: 9 | Generator Loss: 16.276535 | Discriminator Loss: 0.012081\n","| Global Epoch: 98 | Local Epoch: 0 | Generator Loss: 16.073901 | Discriminator Loss: 0.061861\n","| Global Epoch: 98 | Local Epoch: 1 | Generator Loss: 15.750420 | Discriminator Loss: 0.035605\n","| Global Epoch: 98 | Local Epoch: 2 | Generator Loss: 15.681092 | Discriminator Loss: 0.025729\n","| Global Epoch: 98 | Local Epoch: 3 | Generator Loss: 15.559895 | Discriminator Loss: 0.024516\n","| Global Epoch: 98 | Local Epoch: 4 | Generator Loss: 15.658052 | Discriminator Loss: 0.022065\n","| Global Epoch: 98 | Local Epoch: 5 | Generator Loss: 15.726297 | Discriminator Loss: 0.018890\n","| Global Epoch: 98 | Local Epoch: 6 | Generator Loss: 15.781757 | Discriminator Loss: 0.016386\n","| Global Epoch: 98 | Local Epoch: 7 | Generator Loss: 15.829589 | Discriminator Loss: 0.014457\n","| Global Epoch: 98 | Local Epoch: 8 | Generator Loss: 15.768363 | Discriminator Loss: 0.012976\n","| Global Epoch: 98 | Local Epoch: 9 | Generator Loss: 15.762028 | Discriminator Loss: 0.011752\n","After 99 epoch global training, averged local generator loss is: 15.7695, averged local discrimitor loss is: 0.0570\n","Averged validation loss is: 4.7587\n","| Global Epoch: 99 | Local Epoch: 0 | Generator Loss: 16.041849 | Discriminator Loss: 0.049102\n","| Global Epoch: 99 | Local Epoch: 1 | Generator Loss: 16.062743 | Discriminator Loss: 0.031424\n","| Global Epoch: 99 | Local Epoch: 2 | Generator Loss: 16.128713 | Discriminator Loss: 0.021414\n","| Global Epoch: 99 | Local Epoch: 3 | Generator Loss: 16.090987 | Discriminator Loss: 0.016297\n","| Global Epoch: 99 | Local Epoch: 4 | Generator Loss: 15.956626 | Discriminator Loss: 0.013190\n","| Global Epoch: 99 | Local Epoch: 5 | Generator Loss: 15.948446 | Discriminator Loss: 0.011057\n","| Global Epoch: 99 | Local Epoch: 6 | Generator Loss: 15.954412 | Discriminator Loss: 0.009543\n","| Global Epoch: 99 | Local Epoch: 7 | Generator Loss: 15.963451 | Discriminator Loss: 0.008380\n","| Global Epoch: 99 | Local Epoch: 8 | Generator Loss: 16.011787 | Discriminator Loss: 0.007478\n","| Global Epoch: 99 | Local Epoch: 9 | Generator Loss: 16.002819 | Discriminator Loss: 0.006753\n","| Global Epoch: 99 | Local Epoch: 0 | Generator Loss: 15.840063 | Discriminator Loss: 0.212450\n","| Global Epoch: 99 | Local Epoch: 1 | Generator Loss: 15.641571 | Discriminator Loss: 0.128593\n","| Global Epoch: 99 | Local Epoch: 2 | Generator Loss: 15.561783 | Discriminator Loss: 0.088302\n","| Global Epoch: 99 | Local Epoch: 3 | Generator Loss: 15.474993 | Discriminator Loss: 0.067144\n","| Global Epoch: 99 | Local Epoch: 4 | Generator Loss: 15.402051 | Discriminator Loss: 0.054238\n","| Global Epoch: 99 | Local Epoch: 5 | Generator Loss: 15.333732 | Discriminator Loss: 0.045567\n","| Global Epoch: 99 | Local Epoch: 6 | Generator Loss: 15.283622 | Discriminator Loss: 0.040052\n","| Global Epoch: 99 | Local Epoch: 7 | Generator Loss: 15.235697 | Discriminator Loss: 0.040684\n","| Global Epoch: 99 | Local Epoch: 8 | Generator Loss: 15.273954 | Discriminator Loss: 0.037252\n","| Global Epoch: 99 | Local Epoch: 9 | Generator Loss: 15.160792 | Discriminator Loss: 0.040027\n","| Global Epoch: 99 | Local Epoch: 0 | Generator Loss: 14.984930 | Discriminator Loss: 0.051239\n","| Global Epoch: 99 | Local Epoch: 1 | Generator Loss: 15.089641 | Discriminator Loss: 0.028748\n","| Global Epoch: 99 | Local Epoch: 2 | Generator Loss: 15.129260 | Discriminator Loss: 0.019884\n","| Global Epoch: 99 | Local Epoch: 3 | Generator Loss: 15.425031 | Discriminator Loss: 0.015287\n","| Global Epoch: 99 | Local Epoch: 4 | Generator Loss: 15.551003 | Discriminator Loss: 0.012395\n","| Global Epoch: 99 | Local Epoch: 5 | Generator Loss: 15.625018 | Discriminator Loss: 0.010440\n","| Global Epoch: 99 | Local Epoch: 6 | Generator Loss: 15.738709 | Discriminator Loss: 0.009035\n","| Global Epoch: 99 | Local Epoch: 7 | Generator Loss: 15.861256 | Discriminator Loss: 0.007979\n","| Global Epoch: 99 | Local Epoch: 8 | Generator Loss: 15.930770 | Discriminator Loss: 0.007140\n","| Global Epoch: 99 | Local Epoch: 9 | Generator Loss: 15.964988 | Discriminator Loss: 0.006466\n","| Global Epoch: 99 | Local Epoch: 0 | Generator Loss: 16.360991 | Discriminator Loss: 0.427963\n","| Global Epoch: 99 | Local Epoch: 1 | Generator Loss: 15.759469 | Discriminator Loss: 0.238031\n","| Global Epoch: 99 | Local Epoch: 2 | Generator Loss: 15.547284 | Discriminator Loss: 0.162927\n","| Global Epoch: 99 | Local Epoch: 3 | Generator Loss: 15.390099 | Discriminator Loss: 0.123846\n","| Global Epoch: 99 | Local Epoch: 4 | Generator Loss: 15.264774 | Discriminator Loss: 0.100103\n","| Global Epoch: 99 | Local Epoch: 5 | Generator Loss: 15.230262 | Discriminator Loss: 0.084065\n","| Global Epoch: 99 | Local Epoch: 6 | Generator Loss: 15.229836 | Discriminator Loss: 0.072492\n","| Global Epoch: 99 | Local Epoch: 7 | Generator Loss: 15.221604 | Discriminator Loss: 0.063764\n","| Global Epoch: 99 | Local Epoch: 8 | Generator Loss: 15.197666 | Discriminator Loss: 0.056937\n","| Global Epoch: 99 | Local Epoch: 9 | Generator Loss: 15.186063 | Discriminator Loss: 0.051452\n","| Global Epoch: 99 | Local Epoch: 0 | Generator Loss: 15.116694 | Discriminator Loss: 0.056588\n","| Global Epoch: 99 | Local Epoch: 1 | Generator Loss: 14.993587 | Discriminator Loss: 0.033696\n","| Global Epoch: 99 | Local Epoch: 2 | Generator Loss: 15.096761 | Discriminator Loss: 0.023388\n","| Global Epoch: 99 | Local Epoch: 3 | Generator Loss: 15.334222 | Discriminator Loss: 0.017918\n","| Global Epoch: 99 | Local Epoch: 4 | Generator Loss: 15.503539 | Discriminator Loss: 0.016156\n","| Global Epoch: 99 | Local Epoch: 5 | Generator Loss: 15.603750 | Discriminator Loss: 0.013978\n","| Global Epoch: 99 | Local Epoch: 6 | Generator Loss: 15.686993 | Discriminator Loss: 0.012333\n","| Global Epoch: 99 | Local Epoch: 7 | Generator Loss: 15.704119 | Discriminator Loss: 0.011033\n","| Global Epoch: 99 | Local Epoch: 8 | Generator Loss: 15.814531 | Discriminator Loss: 0.009898\n","| Global Epoch: 99 | Local Epoch: 9 | Generator Loss: 15.888526 | Discriminator Loss: 0.008968\n","After 100 epoch global training, averged local generator loss is: 15.7707, averged local discrimitor loss is: 0.0565\n","Averged validation loss is: 5.8778\n"]}],"source":["%cd /content/drive/My Drive/BAGAN\n","\n","! python fed_bagan.py \\\n","--dis_num=5 \\\n","--local_bs=64 \\\n","--validate_bs=64 \\\n","--num_workers=2 \\\n","--local_maxepoch=10 \\\n","--global_maxepoch=100 \\\n","--z_dim=100 \\\n","--gf_dim=128 \\\n","--df_dim=128 \\\n","--iid=0 \\\n","--unequal=1 \\\n","--num_users=10 \\\n","--frac=0.5 \\\n","--base_lr=0.0002 \\\n","--loss='nll' \\\n","--dataset='cifar10' \\\n","--optimizer='adam' \\\n","--save_dir='./exp_cifar10/20220532_H114137' \\\n","--pretrained_dir='./exp_cifar10/20220531_H114137/autoencoder/BestResult' \\\n","--distribution_path='./exp_cifar10/20220531_H114137/Distribution/class_distribution.dt' \\\n","--train_dir='./datasets/cifar10' \\\n"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["qi_jvR9ekuHl","6G2SmIJ2k3Bg","UY2HFsRSk-Qf"],"name":"running_BAGAN.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}